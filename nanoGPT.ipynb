{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO1G2BZn1eFx0/XoN974cfh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlfTang/nanoGPT_exp/blob/main/nanoGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Fpv4z3X_tFn"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/AlfTang/nanoGPT_exp.git\n",
        "!pip install torch numpy transformers datasets tiktoken wandb tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd nanoGPT_exp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZ2rItPLCrkE",
        "outputId": "58d87f5c-0af5-470b-e02e-3a5a45e86c4c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nanoGPT_exp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Shakespear Writer"
      ],
      "metadata": {
        "id": "Zvh0YKhcTCZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python data/shakespeare_char/prepare.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7vEbq86C5RH",
        "outputId": "555e7517-b9b4-4adc-eb98-ef366564add6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters: 1,115,394\n",
            "all the unique characters: \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "vocab size: 65\n",
            "train has 1,003,854 tokens\n",
            "val has 111,540 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py config/train_shakespeare_char.py"
      ],
      "metadata": {
        "id": "meEdjlssEDlE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a00c3b5c-da84-4a52-d353-51cd0da0de08"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding config with config/train_shakespeare_char.py:\n",
            "# train a miniature character-level shakespeare model\n",
            "# good for debugging and playing on macbooks and such\n",
            "\n",
            "out_dir = 'out-shakespeare-char'\n",
            "eval_interval = 250 # keep frequent because we'll overfit\n",
            "eval_iters = 200\n",
            "log_interval = 10 # don't print too too often\n",
            "\n",
            "# we expect to overfit on this small dataset, so only save when val improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "wandb_log = False # override via command line if you like\n",
            "wandb_project = 'shakespeare-char'\n",
            "wandb_run_name = 'mini-gpt'\n",
            "\n",
            "dataset = 'shakespeare_char'\n",
            "gradient_accumulation_steps = 1\n",
            "batch_size = 64\n",
            "block_size = 256 # context of up to 256 previous characters\n",
            "\n",
            "# baby GPT model :)\n",
            "n_layer = 6\n",
            "n_head = 6\n",
            "n_embd = 384\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
            "max_iters = 5000\n",
            "lr_decay_iters = 5000 # make equal to max_iters usually\n",
            "min_lr = 1e-4 # learning_rate / 10 usually\n",
            "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
            "\n",
            "warmup_iters = 100 # not super necessary potentially\n",
            "\n",
            "# on macbook also add\n",
            "# device = 'cpu'  # run on cpu only\n",
            "# compile = False # do not torch compile the model\n",
            "\n",
            "tokens per iteration will be: 16,384\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 10.65M\n",
            "num decayed parameter tensors: 26, with 10,740,096 parameters\n",
            "num non-decayed parameter tensors: 13, with 4,992 parameters\n",
            "using fused AdamW: True\n",
            "compiling the model... (takes a ~minute)\n",
            "step 0: train loss 4.2874, val loss 4.2823\n",
            "[2023-09-18 07:57:03,006] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-18 07:57:03,776] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-18 07:57:05,461] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-18 07:57:05,927] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-18 07:57:06,456] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-18 07:57:06,728] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-18 07:57:07,144] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-18 07:57:07,423] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-18 07:57:07,851] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-18 07:57:08,142] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-18 07:57:08,566] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-18 07:57:08,850] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "iter 0: loss 4.2649, time 36305.07ms, mfu -100.00%\n",
            "iter 10: loss 3.2438, time 107.23ms, mfu 3.47%\n",
            "iter 20: loss 2.7899, time 104.89ms, mfu 3.48%\n",
            "iter 30: loss 2.6383, time 107.23ms, mfu 3.48%\n",
            "iter 40: loss 2.5763, time 108.27ms, mfu 3.48%\n",
            "iter 50: loss 2.5261, time 106.81ms, mfu 3.48%\n",
            "iter 60: loss 2.5136, time 105.65ms, mfu 3.48%\n",
            "iter 70: loss 2.4921, time 108.45ms, mfu 3.48%\n",
            "iter 80: loss 2.4932, time 109.52ms, mfu 3.47%\n",
            "iter 90: loss 2.4696, time 108.95ms, mfu 3.47%\n",
            "iter 100: loss 2.4526, time 107.70ms, mfu 3.47%\n",
            "iter 110: loss 2.4543, time 109.37ms, mfu 3.46%\n",
            "iter 120: loss 2.4223, time 110.35ms, mfu 3.45%\n",
            "iter 130: loss 2.4059, time 107.55ms, mfu 3.45%\n",
            "iter 140: loss 2.3925, time 109.02ms, mfu 3.45%\n",
            "iter 150: loss 2.4098, time 108.77ms, mfu 3.45%\n",
            "iter 160: loss 2.3675, time 108.53ms, mfu 3.45%\n",
            "iter 170: loss 2.3382, time 107.92ms, mfu 3.45%\n",
            "iter 180: loss 2.3011, time 113.72ms, mfu 3.43%\n",
            "iter 190: loss 2.2278, time 107.59ms, mfu 3.43%\n",
            "iter 200: loss 2.2004, time 110.78ms, mfu 3.43%\n",
            "iter 210: loss 2.1244, time 111.36ms, mfu 3.42%\n",
            "iter 220: loss 2.1338, time 110.30ms, mfu 3.41%\n",
            "iter 230: loss 2.0709, time 111.06ms, mfu 3.41%\n",
            "iter 240: loss 2.0742, time 110.34ms, mfu 3.40%\n",
            "step 250: train loss 1.9616, val loss 2.0647\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 250: loss 2.0277, time 14637.90ms, mfu 3.07%\n",
            "iter 260: loss 1.9685, time 113.12ms, mfu 3.09%\n",
            "iter 270: loss 1.9776, time 116.58ms, mfu 3.10%\n",
            "iter 280: loss 1.9798, time 117.16ms, mfu 3.11%\n",
            "iter 290: loss 1.9237, time 115.02ms, mfu 3.12%\n",
            "iter 300: loss 1.8944, time 112.71ms, mfu 3.14%\n",
            "iter 310: loss 1.8637, time 114.86ms, mfu 3.15%\n",
            "iter 320: loss 1.8569, time 117.15ms, mfu 3.15%\n",
            "iter 330: loss 1.8088, time 114.07ms, mfu 3.16%\n",
            "iter 340: loss 1.7812, time 117.67ms, mfu 3.16%\n",
            "iter 350: loss 1.8272, time 115.07ms, mfu 3.17%\n",
            "iter 360: loss 1.7745, time 114.09ms, mfu 3.18%\n",
            "iter 370: loss 1.7414, time 116.56ms, mfu 3.18%\n",
            "iter 380: loss 1.7304, time 117.72ms, mfu 3.18%\n",
            "iter 390: loss 1.7372, time 115.67ms, mfu 3.19%\n",
            "iter 400: loss 1.7640, time 120.02ms, mfu 3.18%\n",
            "iter 410: loss 1.6959, time 116.59ms, mfu 3.18%\n",
            "iter 420: loss 1.7088, time 118.24ms, mfu 3.18%\n",
            "iter 430: loss 1.6815, time 120.59ms, mfu 3.17%\n",
            "iter 440: loss 1.6462, time 118.84ms, mfu 3.16%\n",
            "iter 450: loss 1.6511, time 118.46ms, mfu 3.16%\n",
            "iter 460: loss 1.6024, time 117.42ms, mfu 3.16%\n",
            "iter 470: loss 1.6554, time 117.84ms, mfu 3.16%\n",
            "iter 480: loss 1.6165, time 120.45ms, mfu 3.16%\n",
            "iter 490: loss 1.6016, time 120.66ms, mfu 3.15%\n",
            "step 500: train loss 1.5285, val loss 1.7362\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 500: loss 1.6016, time 12959.67ms, mfu 2.84%\n",
            "iter 510: loss 1.6162, time 123.16ms, mfu 2.86%\n",
            "iter 520: loss 1.6020, time 124.84ms, mfu 2.87%\n",
            "iter 530: loss 1.5657, time 122.27ms, mfu 2.89%\n",
            "iter 540: loss 1.6203, time 123.01ms, mfu 2.90%\n",
            "iter 550: loss 1.5671, time 121.31ms, mfu 2.92%\n",
            "iter 560: loss 1.5651, time 126.45ms, mfu 2.92%\n",
            "iter 570: loss 1.5745, time 121.43ms, mfu 2.94%\n",
            "iter 580: loss 1.5401, time 121.72ms, mfu 2.95%\n",
            "iter 590: loss 1.5016, time 121.48ms, mfu 2.96%\n",
            "iter 600: loss 1.5180, time 124.28ms, mfu 2.96%\n",
            "iter 610: loss 1.5552, time 122.67ms, mfu 2.97%\n",
            "iter 620: loss 1.5292, time 124.04ms, mfu 2.97%\n",
            "iter 630: loss 1.5179, time 123.08ms, mfu 2.98%\n",
            "iter 640: loss 1.4779, time 126.19ms, mfu 2.98%\n",
            "iter 650: loss 1.5043, time 123.69ms, mfu 2.98%\n",
            "iter 660: loss 1.5145, time 128.38ms, mfu 2.97%\n",
            "iter 670: loss 1.4491, time 127.36ms, mfu 2.97%\n",
            "iter 680: loss 1.5118, time 124.03ms, mfu 2.97%\n",
            "iter 690: loss 1.4611, time 121.75ms, mfu 2.98%\n",
            "iter 700: loss 1.4802, time 123.50ms, mfu 2.98%\n",
            "iter 710: loss 1.4568, time 122.95ms, mfu 2.99%\n",
            "iter 720: loss 1.4481, time 123.15ms, mfu 2.99%\n",
            "iter 730: loss 1.4214, time 125.33ms, mfu 2.99%\n",
            "iter 740: loss 1.4311, time 121.50ms, mfu 3.00%\n",
            "step 750: train loss 1.3611, val loss 1.5957\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 750: loss 1.4226, time 13188.13ms, mfu 2.70%\n",
            "iter 760: loss 1.4461, time 120.47ms, mfu 2.74%\n",
            "iter 770: loss 1.4283, time 121.75ms, mfu 2.77%\n",
            "iter 780: loss 1.4123, time 121.14ms, mfu 2.80%\n",
            "iter 790: loss 1.4221, time 121.97ms, mfu 2.83%\n",
            "iter 800: loss 1.4286, time 121.92ms, mfu 2.85%\n",
            "iter 810: loss 1.4068, time 121.17ms, mfu 2.87%\n",
            "iter 820: loss 1.4105, time 121.11ms, mfu 2.89%\n",
            "iter 830: loss 1.3939, time 120.52ms, mfu 2.91%\n",
            "iter 840: loss 1.4056, time 120.32ms, mfu 2.93%\n",
            "iter 850: loss 1.3911, time 121.37ms, mfu 2.95%\n",
            "iter 860: loss 1.4010, time 125.04ms, mfu 2.95%\n",
            "iter 870: loss 1.4061, time 120.33ms, mfu 2.96%\n",
            "iter 880: loss 1.3758, time 118.71ms, mfu 2.98%\n",
            "iter 890: loss 1.3920, time 120.66ms, mfu 2.99%\n",
            "iter 900: loss 1.3728, time 120.05ms, mfu 3.00%\n",
            "iter 910: loss 1.3245, time 122.86ms, mfu 3.01%\n",
            "iter 920: loss 1.3671, time 120.66ms, mfu 3.01%\n",
            "iter 930: loss 1.3670, time 122.11ms, mfu 3.02%\n",
            "iter 940: loss 1.3524, time 122.55ms, mfu 3.02%\n",
            "iter 950: loss 1.3538, time 122.08ms, mfu 3.02%\n",
            "iter 960: loss 1.3668, time 122.25ms, mfu 3.03%\n",
            "iter 970: loss 1.3636, time 119.31ms, mfu 3.04%\n",
            "iter 980: loss 1.3566, time 120.61ms, mfu 3.04%\n",
            "iter 990: loss 1.3400, time 119.38ms, mfu 3.05%\n",
            "step 1000: train loss 1.2762, val loss 1.5256\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1000: loss 1.3415, time 13063.89ms, mfu 2.75%\n",
            "iter 1010: loss 1.3431, time 120.38ms, mfu 2.78%\n",
            "iter 1020: loss 1.3133, time 121.23ms, mfu 2.81%\n",
            "iter 1030: loss 1.3367, time 121.05ms, mfu 2.84%\n",
            "iter 1040: loss 1.3645, time 122.25ms, mfu 2.86%\n",
            "iter 1050: loss 1.2936, time 121.71ms, mfu 2.88%\n",
            "iter 1060: loss 1.3426, time 122.25ms, mfu 2.90%\n",
            "iter 1070: loss 1.3305, time 121.24ms, mfu 2.91%\n",
            "iter 1080: loss 1.3386, time 122.10ms, mfu 2.93%\n",
            "iter 1090: loss 1.3539, time 121.28ms, mfu 2.94%\n",
            "iter 1100: loss 1.3168, time 123.30ms, mfu 2.95%\n",
            "iter 1110: loss 1.3008, time 122.05ms, mfu 2.96%\n",
            "iter 1120: loss 1.3026, time 119.82ms, mfu 2.98%\n",
            "iter 1130: loss 1.3015, time 121.97ms, mfu 2.98%\n",
            "iter 1140: loss 1.2992, time 122.62ms, mfu 2.99%\n",
            "iter 1150: loss 1.3126, time 121.40ms, mfu 3.00%\n",
            "iter 1160: loss 1.3269, time 120.83ms, mfu 3.01%\n",
            "iter 1170: loss 1.3064, time 120.75ms, mfu 3.01%\n",
            "iter 1180: loss 1.3226, time 120.21ms, mfu 3.02%\n",
            "iter 1190: loss 1.2668, time 125.08ms, mfu 3.02%\n",
            "iter 1200: loss 1.2964, time 125.87ms, mfu 3.01%\n",
            "iter 1210: loss 1.2739, time 121.51ms, mfu 3.02%\n",
            "iter 1220: loss 1.3009, time 122.27ms, mfu 3.02%\n",
            "iter 1230: loss 1.2977, time 120.39ms, mfu 3.03%\n",
            "iter 1240: loss 1.3042, time 120.73ms, mfu 3.03%\n",
            "step 1250: train loss 1.2079, val loss 1.4969\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1250: loss 1.2753, time 13273.29ms, mfu 2.73%\n",
            "iter 1260: loss 1.2867, time 121.03ms, mfu 2.77%\n",
            "iter 1270: loss 1.2715, time 121.83ms, mfu 2.80%\n",
            "iter 1280: loss 1.2605, time 120.73ms, mfu 2.83%\n",
            "iter 1290: loss 1.2806, time 122.08ms, mfu 2.85%\n",
            "iter 1300: loss 1.2990, time 120.92ms, mfu 2.87%\n",
            "iter 1310: loss 1.2402, time 121.61ms, mfu 2.89%\n",
            "iter 1320: loss 1.3072, time 121.46ms, mfu 2.91%\n",
            "iter 1330: loss 1.2705, time 122.93ms, mfu 2.92%\n",
            "iter 1340: loss 1.2992, time 121.51ms, mfu 2.94%\n",
            "iter 1350: loss 1.2529, time 122.51ms, mfu 2.95%\n",
            "iter 1360: loss 1.2679, time 120.49ms, mfu 2.96%\n",
            "iter 1370: loss 1.2591, time 120.33ms, mfu 2.97%\n",
            "iter 1380: loss 1.2695, time 120.73ms, mfu 2.99%\n",
            "iter 1390: loss 1.2550, time 120.53ms, mfu 3.00%\n",
            "iter 1400: loss 1.2619, time 122.80ms, mfu 3.00%\n",
            "iter 1410: loss 1.2513, time 122.52ms, mfu 3.00%\n",
            "iter 1420: loss 1.2738, time 121.25ms, mfu 3.01%\n",
            "iter 1430: loss 1.2424, time 122.51ms, mfu 3.01%\n",
            "iter 1440: loss 1.2576, time 122.26ms, mfu 3.02%\n",
            "iter 1450: loss 1.2388, time 119.86ms, mfu 3.03%\n",
            "iter 1460: loss 1.2448, time 121.03ms, mfu 3.03%\n",
            "iter 1470: loss 1.2223, time 123.85ms, mfu 3.03%\n",
            "iter 1480: loss 1.2091, time 120.06ms, mfu 3.04%\n",
            "iter 1490: loss 1.2391, time 121.55ms, mfu 3.04%\n",
            "step 1500: train loss 1.1533, val loss 1.4693\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1500: loss 1.1903, time 13238.89ms, mfu 2.74%\n",
            "iter 1510: loss 1.2334, time 122.95ms, mfu 2.77%\n",
            "iter 1520: loss 1.2242, time 120.43ms, mfu 2.80%\n",
            "iter 1530: loss 1.2554, time 123.84ms, mfu 2.82%\n",
            "iter 1540: loss 1.1952, time 120.15ms, mfu 2.85%\n",
            "iter 1550: loss 1.2376, time 120.18ms, mfu 2.87%\n",
            "iter 1560: loss 1.2124, time 121.45ms, mfu 2.89%\n",
            "iter 1570: loss 1.2289, time 120.46ms, mfu 2.91%\n",
            "iter 1580: loss 1.2126, time 123.89ms, mfu 2.92%\n",
            "iter 1590: loss 1.1923, time 121.30ms, mfu 2.94%\n",
            "iter 1600: loss 1.2019, time 121.49ms, mfu 2.95%\n",
            "iter 1610: loss 1.2397, time 121.13ms, mfu 2.96%\n",
            "iter 1620: loss 1.1842, time 122.54ms, mfu 2.97%\n",
            "iter 1630: loss 1.2066, time 122.58ms, mfu 2.98%\n",
            "iter 1640: loss 1.2057, time 123.01ms, mfu 2.98%\n",
            "iter 1650: loss 1.1848, time 120.93ms, mfu 2.99%\n",
            "iter 1660: loss 1.2201, time 121.90ms, mfu 3.00%\n",
            "iter 1670: loss 1.2026, time 121.10ms, mfu 3.01%\n",
            "iter 1680: loss 1.2037, time 121.69ms, mfu 3.01%\n",
            "iter 1690: loss 1.2076, time 124.09ms, mfu 3.01%\n",
            "iter 1700: loss 1.1913, time 120.48ms, mfu 3.02%\n",
            "iter 1710: loss 1.1841, time 126.35ms, mfu 3.01%\n",
            "iter 1720: loss 1.1854, time 121.06ms, mfu 3.02%\n",
            "iter 1730: loss 1.1986, time 121.60ms, mfu 3.02%\n",
            "iter 1740: loss 1.1763, time 121.31ms, mfu 3.03%\n",
            "step 1750: train loss 1.1028, val loss 1.4603\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1750: loss 1.1805, time 13301.18ms, mfu 2.73%\n",
            "iter 1760: loss 1.1936, time 123.64ms, mfu 2.76%\n",
            "iter 1770: loss 1.1979, time 122.22ms, mfu 2.79%\n",
            "iter 1780: loss 1.1955, time 122.93ms, mfu 2.81%\n",
            "iter 1790: loss 1.1912, time 121.10ms, mfu 2.84%\n",
            "iter 1800: loss 1.1752, time 122.01ms, mfu 2.86%\n",
            "iter 1810: loss 1.1647, time 121.01ms, mfu 2.88%\n",
            "iter 1820: loss 1.1709, time 121.68ms, mfu 2.90%\n",
            "iter 1830: loss 1.1703, time 121.30ms, mfu 2.92%\n",
            "iter 1840: loss 1.1592, time 121.84ms, mfu 2.93%\n",
            "iter 1850: loss 1.1671, time 120.02ms, mfu 2.95%\n",
            "iter 1860: loss 1.1751, time 120.73ms, mfu 2.96%\n",
            "iter 1870: loss 1.1444, time 120.90ms, mfu 2.97%\n",
            "iter 1880: loss 1.1820, time 120.02ms, mfu 2.99%\n",
            "iter 1890: loss 1.1850, time 123.06ms, mfu 2.99%\n",
            "iter 1900: loss 1.1317, time 123.47ms, mfu 2.99%\n",
            "iter 1910: loss 1.1653, time 121.58ms, mfu 3.00%\n",
            "iter 1920: loss 1.1680, time 122.95ms, mfu 3.00%\n",
            "iter 1930: loss 1.1419, time 120.38ms, mfu 3.01%\n",
            "iter 1940: loss 1.1259, time 122.75ms, mfu 3.02%\n",
            "iter 1950: loss 1.1378, time 122.84ms, mfu 3.02%\n",
            "iter 1960: loss 1.1573, time 123.07ms, mfu 3.02%\n",
            "iter 1970: loss 1.1574, time 125.50ms, mfu 3.01%\n",
            "iter 1980: loss 1.1518, time 125.13ms, mfu 3.01%\n",
            "iter 1990: loss 1.1529, time 121.67ms, mfu 3.01%\n",
            "step 2000: train loss 1.0560, val loss 1.4723\n",
            "iter 2000: loss 1.1313, time 13005.28ms, mfu 2.72%\n",
            "iter 2010: loss 1.1371, time 122.28ms, mfu 2.75%\n",
            "iter 2020: loss 1.1267, time 119.81ms, mfu 2.79%\n",
            "iter 2030: loss 1.1575, time 121.77ms, mfu 2.81%\n",
            "iter 2040: loss 1.1369, time 125.14ms, mfu 2.83%\n",
            "iter 2050: loss 1.1200, time 123.03ms, mfu 2.85%\n",
            "iter 2060: loss 1.1054, time 126.84ms, mfu 2.86%\n",
            "iter 2070: loss 1.1241, time 120.48ms, mfu 2.88%\n",
            "iter 2080: loss 1.1240, time 124.90ms, mfu 2.89%\n",
            "iter 2090: loss 1.1352, time 120.73ms, mfu 2.91%\n",
            "iter 2100: loss 1.1349, time 124.25ms, mfu 2.92%\n",
            "iter 2110: loss 1.1359, time 121.22ms, mfu 2.94%\n",
            "iter 2120: loss 1.1305, time 122.62ms, mfu 2.95%\n",
            "iter 2130: loss 1.1397, time 121.77ms, mfu 2.96%\n",
            "iter 2140: loss 1.1365, time 121.61ms, mfu 2.97%\n",
            "iter 2150: loss 1.1284, time 124.15ms, mfu 2.97%\n",
            "iter 2160: loss 1.1414, time 124.20ms, mfu 2.97%\n",
            "iter 2170: loss 1.1367, time 121.75ms, mfu 2.98%\n",
            "iter 2180: loss 1.1187, time 122.74ms, mfu 2.99%\n",
            "iter 2190: loss 1.1095, time 122.84ms, mfu 2.99%\n",
            "iter 2200: loss 1.1194, time 120.61ms, mfu 3.00%\n",
            "iter 2210: loss 1.1110, time 122.93ms, mfu 3.01%\n",
            "iter 2220: loss 1.1232, time 122.97ms, mfu 3.01%\n",
            "iter 2230: loss 1.1231, time 121.92ms, mfu 3.01%\n",
            "iter 2240: loss 1.1356, time 123.70ms, mfu 3.01%\n",
            "step 2250: train loss 1.0100, val loss 1.4719\n",
            "iter 2250: loss 1.1040, time 12966.83ms, mfu 2.71%\n",
            "iter 2260: loss 1.1040, time 125.12ms, mfu 2.74%\n",
            "iter 2270: loss 1.1425, time 121.68ms, mfu 2.77%\n",
            "iter 2280: loss 1.0969, time 121.66ms, mfu 2.80%\n",
            "iter 2290: loss 1.1535, time 121.30ms, mfu 2.83%\n",
            "iter 2300: loss 1.1172, time 120.93ms, mfu 2.85%\n",
            "iter 2310: loss 1.0925, time 122.41ms, mfu 2.87%\n",
            "iter 2320: loss 1.0992, time 121.58ms, mfu 2.89%\n",
            "iter 2330: loss 1.1028, time 121.99ms, mfu 2.91%\n",
            "iter 2340: loss 1.1236, time 123.12ms, mfu 2.92%\n",
            "iter 2350: loss 1.1000, time 121.09ms, mfu 2.94%\n",
            "iter 2360: loss 1.1014, time 122.79ms, mfu 2.95%\n",
            "iter 2370: loss 1.0974, time 123.26ms, mfu 2.95%\n",
            "iter 2380: loss 1.0846, time 123.26ms, mfu 2.96%\n",
            "iter 2390: loss 1.0864, time 123.69ms, mfu 2.97%\n",
            "iter 2400: loss 1.0765, time 120.98ms, mfu 2.98%\n",
            "iter 2410: loss 1.0720, time 123.09ms, mfu 2.98%\n",
            "iter 2420: loss 1.0792, time 121.26ms, mfu 2.99%\n",
            "iter 2430: loss 1.0588, time 120.39ms, mfu 3.00%\n",
            "iter 2440: loss 1.0537, time 123.87ms, mfu 3.00%\n",
            "iter 2450: loss 1.0758, time 118.88ms, mfu 3.02%\n",
            "iter 2460: loss 1.0872, time 121.75ms, mfu 3.02%\n",
            "iter 2470: loss 1.0853, time 121.85ms, mfu 3.02%\n",
            "iter 2480: loss 1.0859, time 122.96ms, mfu 3.02%\n",
            "iter 2490: loss 1.0610, time 124.76ms, mfu 3.02%\n",
            "step 2500: train loss 0.9590, val loss 1.4886\n",
            "iter 2500: loss 1.0800, time 12929.91ms, mfu 2.72%\n",
            "iter 2510: loss 1.0775, time 122.30ms, mfu 2.75%\n",
            "iter 2520: loss 1.0480, time 121.09ms, mfu 2.79%\n",
            "iter 2530: loss 1.0608, time 122.65ms, mfu 2.81%\n",
            "iter 2540: loss 1.0559, time 121.62ms, mfu 2.84%\n",
            "iter 2550: loss 1.0655, time 121.72ms, mfu 2.86%\n",
            "iter 2560: loss 1.0615, time 122.53ms, mfu 2.88%\n",
            "iter 2570: loss 1.0752, time 122.13ms, mfu 2.89%\n",
            "iter 2580: loss 1.0818, time 121.66ms, mfu 2.91%\n",
            "iter 2590: loss 1.0710, time 122.21ms, mfu 2.93%\n",
            "iter 2600: loss 1.0741, time 121.86ms, mfu 2.94%\n",
            "iter 2610: loss 1.0476, time 120.33ms, mfu 2.95%\n",
            "iter 2620: loss 1.0413, time 121.39ms, mfu 2.97%\n",
            "iter 2630: loss 1.0266, time 120.92ms, mfu 2.98%\n",
            "iter 2640: loss 1.0461, time 121.46ms, mfu 2.99%\n",
            "iter 2650: loss 1.0662, time 121.42ms, mfu 2.99%\n",
            "iter 2660: loss 1.0466, time 121.40ms, mfu 3.00%\n",
            "iter 2670: loss 1.0205, time 121.30ms, mfu 3.01%\n",
            "iter 2680: loss 1.0463, time 120.95ms, mfu 3.02%\n",
            "iter 2690: loss 1.0574, time 121.67ms, mfu 3.02%\n",
            "iter 2700: loss 1.0213, time 120.01ms, mfu 3.03%\n",
            "iter 2710: loss 1.0412, time 122.08ms, mfu 3.03%\n",
            "iter 2720: loss 1.0447, time 120.66ms, mfu 3.04%\n",
            "iter 2730: loss 1.0544, time 122.41ms, mfu 3.04%\n",
            "iter 2740: loss 1.0265, time 122.57ms, mfu 3.04%\n",
            "step 2750: train loss 0.9135, val loss 1.5104\n",
            "iter 2750: loss 1.0351, time 12905.70ms, mfu 2.74%\n",
            "iter 2760: loss 1.0271, time 124.15ms, mfu 2.76%\n",
            "iter 2770: loss 1.0300, time 120.65ms, mfu 2.80%\n",
            "iter 2780: loss 1.0212, time 122.75ms, mfu 2.82%\n",
            "iter 2790: loss 1.0406, time 122.15ms, mfu 2.84%\n",
            "iter 2800: loss 1.0119, time 120.00ms, mfu 2.87%\n",
            "iter 2810: loss 1.0451, time 122.05ms, mfu 2.89%\n",
            "iter 2820: loss 1.0255, time 123.84ms, mfu 2.90%\n",
            "iter 2830: loss 1.0377, time 123.82ms, mfu 2.91%\n",
            "iter 2840: loss 1.0019, time 118.94ms, mfu 2.93%\n",
            "iter 2850: loss 1.0306, time 121.94ms, mfu 2.95%\n",
            "iter 2860: loss 1.0237, time 120.33ms, mfu 2.96%\n",
            "iter 2870: loss 1.0016, time 121.80ms, mfu 2.97%\n",
            "iter 2880: loss 1.0270, time 122.32ms, mfu 2.98%\n",
            "iter 2890: loss 1.0150, time 122.13ms, mfu 2.99%\n",
            "iter 2900: loss 0.9963, time 121.72ms, mfu 2.99%\n",
            "iter 2910: loss 1.0427, time 122.31ms, mfu 3.00%\n",
            "iter 2920: loss 1.0190, time 120.50ms, mfu 3.01%\n",
            "iter 2930: loss 0.9971, time 124.17ms, mfu 3.01%\n",
            "iter 2940: loss 0.9860, time 120.15ms, mfu 3.02%\n",
            "iter 2950: loss 1.0231, time 120.79ms, mfu 3.02%\n",
            "iter 2960: loss 1.0000, time 119.78ms, mfu 3.03%\n",
            "iter 2970: loss 0.9940, time 121.10ms, mfu 3.04%\n",
            "iter 2980: loss 1.0014, time 122.55ms, mfu 3.04%\n",
            "iter 2990: loss 0.9899, time 119.77ms, mfu 3.04%\n",
            "step 3000: train loss 0.8673, val loss 1.5194\n",
            "iter 3000: loss 0.9866, time 12940.94ms, mfu 2.74%\n",
            "iter 3010: loss 0.9940, time 121.46ms, mfu 2.78%\n",
            "iter 3020: loss 0.9946, time 121.08ms, mfu 2.81%\n",
            "iter 3030: loss 1.0067, time 120.71ms, mfu 2.83%\n",
            "iter 3040: loss 1.0279, time 122.45ms, mfu 2.85%\n",
            "iter 3050: loss 0.9855, time 122.58ms, mfu 2.87%\n",
            "iter 3060: loss 0.9980, time 121.41ms, mfu 2.89%\n",
            "iter 3070: loss 1.0166, time 121.69ms, mfu 2.91%\n",
            "iter 3080: loss 1.0064, time 121.23ms, mfu 2.93%\n",
            "iter 3090: loss 0.9795, time 122.85ms, mfu 2.94%\n",
            "iter 3100: loss 0.9986, time 120.63ms, mfu 2.95%\n",
            "iter 3110: loss 0.9806, time 121.03ms, mfu 2.96%\n",
            "iter 3120: loss 0.9921, time 120.84ms, mfu 2.98%\n",
            "iter 3130: loss 0.9779, time 119.69ms, mfu 2.99%\n",
            "iter 3140: loss 0.9753, time 123.08ms, mfu 2.99%\n",
            "iter 3150: loss 0.9882, time 122.93ms, mfu 3.00%\n",
            "iter 3160: loss 1.0155, time 122.71ms, mfu 3.00%\n",
            "iter 3170: loss 0.9620, time 123.46ms, mfu 3.00%\n",
            "iter 3180: loss 0.9784, time 122.91ms, mfu 3.01%\n",
            "iter 3190: loss 0.9972, time 121.45ms, mfu 3.01%\n",
            "iter 3200: loss 0.9690, time 122.42ms, mfu 3.02%\n",
            "iter 3210: loss 0.9701, time 124.39ms, mfu 3.01%\n",
            "iter 3220: loss 0.9613, time 123.26ms, mfu 3.01%\n",
            "iter 3230: loss 0.9596, time 121.88ms, mfu 3.02%\n",
            "iter 3240: loss 0.9608, time 120.59ms, mfu 3.03%\n",
            "step 3250: train loss 0.8239, val loss 1.5611\n",
            "iter 3250: loss 0.9846, time 13005.14ms, mfu 2.73%\n",
            "iter 3260: loss 0.9668, time 121.73ms, mfu 2.76%\n",
            "iter 3270: loss 0.9772, time 121.06ms, mfu 2.79%\n",
            "iter 3280: loss 0.9445, time 122.32ms, mfu 2.82%\n",
            "iter 3290: loss 0.9432, time 121.26ms, mfu 2.84%\n",
            "iter 3300: loss 0.9451, time 122.29ms, mfu 2.86%\n",
            "iter 3310: loss 0.9571, time 121.51ms, mfu 2.88%\n",
            "iter 3320: loss 0.9727, time 123.57ms, mfu 2.90%\n",
            "iter 3330: loss 0.9581, time 122.78ms, mfu 2.91%\n",
            "iter 3340: loss 0.9502, time 123.32ms, mfu 2.92%\n",
            "iter 3350: loss 0.9538, time 123.11ms, mfu 2.93%\n",
            "iter 3360: loss 0.9362, time 120.13ms, mfu 2.95%\n",
            "iter 3370: loss 0.9606, time 121.77ms, mfu 2.96%\n",
            "iter 3380: loss 0.9514, time 119.99ms, mfu 2.97%\n",
            "iter 3390: loss 0.9561, time 121.31ms, mfu 2.98%\n",
            "iter 3400: loss 0.9596, time 120.91ms, mfu 2.99%\n",
            "iter 3410: loss 0.9444, time 121.94ms, mfu 3.00%\n",
            "iter 3420: loss 0.9479, time 123.41ms, mfu 3.00%\n",
            "iter 3430: loss 0.9430, time 120.38ms, mfu 3.01%\n",
            "iter 3440: loss 0.9693, time 122.02ms, mfu 3.02%\n",
            "iter 3450: loss 0.9509, time 123.12ms, mfu 3.02%\n",
            "iter 3460: loss 0.9453, time 121.70ms, mfu 3.02%\n",
            "iter 3470: loss 0.9385, time 121.83ms, mfu 3.03%\n",
            "iter 3480: loss 0.9504, time 120.78ms, mfu 3.03%\n",
            "iter 3490: loss 0.9106, time 120.70ms, mfu 3.04%\n",
            "step 3500: train loss 0.7790, val loss 1.5720\n",
            "iter 3500: loss 0.9011, time 12956.42ms, mfu 2.74%\n",
            "iter 3510: loss 0.9180, time 122.15ms, mfu 2.77%\n",
            "iter 3520: loss 0.9251, time 122.11ms, mfu 2.80%\n",
            "iter 3530: loss 0.9584, time 122.07ms, mfu 2.82%\n",
            "iter 3540: loss 0.9311, time 123.39ms, mfu 2.84%\n",
            "iter 3550: loss 0.9215, time 119.72ms, mfu 2.87%\n",
            "iter 3560: loss 0.9519, time 120.47ms, mfu 2.89%\n",
            "iter 3570: loss 0.9363, time 122.66ms, mfu 2.91%\n",
            "iter 3580: loss 0.9419, time 120.34ms, mfu 2.92%\n",
            "iter 3590: loss 0.9228, time 123.74ms, mfu 2.93%\n",
            "iter 3600: loss 0.9237, time 123.51ms, mfu 2.94%\n",
            "iter 3610: loss 0.9118, time 120.61ms, mfu 2.96%\n",
            "iter 3620: loss 0.9171, time 122.06ms, mfu 2.97%\n",
            "iter 3630: loss 0.9199, time 122.82ms, mfu 2.97%\n",
            "iter 3640: loss 0.9228, time 122.49ms, mfu 2.98%\n",
            "iter 3650: loss 0.9075, time 121.52ms, mfu 2.99%\n",
            "iter 3660: loss 0.9391, time 122.02ms, mfu 2.99%\n",
            "iter 3670: loss 0.9375, time 119.96ms, mfu 3.01%\n",
            "iter 3680: loss 0.9084, time 120.19ms, mfu 3.02%\n",
            "iter 3690: loss 0.9350, time 120.63ms, mfu 3.02%\n",
            "iter 3700: loss 0.8690, time 121.88ms, mfu 3.03%\n",
            "iter 3710: loss 0.8807, time 120.77ms, mfu 3.03%\n",
            "iter 3720: loss 0.9150, time 120.97ms, mfu 3.04%\n",
            "iter 3730: loss 0.9002, time 120.78ms, mfu 3.04%\n",
            "iter 3740: loss 0.9056, time 120.84ms, mfu 3.05%\n",
            "step 3750: train loss 0.7414, val loss 1.6029\n",
            "iter 3750: loss 0.9066, time 12930.37ms, mfu 2.74%\n",
            "iter 3760: loss 0.9350, time 120.16ms, mfu 2.78%\n",
            "iter 3770: loss 0.9320, time 121.75ms, mfu 2.81%\n",
            "iter 3780: loss 0.9233, time 120.87ms, mfu 2.84%\n",
            "iter 3790: loss 0.9007, time 121.67ms, mfu 2.86%\n",
            "iter 3800: loss 0.9027, time 122.74ms, mfu 2.88%\n",
            "iter 3810: loss 0.9152, time 122.55ms, mfu 2.89%\n",
            "iter 3820: loss 0.8884, time 120.54ms, mfu 2.91%\n",
            "iter 3830: loss 0.9023, time 124.12ms, mfu 2.92%\n",
            "iter 3840: loss 0.8860, time 123.29ms, mfu 2.93%\n",
            "iter 3850: loss 0.8922, time 120.07ms, mfu 2.95%\n",
            "iter 3860: loss 0.8694, time 123.09ms, mfu 2.96%\n",
            "iter 3870: loss 0.8979, time 122.02ms, mfu 2.97%\n",
            "iter 3880: loss 0.8883, time 119.82ms, mfu 2.98%\n",
            "iter 3890: loss 0.8921, time 123.36ms, mfu 2.98%\n",
            "iter 3900: loss 0.8796, time 120.61ms, mfu 3.00%\n",
            "iter 3910: loss 0.8910, time 123.93ms, mfu 3.00%\n",
            "iter 3920: loss 0.8786, time 121.49ms, mfu 3.00%\n",
            "iter 3930: loss 0.8860, time 122.38ms, mfu 3.01%\n",
            "iter 3940: loss 0.8757, time 120.69ms, mfu 3.02%\n",
            "iter 3950: loss 0.8747, time 122.06ms, mfu 3.02%\n",
            "iter 3960: loss 0.9120, time 121.97ms, mfu 3.02%\n",
            "iter 3970: loss 0.8914, time 120.87ms, mfu 3.03%\n",
            "iter 3980: loss 0.9046, time 119.26ms, mfu 3.04%\n",
            "iter 3990: loss 0.8761, time 122.49ms, mfu 3.04%\n",
            "step 4000: train loss 0.7089, val loss 1.6181\n",
            "iter 4000: loss 0.8590, time 12920.68ms, mfu 2.74%\n",
            "iter 4010: loss 0.8836, time 122.83ms, mfu 2.77%\n",
            "iter 4020: loss 0.8819, time 120.97ms, mfu 2.80%\n",
            "iter 4030: loss 0.8776, time 121.17ms, mfu 2.83%\n",
            "iter 4040: loss 0.8846, time 123.56ms, mfu 2.85%\n",
            "iter 4050: loss 0.8734, time 121.22ms, mfu 2.87%\n",
            "iter 4060: loss 0.8648, time 120.50ms, mfu 2.89%\n",
            "iter 4070: loss 0.8631, time 123.24ms, mfu 2.90%\n",
            "iter 4080: loss 0.8867, time 122.60ms, mfu 2.92%\n",
            "iter 4090: loss 0.8479, time 122.79ms, mfu 2.93%\n",
            "iter 4100: loss 0.8987, time 121.98ms, mfu 2.94%\n",
            "iter 4110: loss 0.8641, time 124.10ms, mfu 2.95%\n",
            "iter 4120: loss 0.8797, time 119.98ms, mfu 2.96%\n",
            "iter 4130: loss 0.8548, time 122.44ms, mfu 2.97%\n",
            "iter 4140: loss 0.8778, time 120.62ms, mfu 2.98%\n",
            "iter 4150: loss 0.8723, time 124.48ms, mfu 2.98%\n",
            "iter 4160: loss 0.8512, time 121.46ms, mfu 2.99%\n",
            "iter 4170: loss 0.8705, time 119.55ms, mfu 3.01%\n",
            "iter 4180: loss 0.8739, time 120.53ms, mfu 3.01%\n",
            "iter 4190: loss 0.8654, time 120.10ms, mfu 3.02%\n",
            "iter 4200: loss 0.8579, time 121.41ms, mfu 3.03%\n",
            "iter 4210: loss 0.8754, time 122.02ms, mfu 3.03%\n",
            "iter 4220: loss 0.8595, time 120.13ms, mfu 3.04%\n",
            "iter 4230: loss 0.8805, time 121.27ms, mfu 3.04%\n",
            "iter 4240: loss 0.8654, time 125.00ms, mfu 3.03%\n",
            "step 4250: train loss 0.6784, val loss 1.6442\n",
            "iter 4250: loss 0.8753, time 12971.90ms, mfu 2.73%\n",
            "iter 4260: loss 0.8598, time 121.98ms, mfu 2.77%\n",
            "iter 4270: loss 0.8634, time 123.85ms, mfu 2.79%\n",
            "iter 4280: loss 0.8539, time 120.59ms, mfu 2.82%\n",
            "iter 4290: loss 0.8318, time 124.14ms, mfu 2.84%\n",
            "iter 4300: loss 0.8307, time 119.36ms, mfu 2.87%\n",
            "iter 4310: loss 0.8535, time 122.13ms, mfu 2.89%\n",
            "iter 4320: loss 0.8466, time 122.94ms, mfu 2.90%\n",
            "iter 4330: loss 0.8621, time 121.72ms, mfu 2.92%\n",
            "iter 4340: loss 0.8286, time 120.55ms, mfu 2.93%\n",
            "iter 4350: loss 0.8388, time 122.68ms, mfu 2.94%\n",
            "iter 4360: loss 0.8612, time 120.94ms, mfu 2.96%\n",
            "iter 4370: loss 0.8579, time 121.67ms, mfu 2.97%\n",
            "iter 4380: loss 0.8347, time 122.28ms, mfu 2.98%\n",
            "iter 4390: loss 0.8672, time 122.80ms, mfu 2.98%\n",
            "iter 4400: loss 0.8440, time 122.04ms, mfu 2.99%\n",
            "iter 4410: loss 0.8630, time 120.71ms, mfu 3.00%\n",
            "iter 4420: loss 0.8611, time 120.82ms, mfu 3.01%\n",
            "iter 4430: loss 0.8399, time 121.04ms, mfu 3.01%\n",
            "iter 4440: loss 0.8514, time 123.13ms, mfu 3.02%\n",
            "iter 4450: loss 0.8549, time 125.24ms, mfu 3.01%\n",
            "iter 4460: loss 0.8366, time 120.26ms, mfu 3.02%\n",
            "iter 4470: loss 0.8530, time 123.37ms, mfu 3.02%\n",
            "iter 4480: loss 0.8332, time 120.94ms, mfu 3.03%\n",
            "iter 4490: loss 0.8419, time 125.16ms, mfu 3.02%\n",
            "step 4500: train loss 0.6544, val loss 1.6613\n",
            "iter 4500: loss 0.8533, time 13010.56ms, mfu 2.72%\n",
            "iter 4510: loss 0.8460, time 121.52ms, mfu 2.76%\n",
            "iter 4520: loss 0.8332, time 124.06ms, mfu 2.78%\n",
            "iter 4530: loss 0.8519, time 120.21ms, mfu 2.81%\n",
            "iter 4540: loss 0.8431, time 121.88ms, mfu 2.84%\n",
            "iter 4550: loss 0.8768, time 120.44ms, mfu 2.86%\n",
            "iter 4560: loss 0.8361, time 121.86ms, mfu 2.88%\n",
            "iter 4570: loss 0.8384, time 121.69ms, mfu 2.90%\n",
            "iter 4580: loss 0.8528, time 122.46ms, mfu 2.91%\n",
            "iter 4590: loss 0.8554, time 121.13ms, mfu 2.93%\n",
            "iter 4600: loss 0.8282, time 123.90ms, mfu 2.94%\n",
            "iter 4610: loss 0.8602, time 124.43ms, mfu 2.94%\n",
            "iter 4620: loss 0.8341, time 121.98ms, mfu 2.96%\n",
            "iter 4630: loss 0.8136, time 119.68ms, mfu 2.97%\n",
            "iter 4640: loss 0.8465, time 120.13ms, mfu 2.98%\n",
            "iter 4650: loss 0.8606, time 121.41ms, mfu 2.99%\n",
            "iter 4660: loss 0.8533, time 123.75ms, mfu 2.99%\n",
            "iter 4670: loss 0.8358, time 125.38ms, mfu 2.99%\n",
            "iter 4680: loss 0.8533, time 125.01ms, mfu 2.99%\n",
            "iter 4690: loss 0.8411, time 121.54ms, mfu 3.00%\n",
            "iter 4700: loss 0.8206, time 122.71ms, mfu 3.00%\n",
            "iter 4710: loss 0.7934, time 124.31ms, mfu 3.00%\n",
            "iter 4720: loss 0.8374, time 121.47ms, mfu 3.01%\n",
            "iter 4730: loss 0.8223, time 120.44ms, mfu 3.02%\n",
            "iter 4740: loss 0.8268, time 119.99ms, mfu 3.03%\n",
            "step 4750: train loss 0.6359, val loss 1.6802\n",
            "iter 4750: loss 0.8078, time 12964.86ms, mfu 2.73%\n",
            "iter 4760: loss 0.8218, time 120.94ms, mfu 2.76%\n",
            "iter 4770: loss 0.8055, time 122.22ms, mfu 2.79%\n",
            "iter 4780: loss 0.8098, time 122.11ms, mfu 2.82%\n",
            "iter 4790: loss 0.8343, time 120.34ms, mfu 2.84%\n",
            "iter 4800: loss 0.8281, time 121.64ms, mfu 2.87%\n",
            "iter 4810: loss 0.8343, time 122.82ms, mfu 2.88%\n",
            "iter 4820: loss 0.8217, time 120.77ms, mfu 2.90%\n",
            "iter 4830: loss 0.8183, time 118.91ms, mfu 2.93%\n",
            "iter 4840: loss 0.8372, time 121.27ms, mfu 2.94%\n",
            "iter 4850: loss 0.8202, time 122.64ms, mfu 2.95%\n",
            "iter 4860: loss 0.8241, time 120.26ms, mfu 2.97%\n",
            "iter 4870: loss 0.8103, time 121.76ms, mfu 2.97%\n",
            "iter 4880: loss 0.8312, time 121.85ms, mfu 2.98%\n",
            "iter 4890: loss 0.8079, time 120.45ms, mfu 2.99%\n",
            "iter 4900: loss 0.8054, time 122.71ms, mfu 3.00%\n",
            "iter 4910: loss 0.8333, time 121.00ms, mfu 3.01%\n",
            "iter 4920: loss 0.8248, time 121.29ms, mfu 3.01%\n",
            "iter 4930: loss 0.8068, time 123.53ms, mfu 3.01%\n",
            "iter 4940: loss 0.8003, time 122.40ms, mfu 3.02%\n",
            "iter 4950: loss 0.8276, time 124.20ms, mfu 3.01%\n",
            "iter 4960: loss 0.8309, time 122.10ms, mfu 3.02%\n",
            "iter 4970: loss 0.7909, time 120.75ms, mfu 3.03%\n",
            "iter 4980: loss 0.7935, time 122.88ms, mfu 3.03%\n",
            "iter 4990: loss 0.8205, time 123.50ms, mfu 3.03%\n",
            "step 5000: train loss 0.6210, val loss 1.7005\n",
            "iter 5000: loss 0.8196, time 12917.23ms, mfu 2.73%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python sample.py --out_dir=out-shakespeare-char --start=\"star \""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWb71dfyOe7e",
        "outputId": "0f68ed36-fb03-4425-cc6c-15ad8a758a17"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: out_dir = out-shakespeare-char\n",
            "Overriding: start = star \n",
            "number of parameters: 10.65M\n",
            "Loading meta from data/shakespeare_char/meta.pkl...\n",
            "star the time of whom and is not\n",
            "three servants and all ready that hangs art\n",
            "that uses the tracterlessand are now an answer'd of mine\n",
            "what to the heart of hell.\n",
            "\n",
            "ROMEO:\n",
            "Yet I have late made overthrow of love,\n",
            "Will I see love to ear.\n",
            "\n",
            "Nurse:\n",
            "I have been a soldier, and too much\n",
            "To this despised sword. Good morrow, good Capitol!\n",
            "Thou dost say it, it is poor great to\n",
            "That dangerous for traitor! must I do along.\n",
            "\n",
            "Nurse:\n",
            "Well, how thou wert!\n",
            "\n",
            "JULIET:\n",
            "What a curse of the dishonour?\n",
            "\n",
            "Nurse:\n",
            "Go see the frown,\n",
            "---------------\n",
            "star with the king shall be so graced of all.\n",
            "Where is my heart is ready? when he has the world\n",
            "Is no more free than a case, more in my thumb.\n",
            "\n",
            "SLY:\n",
            "See the heavens of my soul words in meeting.\n",
            "\n",
            "DUKE OF AUMERLE:\n",
            "He do not commit me to the ground.\n",
            "\n",
            "KING RICHARD II:\n",
            "But a woman from the walls of his self;\n",
            "For the wolf have in himself I see thee.\n",
            "\n",
            "KING RICHARD II:\n",
            "Call your grace in impostion.\n",
            "\n",
            "DUKE OF YORK:\n",
            "I thank you. thou wilt say this most man carried\n",
            "For this man that it will be set upon thee,\n",
            "And\n",
            "---------------\n",
            "star we roar, will we are,\n",
            "We two conceive to the very hear of the cold\n",
            "That thou hast put our sweet wis sweet words.\n",
            "\n",
            "MARIANA:\n",
            "No, my lord,\n",
            "Henry must be so.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "This is a polly that ever known him!\n",
            "\n",
            "ISABELLA:\n",
            "I hide you, not my hearted to grieve a determined.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Ay, true of the world.\n",
            "\n",
            "ANGELO:\n",
            "So much he is here but slain in his head;\n",
            "And there late you come so.\n",
            "\n",
            "ISABELLA:\n",
            "Here in the gates villanor horse;\n",
            "And the number of mine enemy.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "My soul, wholesomet\n",
            "---------------\n",
            "star not so far\n",
            "In the king, and he seems and sweet as virtuous\n",
            "Could prove of when he was not to revenge\n",
            "But right on my Lartius.\n",
            "\n",
            "MARIANA:\n",
            "'Tis dead; a royal marvellous maid.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Marry, sir, I have seen him a land of lest\n",
            "A wife in his majesty: he could have here, here's\n",
            "a tale work of her banishments, I have but relied\n",
            "Unto 'like her unscorn.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Then will say, thy prosperous wars,\n",
            "That Jove hath been a king so here.\n",
            "\n",
            "Nurse:\n",
            "So shall I fight him so much for her be a half?\n",
            "---------------\n",
            "star site,\n",
            "And three the bloody of Tybalt's last.\n",
            "\n",
            "KING RICHARD II:\n",
            "Why she, the two executioners hath been slain,\n",
            "That stroke to conjure this adversaries\n",
            "Are offended in this docted more\n",
            "And bear the people is come to make his most\n",
            "Than presently of charge, that chamber'd\n",
            "All many that which a something does\n",
            "To anciently have touch'd. Alas, noble lord!\n",
            "Here is shame my father's son and honour\n",
            "That wayward i' the air: she was overboard\n",
            "A piper of her death, and her defills,\n",
            "Her best officers, and for\n",
            "---------------\n",
            "star in him, like to bear him for the rest\n",
            "Affection and make him talk of his peace; he would\n",
            "Have deserved of death shall be thought that run or more.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "I would you got you well, sir, if you have\n",
            "Hold that every more, you have been a guard of this,\n",
            "Which noise the issue of the more provided.\n",
            "\n",
            "Provost:\n",
            "I will not be not a feed of my brow, and the\n",
            "servant of God's name word will answer with the Capitol.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "I say, it is the gods of it, and therefore\n",
            "to see him for Rome is t\n",
            "---------------\n",
            "star shall be well appeard'd,\n",
            "And filling she, when in the royal issue,\n",
            "He is all the else other shall the air.\n",
            "\n",
            "GLOUCESTER:\n",
            "No, not to the Lord Warwick's title of our bahk;\n",
            "But she shall be long like that love of death;\n",
            "And her with a noble contract when they have but a cause,\n",
            "To save war frown'd interchance of their worship.\n",
            "\n",
            "GLOUCESTER:\n",
            "\n",
            "HASTINGS:\n",
            "In the oracle them on the speech or happiness.\n",
            "\n",
            "YORK:\n",
            "Here comes, my lord, your majesty\n",
            "Is not so grown to our fortune: be it strange all.\n",
            "\n",
            "GEORGE:\n",
            "But \n",
            "---------------\n",
            "star late,\n",
            "Hold me long not fellows to the saucy of death,\n",
            "I say 'twere a part of the east, and they are born\n",
            "And play'd your power in place, beholding in enterials;\n",
            "And to use your guilty and obidious\n",
            "Show your highness' honour, and to such a brother\n",
            "With silker blood as reason tears their heads,\n",
            "And here was quitted withal! Against the varlets\n",
            "Of their own throne with the power. The thoughts\n",
            "And his eye the forehead the same other there\n",
            "Within the advantage of those stranged war: but, in the worls\n",
            "\n",
            "---------------\n",
            "star have purchased\n",
            "My gracious master and play'd them like the sky\n",
            "Endured with her! Go, go, girls! well have heed\n",
            "The statue of the hope of his soldiers,\n",
            "Strength of his foul may have put upon them\n",
            "To the dangerous man. This silent and reason\n",
            "To have sure of many words: but it were my noble lesser\n",
            "Shall not be cause to first, and, as if they great\n",
            "I have touch'd him with their grief, and all the gooses\n",
            "That nest they will not love them. As I am so,\n",
            "As I can under him, as I can desire\n",
            "The parts in t\n",
            "---------------\n",
            "star with her body:\n",
            "I respect not; my lord, for that I am here is\n",
            "to enbroke my son: I do not know the tears of such a taste\n",
            "as best your terril\n",
            "Is thus. If it be not so a woman's fear,\n",
            "As you are not but your doubt is a world,\n",
            "I am of royal disloyal.\n",
            "\n",
            "MENENIUS:\n",
            "This is offence.\n",
            "\n",
            "MENENIUS:\n",
            "You have said the people, it is in war\n",
            "To the name of yourselves; but the varlet-winds be\n",
            "As the consent your times and speak of the parties\n",
            "With remove of his side. He thinks you, sir,\n",
            "The vaultain of Polixenes\n",
            "An\n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Song Writer"
      ],
      "metadata": {
        "id": "88DqCYjYWUMP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python data/lyrics/prepare.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQTryJ-_SJ4C",
        "outputId": "eb9ff4ce-eb61-47bf-e3bf-d98fd8d8331b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train has 22,308,928 tokens\n",
            "val has 2,456,916 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py config/train_lyrics.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0XI4S2tWvoI",
        "outputId": "bd9fe051-3238-456c-8b04-a2870e062999"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding config with config/train_lyrics.py:\n",
            "# train a miniature character-level shakespeare model\n",
            "# good for debugging and playing on macbooks and such\n",
            "\n",
            "out_dir = 'out-lyrics'\n",
            "eval_interval = 250 # keep frequent because we'll overfit\n",
            "eval_iters = 200\n",
            "log_interval = 10 # don't print too too often\n",
            "\n",
            "# we expect to overfit on this small dataset, so only save when val improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "wandb_log = False # override via command line if you like\n",
            "wandb_project = 'lyrics'\n",
            "wandb_run_name = 'mini-gpt'\n",
            "\n",
            "dataset = 'lyrics'\n",
            "gradient_accumulation_steps = 1\n",
            "batch_size = 64\n",
            "block_size = 256 # context of up to 256 previous characters\n",
            "\n",
            "# baby GPT model :)\n",
            "n_layer = 6\n",
            "n_head = 6\n",
            "n_embd = 384\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
            "max_iters = 5000\n",
            "lr_decay_iters = 5000 # make equal to max_iters usually\n",
            "min_lr = 1e-4 # learning_rate / 10 usually\n",
            "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
            "\n",
            "warmup_iters = 100 # not super necessary potentially\n",
            "\n",
            "# on macbook also add\n",
            "# device = 'cpu'  # run on cpu only\n",
            "# compile = False # do not torch compile the model\n",
            "\n",
            "tokens per iteration will be: 16,384\n",
            "Initializing a new model from scratch\n",
            "defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\n",
            "number of parameters: 29.94M\n",
            "num decayed parameter tensors: 26, with 30,031,872 parameters\n",
            "num non-decayed parameter tensors: 13, with 4,992 parameters\n",
            "using fused AdamW: True\n",
            "compiling the model... (takes a ~minute)\n",
            "step 0: train loss 10.7094, val loss 10.7105\n",
            "[2023-09-18 08:16:30,237] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-18 08:16:30,520] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-18 08:16:30,971] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-18 08:16:31,245] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-18 08:16:31,649] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-18 08:16:31,910] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-18 08:16:32,316] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-18 08:16:32,580] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-18 08:16:32,978] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-18 08:16:33,257] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-18 08:16:33,779] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-18 08:16:34,212] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "iter 0: loss 10.7344, time 41046.71ms, mfu -100.00%\n",
            "iter 10: loss 8.8351, time 320.85ms, mfu 3.06%\n",
            "iter 20: loss 7.9036, time 325.02ms, mfu 3.05%\n",
            "iter 30: loss 6.5586, time 318.31ms, mfu 3.05%\n",
            "iter 40: loss 5.4099, time 334.40ms, mfu 3.04%\n",
            "iter 50: loss 4.6167, time 327.02ms, mfu 3.04%\n",
            "iter 60: loss 4.4833, time 324.08ms, mfu 3.04%\n",
            "iter 70: loss 4.3515, time 324.92ms, mfu 3.03%\n",
            "iter 80: loss 4.2454, time 334.37ms, mfu 3.02%\n",
            "iter 90: loss 4.0503, time 327.50ms, mfu 3.02%\n",
            "iter 100: loss 4.0226, time 338.52ms, mfu 3.01%\n",
            "iter 110: loss 3.9475, time 331.29ms, mfu 3.00%\n",
            "iter 120: loss 3.8612, time 339.31ms, mfu 2.99%\n",
            "iter 130: loss 3.8511, time 338.07ms, mfu 2.98%\n",
            "iter 140: loss 3.9758, time 340.25ms, mfu 2.97%\n",
            "iter 150: loss 3.8317, time 346.89ms, mfu 2.96%\n",
            "iter 160: loss 3.6626, time 341.15ms, mfu 2.95%\n",
            "iter 170: loss 3.6629, time 344.59ms, mfu 2.94%\n",
            "iter 180: loss 3.6852, time 344.30ms, mfu 2.93%\n",
            "iter 190: loss 3.7087, time 356.28ms, mfu 2.91%\n",
            "iter 200: loss 3.7163, time 348.57ms, mfu 2.90%\n",
            "iter 210: loss 3.6334, time 347.42ms, mfu 2.89%\n",
            "iter 220: loss 3.6333, time 346.64ms, mfu 2.89%\n",
            "iter 230: loss 3.6447, time 355.06ms, mfu 2.88%\n",
            "iter 240: loss 3.7147, time 349.23ms, mfu 2.87%\n",
            "step 250: train loss 3.5135, val loss 3.5959\n",
            "saving checkpoint to out-lyrics\n",
            "iter 250: loss 3.5746, time 50061.95ms, mfu 2.58%\n",
            "iter 260: loss 3.6605, time 326.49ms, mfu 2.63%\n",
            "iter 270: loss 3.4628, time 380.18ms, mfu 2.62%\n",
            "iter 280: loss 3.6133, time 329.74ms, mfu 2.66%\n",
            "iter 290: loss 3.5141, time 327.69ms, mfu 2.69%\n",
            "iter 300: loss 3.4140, time 383.14ms, mfu 2.68%\n",
            "iter 310: loss 3.4340, time 327.70ms, mfu 2.71%\n",
            "iter 320: loss 3.4435, time 332.16ms, mfu 2.73%\n",
            "iter 330: loss 3.3476, time 357.10ms, mfu 2.73%\n",
            "iter 340: loss 3.4221, time 338.78ms, mfu 2.75%\n",
            "iter 350: loss 3.3921, time 344.20ms, mfu 2.76%\n",
            "iter 360: loss 3.3331, time 373.93ms, mfu 2.75%\n",
            "iter 370: loss 3.2886, time 325.09ms, mfu 2.77%\n",
            "iter 380: loss 3.2997, time 341.37ms, mfu 2.78%\n",
            "iter 390: loss 3.3286, time 344.35ms, mfu 2.79%\n",
            "iter 400: loss 3.3993, time 339.82ms, mfu 2.80%\n",
            "iter 410: loss 3.3082, time 341.00ms, mfu 2.81%\n",
            "iter 420: loss 3.1560, time 343.43ms, mfu 2.81%\n",
            "iter 430: loss 3.3345, time 340.16ms, mfu 2.82%\n",
            "iter 440: loss 3.2852, time 335.07ms, mfu 2.83%\n",
            "iter 450: loss 3.1445, time 366.40ms, mfu 2.81%\n",
            "iter 460: loss 3.2252, time 331.76ms, mfu 2.83%\n",
            "iter 470: loss 3.3621, time 326.08ms, mfu 2.85%\n",
            "iter 480: loss 3.3060, time 367.75ms, mfu 2.83%\n",
            "iter 490: loss 3.2731, time 331.21ms, mfu 2.84%\n",
            "step 500: train loss 3.1541, val loss 3.2681\n",
            "saving checkpoint to out-lyrics\n",
            "iter 500: loss 3.2678, time 51567.99ms, mfu 2.56%\n",
            "iter 510: loss 3.1221, time 350.06ms, mfu 2.58%\n",
            "iter 520: loss 3.2343, time 328.55ms, mfu 2.62%\n",
            "iter 530: loss 3.2545, time 339.64ms, mfu 2.65%\n",
            "iter 540: loss 3.1669, time 346.22ms, mfu 2.67%\n",
            "iter 550: loss 3.1681, time 367.37ms, mfu 2.67%\n",
            "iter 560: loss 3.1808, time 340.11ms, mfu 2.69%\n",
            "iter 570: loss 2.9993, time 338.49ms, mfu 2.71%\n",
            "iter 580: loss 3.0684, time 340.58ms, mfu 2.73%\n",
            "iter 590: loss 3.1672, time 355.79ms, mfu 2.73%\n",
            "iter 600: loss 3.1063, time 336.05ms, mfu 2.75%\n",
            "iter 610: loss 3.1796, time 349.95ms, mfu 2.75%\n",
            "iter 620: loss 3.1487, time 336.14ms, mfu 2.77%\n",
            "iter 630: loss 3.2595, time 333.89ms, mfu 2.79%\n",
            "iter 640: loss 3.1039, time 353.38ms, mfu 2.79%\n",
            "iter 650: loss 3.1260, time 328.52ms, mfu 2.81%\n",
            "iter 660: loss 3.1743, time 336.96ms, mfu 2.82%\n",
            "iter 670: loss 3.0398, time 338.50ms, mfu 2.82%\n",
            "iter 680: loss 3.0256, time 339.70ms, mfu 2.83%\n",
            "iter 690: loss 3.1837, time 340.62ms, mfu 2.84%\n",
            "iter 700: loss 3.1311, time 339.45ms, mfu 2.84%\n",
            "iter 710: loss 3.0578, time 344.98ms, mfu 2.84%\n",
            "iter 720: loss 2.9843, time 339.52ms, mfu 2.85%\n",
            "iter 730: loss 2.8871, time 346.02ms, mfu 2.84%\n",
            "iter 740: loss 3.1386, time 336.84ms, mfu 2.85%\n",
            "step 750: train loss 2.9509, val loss 3.0794\n",
            "saving checkpoint to out-lyrics\n",
            "iter 750: loss 3.0699, time 52654.11ms, mfu 2.57%\n",
            "iter 760: loss 3.1355, time 334.29ms, mfu 2.60%\n",
            "iter 770: loss 3.0461, time 371.87ms, mfu 2.61%\n",
            "iter 780: loss 3.0548, time 341.79ms, mfu 2.63%\n",
            "iter 790: loss 3.0981, time 338.30ms, mfu 2.66%\n",
            "iter 800: loss 2.9545, time 344.28ms, mfu 2.68%\n",
            "iter 810: loss 2.9950, time 340.53ms, mfu 2.70%\n",
            "iter 820: loss 3.0006, time 339.67ms, mfu 2.72%\n",
            "iter 830: loss 3.0107, time 342.46ms, mfu 2.73%\n",
            "iter 840: loss 2.9669, time 345.30ms, mfu 2.74%\n",
            "iter 850: loss 2.9816, time 344.66ms, mfu 2.75%\n",
            "iter 860: loss 2.9119, time 345.32ms, mfu 2.76%\n",
            "iter 870: loss 2.8690, time 342.44ms, mfu 2.77%\n",
            "iter 880: loss 2.8908, time 342.15ms, mfu 2.78%\n",
            "iter 890: loss 3.0243, time 338.79ms, mfu 2.79%\n",
            "iter 900: loss 2.8693, time 334.63ms, mfu 2.81%\n",
            "iter 910: loss 2.8886, time 341.04ms, mfu 2.81%\n",
            "iter 920: loss 2.7711, time 351.01ms, mfu 2.81%\n",
            "iter 930: loss 2.7693, time 335.38ms, mfu 2.82%\n",
            "iter 940: loss 2.9965, time 344.23ms, mfu 2.82%\n",
            "iter 950: loss 2.9714, time 346.26ms, mfu 2.83%\n",
            "iter 960: loss 2.8569, time 348.87ms, mfu 2.82%\n",
            "iter 970: loss 2.8247, time 337.08ms, mfu 2.83%\n",
            "iter 980: loss 2.8812, time 340.41ms, mfu 2.84%\n",
            "iter 990: loss 2.8017, time 336.47ms, mfu 2.84%\n",
            "step 1000: train loss 2.7322, val loss 2.8888\n",
            "saving checkpoint to out-lyrics\n",
            "iter 1000: loss 2.9430, time 52591.21ms, mfu 2.56%\n",
            "iter 1010: loss 2.9367, time 339.19ms, mfu 2.60%\n",
            "iter 1020: loss 2.8849, time 341.91ms, mfu 2.62%\n",
            "iter 1030: loss 3.0091, time 338.93ms, mfu 2.65%\n",
            "iter 1040: loss 2.7551, time 342.43ms, mfu 2.67%\n",
            "iter 1050: loss 2.8007, time 346.48ms, mfu 2.69%\n",
            "iter 1060: loss 2.7279, time 346.88ms, mfu 2.70%\n",
            "iter 1070: loss 2.7244, time 347.01ms, mfu 2.71%\n",
            "iter 1080: loss 2.8975, time 342.70ms, mfu 2.73%\n",
            "iter 1090: loss 2.8577, time 346.12ms, mfu 2.74%\n",
            "iter 1100: loss 2.8054, time 342.48ms, mfu 2.75%\n",
            "iter 1110: loss 2.7002, time 342.24ms, mfu 2.76%\n",
            "iter 1120: loss 2.8135, time 346.81ms, mfu 2.77%\n",
            "iter 1130: loss 2.7613, time 339.38ms, mfu 2.78%\n",
            "iter 1140: loss 2.8399, time 344.25ms, mfu 2.79%\n",
            "iter 1150: loss 2.7849, time 338.62ms, mfu 2.80%\n",
            "iter 1160: loss 2.7821, time 341.98ms, mfu 2.81%\n",
            "iter 1170: loss 2.8328, time 339.99ms, mfu 2.81%\n",
            "iter 1180: loss 2.5968, time 350.28ms, mfu 2.81%\n",
            "iter 1190: loss 2.8828, time 328.63ms, mfu 2.83%\n",
            "iter 1200: loss 2.8196, time 335.99ms, mfu 2.84%\n",
            "iter 1210: loss 2.7753, time 340.04ms, mfu 2.84%\n",
            "iter 1220: loss 2.7540, time 340.14ms, mfu 2.85%\n",
            "iter 1230: loss 2.8093, time 335.83ms, mfu 2.85%\n",
            "iter 1240: loss 2.6669, time 342.96ms, mfu 2.85%\n",
            "step 1250: train loss 2.5986, val loss 2.7550\n",
            "saving checkpoint to out-lyrics\n",
            "iter 1250: loss 2.7173, time 52657.26ms, mfu 2.57%\n",
            "iter 1260: loss 2.7580, time 342.77ms, mfu 2.60%\n",
            "iter 1270: loss 2.7758, time 344.02ms, mfu 2.62%\n",
            "iter 1280: loss 2.9503, time 327.64ms, mfu 2.66%\n",
            "iter 1290: loss 2.6431, time 332.63ms, mfu 2.69%\n",
            "iter 1300: loss 2.6463, time 376.61ms, mfu 2.68%\n",
            "iter 1310: loss 2.7660, time 341.64ms, mfu 2.70%\n",
            "iter 1320: loss 2.5831, time 342.23ms, mfu 2.72%\n",
            "iter 1330: loss 2.6424, time 350.03ms, mfu 2.73%\n",
            "iter 1340: loss 2.6138, time 346.37ms, mfu 2.74%\n",
            "iter 1350: loss 2.5950, time 343.32ms, mfu 2.75%\n",
            "iter 1360: loss 2.5652, time 342.45ms, mfu 2.76%\n",
            "iter 1370: loss 2.7021, time 339.14ms, mfu 2.77%\n",
            "iter 1380: loss 2.7181, time 337.32ms, mfu 2.79%\n",
            "iter 1390: loss 2.5449, time 344.94ms, mfu 2.79%\n",
            "iter 1400: loss 2.7111, time 333.75ms, mfu 2.81%\n",
            "iter 1410: loss 2.7260, time 331.97ms, mfu 2.82%\n",
            "iter 1420: loss 2.7248, time 331.86ms, mfu 2.83%\n",
            "iter 1430: loss 2.5817, time 330.74ms, mfu 2.85%\n",
            "iter 1440: loss 2.6341, time 361.60ms, mfu 2.83%\n",
            "iter 1450: loss 2.6330, time 336.46ms, mfu 2.84%\n",
            "iter 1460: loss 2.6728, time 343.19ms, mfu 2.84%\n",
            "iter 1470: loss 2.6687, time 363.81ms, mfu 2.83%\n",
            "iter 1480: loss 2.6855, time 334.75ms, mfu 2.84%\n",
            "iter 1490: loss 2.7519, time 334.96ms, mfu 2.85%\n",
            "step 1500: train loss 2.5136, val loss 2.6913\n",
            "saving checkpoint to out-lyrics\n",
            "iter 1500: loss 2.8112, time 52749.73ms, mfu 2.56%\n",
            "iter 1510: loss 2.7038, time 339.18ms, mfu 2.60%\n",
            "iter 1520: loss 2.7971, time 350.13ms, mfu 2.62%\n",
            "iter 1530: loss 2.6913, time 333.03ms, mfu 2.65%\n",
            "iter 1540: loss 2.6950, time 335.23ms, mfu 2.68%\n",
            "iter 1550: loss 2.7061, time 340.85ms, mfu 2.70%\n",
            "iter 1560: loss 2.5247, time 340.83ms, mfu 2.72%\n",
            "iter 1570: loss 2.5860, time 340.63ms, mfu 2.73%\n",
            "iter 1580: loss 2.6237, time 345.14ms, mfu 2.74%\n",
            "iter 1590: loss 2.6162, time 340.75ms, mfu 2.76%\n",
            "iter 1600: loss 2.6719, time 346.69ms, mfu 2.76%\n",
            "iter 1610: loss 2.4868, time 337.59ms, mfu 2.78%\n",
            "iter 1620: loss 2.7224, time 344.90ms, mfu 2.78%\n",
            "iter 1630: loss 2.6351, time 339.21ms, mfu 2.79%\n",
            "iter 1640: loss 2.5558, time 343.06ms, mfu 2.80%\n",
            "iter 1650: loss 2.3924, time 337.70ms, mfu 2.81%\n",
            "iter 1660: loss 2.5250, time 340.07ms, mfu 2.82%\n",
            "iter 1670: loss 2.3957, time 333.57ms, mfu 2.83%\n",
            "iter 1680: loss 2.4747, time 342.43ms, mfu 2.83%\n",
            "iter 1690: loss 2.6202, time 343.94ms, mfu 2.84%\n",
            "iter 1700: loss 2.6609, time 344.06ms, mfu 2.84%\n",
            "iter 1710: loss 2.6317, time 334.03ms, mfu 2.85%\n",
            "iter 1720: loss 2.5957, time 331.79ms, mfu 2.86%\n",
            "iter 1730: loss 2.6875, time 336.71ms, mfu 2.86%\n",
            "iter 1740: loss 2.4648, time 333.40ms, mfu 2.87%\n",
            "step 1750: train loss 2.4607, val loss 2.6298\n",
            "saving checkpoint to out-lyrics\n",
            "iter 1750: loss 2.6471, time 52730.87ms, mfu 2.59%\n",
            "iter 1760: loss 2.5462, time 340.36ms, mfu 2.62%\n",
            "iter 1770: loss 2.6703, time 345.97ms, mfu 2.64%\n",
            "iter 1780: loss 2.4086, time 330.44ms, mfu 2.67%\n",
            "iter 1790: loss 2.4923, time 328.17ms, mfu 2.70%\n",
            "iter 1800: loss 2.4133, time 345.62ms, mfu 2.72%\n",
            "iter 1810: loss 2.7201, time 345.17ms, mfu 2.73%\n",
            "iter 1820: loss 2.7000, time 341.85ms, mfu 2.74%\n",
            "iter 1830: loss 2.5246, time 348.55ms, mfu 2.75%\n",
            "iter 1840: loss 2.6345, time 341.84ms, mfu 2.76%\n",
            "iter 1850: loss 2.4357, time 347.15ms, mfu 2.77%\n",
            "iter 1860: loss 2.5837, time 342.41ms, mfu 2.78%\n",
            "iter 1870: loss 2.5488, time 344.43ms, mfu 2.78%\n",
            "iter 1880: loss 2.4707, time 342.31ms, mfu 2.79%\n",
            "iter 1890: loss 2.4749, time 341.65ms, mfu 2.80%\n",
            "iter 1900: loss 2.5066, time 338.02ms, mfu 2.81%\n",
            "iter 1910: loss 2.5349, time 340.34ms, mfu 2.82%\n",
            "iter 1920: loss 2.5040, time 343.27ms, mfu 2.82%\n",
            "iter 1930: loss 2.4854, time 353.21ms, mfu 2.82%\n",
            "iter 1940: loss 2.5770, time 340.94ms, mfu 2.82%\n",
            "iter 1950: loss 2.5570, time 337.03ms, mfu 2.83%\n",
            "iter 1960: loss 2.5127, time 330.83ms, mfu 2.84%\n",
            "iter 1970: loss 2.4670, time 350.94ms, mfu 2.84%\n",
            "iter 1980: loss 2.4477, time 350.25ms, mfu 2.84%\n",
            "iter 1990: loss 2.5474, time 343.27ms, mfu 2.84%\n",
            "step 2000: train loss 2.4091, val loss 2.5847\n",
            "saving checkpoint to out-lyrics\n",
            "iter 2000: loss 2.5101, time 48055.79ms, mfu 2.56%\n",
            "iter 2010: loss 2.4728, time 328.40ms, mfu 2.60%\n",
            "iter 2020: loss 2.4346, time 380.18ms, mfu 2.60%\n",
            "iter 2030: loss 2.5549, time 334.44ms, mfu 2.63%\n",
            "iter 2040: loss 2.3968, time 329.73ms, mfu 2.66%\n",
            "iter 2050: loss 2.4051, time 365.54ms, mfu 2.67%\n",
            "iter 2060: loss 2.4879, time 330.58ms, mfu 2.70%\n",
            "iter 2070: loss 2.5225, time 335.57ms, mfu 2.72%\n",
            "iter 2080: loss 2.6402, time 367.28ms, mfu 2.71%\n",
            "iter 2090: loss 2.4514, time 329.94ms, mfu 2.74%\n",
            "iter 2100: loss 2.4357, time 330.96ms, mfu 2.76%\n",
            "iter 2110: loss 2.5357, time 364.08ms, mfu 2.76%\n",
            "iter 2120: loss 2.4724, time 330.45ms, mfu 2.78%\n",
            "iter 2130: loss 2.4177, time 331.45ms, mfu 2.79%\n",
            "iter 2140: loss 2.4342, time 364.75ms, mfu 2.78%\n",
            "iter 2150: loss 2.5041, time 327.94ms, mfu 2.80%\n",
            "iter 2160: loss 2.4775, time 327.10ms, mfu 2.82%\n",
            "iter 2170: loss 2.4852, time 373.14ms, mfu 2.80%\n",
            "iter 2180: loss 2.4418, time 336.32ms, mfu 2.82%\n",
            "iter 2190: loss 2.4648, time 336.83ms, mfu 2.82%\n",
            "iter 2200: loss 2.5627, time 328.88ms, mfu 2.84%\n",
            "iter 2210: loss 2.5365, time 340.46ms, mfu 2.84%\n",
            "iter 2220: loss 2.5813, time 336.63ms, mfu 2.85%\n",
            "iter 2230: loss 2.4133, time 345.74ms, mfu 2.85%\n",
            "iter 2240: loss 2.3494, time 337.99ms, mfu 2.85%\n",
            "step 2250: train loss 2.3878, val loss 2.5725\n",
            "saving checkpoint to out-lyrics\n",
            "iter 2250: loss 2.5152, time 52707.70ms, mfu 2.57%\n",
            "iter 2260: loss 2.3801, time 337.96ms, mfu 2.60%\n",
            "iter 2270: loss 2.5045, time 336.70ms, mfu 2.63%\n",
            "iter 2280: loss 2.5189, time 338.83ms, mfu 2.66%\n",
            "iter 2290: loss 2.4246, time 343.08ms, mfu 2.68%\n",
            "iter 2300: loss 2.5223, time 346.20ms, mfu 2.70%\n",
            "iter 2310: loss 2.4659, time 344.22ms, mfu 2.71%\n",
            "iter 2320: loss 2.4438, time 350.52ms, mfu 2.72%\n",
            "iter 2330: loss 2.3799, time 344.84ms, mfu 2.73%\n",
            "iter 2340: loss 2.5604, time 346.97ms, mfu 2.74%\n",
            "iter 2350: loss 2.5582, time 342.58ms, mfu 2.75%\n",
            "iter 2360: loss 2.5558, time 348.11ms, mfu 2.76%\n",
            "iter 2370: loss 2.4780, time 339.85ms, mfu 2.77%\n",
            "iter 2380: loss 2.4739, time 344.64ms, mfu 2.78%\n",
            "iter 2390: loss 2.4760, time 342.57ms, mfu 2.79%\n",
            "iter 2400: loss 2.3934, time 343.71ms, mfu 2.79%\n",
            "iter 2410: loss 2.5330, time 340.32ms, mfu 2.80%\n",
            "iter 2420: loss 2.4029, time 345.84ms, mfu 2.81%\n",
            "iter 2430: loss 2.3752, time 343.11ms, mfu 2.81%\n",
            "iter 2440: loss 2.4331, time 332.81ms, mfu 2.82%\n",
            "iter 2450: loss 2.5200, time 339.60ms, mfu 2.83%\n",
            "iter 2460: loss 2.4267, time 325.26ms, mfu 2.85%\n",
            "iter 2470: loss 2.3631, time 336.66ms, mfu 2.86%\n",
            "iter 2480: loss 2.4071, time 348.18ms, mfu 2.85%\n",
            "iter 2490: loss 2.4460, time 331.06ms, mfu 2.86%\n",
            "step 2500: train loss 2.3664, val loss 2.5371\n",
            "saving checkpoint to out-lyrics\n",
            "iter 2500: loss 2.4451, time 52696.31ms, mfu 2.58%\n",
            "iter 2510: loss 2.5146, time 340.08ms, mfu 2.61%\n",
            "iter 2520: loss 2.3707, time 341.01ms, mfu 2.64%\n",
            "iter 2530: loss 2.4738, time 338.92ms, mfu 2.66%\n",
            "iter 2540: loss 2.4791, time 342.59ms, mfu 2.68%\n",
            "iter 2550: loss 2.4229, time 340.67ms, mfu 2.70%\n",
            "iter 2560: loss 2.3974, time 337.87ms, mfu 2.72%\n",
            "iter 2570: loss 2.3810, time 348.88ms, mfu 2.73%\n",
            "iter 2580: loss 2.4607, time 343.02ms, mfu 2.74%\n",
            "iter 2590: loss 2.3192, time 346.88ms, mfu 2.75%\n",
            "iter 2600: loss 2.3853, time 346.70ms, mfu 2.76%\n",
            "iter 2610: loss 2.5525, time 342.57ms, mfu 2.77%\n",
            "iter 2620: loss 2.5487, time 349.52ms, mfu 2.77%\n",
            "iter 2630: loss 2.3365, time 344.31ms, mfu 2.78%\n",
            "iter 2640: loss 2.4711, time 351.26ms, mfu 2.78%\n",
            "iter 2650: loss 2.4454, time 343.29ms, mfu 2.79%\n",
            "iter 2660: loss 2.5582, time 332.98ms, mfu 2.80%\n",
            "iter 2670: loss 2.4193, time 342.67ms, mfu 2.81%\n",
            "iter 2680: loss 2.4846, time 339.57ms, mfu 2.82%\n",
            "iter 2690: loss 2.3814, time 352.57ms, mfu 2.81%\n",
            "iter 2700: loss 2.3915, time 345.47ms, mfu 2.82%\n",
            "iter 2710: loss 2.3966, time 336.92ms, mfu 2.83%\n",
            "iter 2720: loss 2.4477, time 331.65ms, mfu 2.84%\n",
            "iter 2730: loss 2.3180, time 331.57ms, mfu 2.85%\n",
            "iter 2740: loss 2.4050, time 341.14ms, mfu 2.85%\n",
            "step 2750: train loss 2.3320, val loss 2.5207\n",
            "saving checkpoint to out-lyrics\n",
            "iter 2750: loss 2.5758, time 49085.87ms, mfu 2.57%\n",
            "iter 2760: loss 2.3995, time 332.54ms, mfu 2.61%\n",
            "iter 2770: loss 2.4657, time 339.53ms, mfu 2.64%\n",
            "iter 2780: loss 2.3412, time 341.68ms, mfu 2.66%\n",
            "iter 2790: loss 2.3966, time 338.57ms, mfu 2.68%\n",
            "iter 2800: loss 2.3481, time 337.87ms, mfu 2.70%\n",
            "iter 2810: loss 2.4980, time 336.91ms, mfu 2.73%\n",
            "iter 2820: loss 2.3630, time 344.31ms, mfu 2.74%\n",
            "iter 2830: loss 2.3556, time 334.91ms, mfu 2.76%\n",
            "iter 2840: loss 2.3044, time 339.59ms, mfu 2.77%\n",
            "iter 2850: loss 2.3906, time 343.52ms, mfu 2.78%\n",
            "iter 2860: loss 2.4173, time 343.51ms, mfu 2.79%\n",
            "iter 2870: loss 2.5768, time 345.11ms, mfu 2.79%\n",
            "iter 2880: loss 2.3751, time 340.35ms, mfu 2.80%\n",
            "iter 2890: loss 2.3224, time 338.67ms, mfu 2.81%\n",
            "iter 2900: loss 2.2767, time 337.17ms, mfu 2.82%\n",
            "iter 2910: loss 2.2965, time 339.70ms, mfu 2.83%\n",
            "iter 2920: loss 2.4717, time 339.31ms, mfu 2.83%\n",
            "iter 2930: loss 2.3871, time 335.35ms, mfu 2.84%\n",
            "iter 2940: loss 2.4296, time 338.98ms, mfu 2.85%\n",
            "iter 2950: loss 2.4247, time 338.28ms, mfu 2.85%\n",
            "iter 2960: loss 2.3369, time 332.63ms, mfu 2.86%\n",
            "iter 2970: loss 2.3806, time 343.18ms, mfu 2.86%\n",
            "iter 2980: loss 2.4354, time 337.10ms, mfu 2.87%\n",
            "iter 2990: loss 2.3650, time 336.26ms, mfu 2.87%\n",
            "step 3000: train loss 2.3007, val loss 2.4933\n",
            "saving checkpoint to out-lyrics\n",
            "iter 3000: loss 2.2788, time 52569.75ms, mfu 2.59%\n",
            "iter 3010: loss 2.4568, time 348.88ms, mfu 2.61%\n",
            "iter 3020: loss 2.2183, time 336.73ms, mfu 2.64%\n",
            "iter 3030: loss 2.3791, time 342.87ms, mfu 2.66%\n",
            "iter 3040: loss 2.3728, time 344.84ms, mfu 2.68%\n",
            "iter 3050: loss 2.4349, time 339.94ms, mfu 2.70%\n",
            "iter 3060: loss 2.3895, time 353.43ms, mfu 2.71%\n",
            "iter 3070: loss 2.3358, time 332.37ms, mfu 2.73%\n",
            "iter 3080: loss 2.4072, time 337.78ms, mfu 2.75%\n",
            "iter 3090: loss 2.2890, time 350.70ms, mfu 2.75%\n",
            "iter 3100: loss 2.5434, time 347.73ms, mfu 2.76%\n",
            "iter 3110: loss 2.2814, time 343.63ms, mfu 2.77%\n",
            "iter 3120: loss 2.3700, time 339.23ms, mfu 2.78%\n",
            "iter 3130: loss 2.5084, time 364.60ms, mfu 2.77%\n",
            "iter 3140: loss 2.3878, time 324.95ms, mfu 2.80%\n",
            "iter 3150: loss 2.3404, time 324.42ms, mfu 2.82%\n",
            "iter 3160: loss 2.3314, time 352.12ms, mfu 2.82%\n",
            "iter 3170: loss 2.5345, time 340.67ms, mfu 2.82%\n",
            "iter 3180: loss 2.3736, time 339.75ms, mfu 2.83%\n",
            "iter 3190: loss 2.2477, time 335.43ms, mfu 2.84%\n",
            "iter 3200: loss 2.4381, time 335.91ms, mfu 2.85%\n",
            "iter 3210: loss 2.3731, time 342.20ms, mfu 2.85%\n",
            "iter 3220: loss 2.4239, time 343.66ms, mfu 2.85%\n",
            "iter 3230: loss 2.2432, time 336.08ms, mfu 2.86%\n",
            "iter 3240: loss 2.4449, time 338.82ms, mfu 2.86%\n",
            "step 3250: train loss 2.2882, val loss 2.4783\n",
            "saving checkpoint to out-lyrics\n",
            "iter 3250: loss 2.2710, time 52696.69ms, mfu 2.58%\n",
            "iter 3260: loss 2.3837, time 353.36ms, mfu 2.59%\n",
            "iter 3270: loss 2.3993, time 336.65ms, mfu 2.63%\n",
            "iter 3280: loss 2.3047, time 344.03ms, mfu 2.65%\n",
            "iter 3290: loss 2.3227, time 346.72ms, mfu 2.67%\n",
            "iter 3300: loss 2.3401, time 344.28ms, mfu 2.69%\n",
            "iter 3310: loss 2.3444, time 341.17ms, mfu 2.70%\n",
            "iter 3320: loss 2.4347, time 343.98ms, mfu 2.72%\n",
            "iter 3330: loss 2.2958, time 344.42ms, mfu 2.73%\n",
            "iter 3340: loss 2.5331, time 340.25ms, mfu 2.75%\n",
            "iter 3350: loss 2.3780, time 346.44ms, mfu 2.75%\n",
            "iter 3360: loss 2.4691, time 342.30ms, mfu 2.77%\n",
            "iter 3370: loss 2.2231, time 339.66ms, mfu 2.78%\n",
            "iter 3380: loss 2.2843, time 340.48ms, mfu 2.79%\n",
            "iter 3390: loss 2.3953, time 348.17ms, mfu 2.79%\n",
            "iter 3400: loss 2.3953, time 340.01ms, mfu 2.80%\n",
            "iter 3410: loss 2.3212, time 333.91ms, mfu 2.81%\n",
            "iter 3420: loss 2.3381, time 324.87ms, mfu 2.83%\n",
            "iter 3430: loss 2.3927, time 337.24ms, mfu 2.84%\n",
            "iter 3440: loss 2.4512, time 338.02ms, mfu 2.85%\n",
            "iter 3450: loss 2.2376, time 332.28ms, mfu 2.86%\n",
            "iter 3460: loss 2.3723, time 351.73ms, mfu 2.85%\n",
            "iter 3470: loss 2.3345, time 338.62ms, mfu 2.86%\n",
            "iter 3480: loss 2.2223, time 335.85ms, mfu 2.86%\n",
            "iter 3490: loss 2.4080, time 340.72ms, mfu 2.86%\n",
            "step 3500: train loss 2.2657, val loss 2.4536\n",
            "saving checkpoint to out-lyrics\n",
            "iter 3500: loss 2.4362, time 52688.02ms, mfu 2.58%\n",
            "iter 3510: loss 2.3588, time 341.06ms, mfu 2.61%\n",
            "iter 3520: loss 2.2531, time 330.69ms, mfu 2.64%\n",
            "iter 3530: loss 2.3139, time 335.16ms, mfu 2.67%\n",
            "iter 3540: loss 2.4382, time 341.52ms, mfu 2.69%\n",
            "iter 3550: loss 2.3523, time 346.41ms, mfu 2.71%\n",
            "iter 3560: loss 2.4468, time 348.79ms, mfu 2.72%\n",
            "iter 3570: loss 2.2591, time 341.48ms, mfu 2.73%\n",
            "iter 3580: loss 2.2594, time 344.11ms, mfu 2.74%\n",
            "iter 3590: loss 2.3417, time 346.15ms, mfu 2.75%\n",
            "iter 3600: loss 2.3685, time 347.22ms, mfu 2.76%\n",
            "iter 3610: loss 2.3479, time 345.28ms, mfu 2.77%\n",
            "iter 3620: loss 2.3405, time 345.67ms, mfu 2.77%\n",
            "iter 3630: loss 2.3390, time 339.59ms, mfu 2.79%\n",
            "iter 3640: loss 2.2945, time 340.08ms, mfu 2.80%\n",
            "iter 3650: loss 2.2927, time 343.58ms, mfu 2.80%\n",
            "iter 3660: loss 2.3493, time 338.47ms, mfu 2.81%\n",
            "iter 3670: loss 2.3718, time 330.46ms, mfu 2.83%\n",
            "iter 3680: loss 2.2641, time 340.82ms, mfu 2.83%\n",
            "iter 3690: loss 2.4959, time 348.15ms, mfu 2.83%\n",
            "iter 3700: loss 2.4215, time 341.16ms, mfu 2.83%\n",
            "iter 3710: loss 2.3774, time 359.78ms, mfu 2.82%\n",
            "iter 3720: loss 2.2457, time 328.88ms, mfu 2.84%\n",
            "iter 3730: loss 2.4320, time 328.51ms, mfu 2.85%\n",
            "iter 3740: loss 2.4186, time 360.94ms, mfu 2.84%\n",
            "step 3750: train loss 2.2567, val loss 2.4630\n",
            "iter 3750: loss 2.2685, time 46853.66ms, mfu 2.56%\n",
            "iter 3760: loss 2.3285, time 341.58ms, mfu 2.59%\n",
            "iter 3770: loss 2.3250, time 337.03ms, mfu 2.62%\n",
            "iter 3780: loss 2.3046, time 345.98ms, mfu 2.64%\n",
            "iter 3790: loss 2.2507, time 338.04ms, mfu 2.67%\n",
            "iter 3800: loss 2.2795, time 342.73ms, mfu 2.69%\n",
            "iter 3810: loss 2.1308, time 343.79ms, mfu 2.70%\n",
            "iter 3820: loss 2.4022, time 342.00ms, mfu 2.72%\n",
            "iter 3830: loss 2.3298, time 343.36ms, mfu 2.73%\n",
            "iter 3840: loss 2.3269, time 336.20ms, mfu 2.75%\n",
            "iter 3850: loss 2.2980, time 336.68ms, mfu 2.77%\n",
            "iter 3860: loss 2.4052, time 340.15ms, mfu 2.78%\n",
            "iter 3870: loss 2.2445, time 336.15ms, mfu 2.79%\n",
            "iter 3880: loss 2.3945, time 343.15ms, mfu 2.80%\n",
            "iter 3890: loss 2.2817, time 338.30ms, mfu 2.81%\n",
            "iter 3900: loss 2.4346, time 342.12ms, mfu 2.82%\n",
            "iter 3910: loss 2.3689, time 341.93ms, mfu 2.82%\n",
            "iter 3920: loss 2.2826, time 341.18ms, mfu 2.83%\n",
            "iter 3930: loss 2.3952, time 343.00ms, mfu 2.83%\n",
            "iter 3940: loss 2.2317, time 338.43ms, mfu 2.84%\n",
            "iter 3950: loss 2.2419, time 343.39ms, mfu 2.84%\n",
            "iter 3960: loss 2.2938, time 340.10ms, mfu 2.84%\n",
            "iter 3970: loss 2.2400, time 335.30ms, mfu 2.85%\n",
            "iter 3980: loss 2.3386, time 338.81ms, mfu 2.85%\n",
            "iter 3990: loss 2.3240, time 338.00ms, mfu 2.86%\n",
            "step 4000: train loss 2.2333, val loss 2.4502\n",
            "saving checkpoint to out-lyrics\n",
            "iter 4000: loss 2.3176, time 52824.72ms, mfu 2.58%\n",
            "iter 4010: loss 2.3110, time 352.30ms, mfu 2.60%\n",
            "iter 4020: loss 2.3307, time 345.01ms, mfu 2.62%\n",
            "iter 4030: loss 2.3019, time 336.48ms, mfu 2.65%\n",
            "iter 4040: loss 2.2903, time 346.64ms, mfu 2.67%\n",
            "iter 4050: loss 2.3013, time 347.91ms, mfu 2.68%\n",
            "iter 4060: loss 2.3854, time 344.40ms, mfu 2.70%\n",
            "iter 4070: loss 2.3158, time 334.67ms, mfu 2.72%\n",
            "iter 4080: loss 2.3253, time 341.07ms, mfu 2.74%\n",
            "iter 4090: loss 2.2911, time 349.94ms, mfu 2.74%\n",
            "iter 4100: loss 2.3643, time 341.66ms, mfu 2.76%\n",
            "iter 4110: loss 2.1757, time 339.14ms, mfu 2.77%\n",
            "iter 4120: loss 2.2365, time 345.69ms, mfu 2.78%\n",
            "iter 4130: loss 2.3092, time 339.77ms, mfu 2.79%\n",
            "iter 4140: loss 2.3008, time 342.85ms, mfu 2.79%\n",
            "iter 4150: loss 2.2536, time 344.03ms, mfu 2.80%\n",
            "iter 4160: loss 2.2029, time 342.19ms, mfu 2.81%\n",
            "iter 4170: loss 2.2827, time 340.54ms, mfu 2.81%\n",
            "iter 4180: loss 2.2131, time 344.18ms, mfu 2.82%\n",
            "iter 4190: loss 2.2507, time 335.37ms, mfu 2.83%\n",
            "iter 4200: loss 2.2304, time 348.58ms, mfu 2.83%\n",
            "iter 4210: loss 2.3768, time 345.15ms, mfu 2.83%\n",
            "iter 4220: loss 2.2759, time 341.87ms, mfu 2.83%\n",
            "iter 4230: loss 2.3686, time 341.98ms, mfu 2.84%\n",
            "iter 4240: loss 2.2961, time 341.48ms, mfu 2.84%\n",
            "step 4250: train loss 2.2303, val loss 2.4328\n",
            "saving checkpoint to out-lyrics\n",
            "iter 4250: loss 2.2555, time 52469.31ms, mfu 2.56%\n",
            "iter 4260: loss 2.2934, time 342.39ms, mfu 2.59%\n",
            "iter 4270: loss 2.4800, time 335.40ms, mfu 2.62%\n",
            "iter 4280: loss 2.3877, time 347.24ms, mfu 2.64%\n",
            "iter 4290: loss 2.2404, time 345.10ms, mfu 2.66%\n",
            "iter 4300: loss 2.3015, time 343.90ms, mfu 2.68%\n",
            "iter 4310: loss 2.1782, time 343.21ms, mfu 2.70%\n",
            "iter 4320: loss 2.1431, time 346.38ms, mfu 2.71%\n",
            "iter 4330: loss 2.4082, time 350.74ms, mfu 2.72%\n",
            "iter 4340: loss 2.3827, time 344.45ms, mfu 2.73%\n",
            "iter 4350: loss 2.3902, time 349.54ms, mfu 2.74%\n",
            "iter 4360: loss 2.1985, time 342.49ms, mfu 2.75%\n",
            "iter 4370: loss 2.2183, time 343.64ms, mfu 2.76%\n",
            "iter 4380: loss 2.2657, time 333.14ms, mfu 2.78%\n",
            "iter 4390: loss 2.3537, time 325.37ms, mfu 2.80%\n",
            "iter 4400: loss 2.2324, time 353.12ms, mfu 2.80%\n",
            "iter 4410: loss 2.2498, time 336.36ms, mfu 2.81%\n",
            "iter 4420: loss 2.3375, time 323.93ms, mfu 2.83%\n",
            "iter 4430: loss 2.2884, time 355.94ms, mfu 2.83%\n",
            "iter 4440: loss 2.1987, time 337.82ms, mfu 2.83%\n",
            "iter 4450: loss 2.1151, time 334.84ms, mfu 2.84%\n",
            "iter 4460: loss 2.2974, time 341.29ms, mfu 2.85%\n",
            "iter 4470: loss 2.1902, time 337.86ms, mfu 2.85%\n",
            "iter 4480: loss 2.3092, time 352.55ms, mfu 2.84%\n",
            "iter 4490: loss 2.3998, time 339.66ms, mfu 2.85%\n",
            "step 4500: train loss 2.2269, val loss 2.4329\n",
            "iter 4500: loss 2.1581, time 46787.19ms, mfu 2.57%\n",
            "iter 4510: loss 2.3106, time 330.48ms, mfu 2.61%\n",
            "iter 4520: loss 2.2950, time 342.96ms, mfu 2.63%\n",
            "iter 4530: loss 2.1738, time 368.24ms, mfu 2.63%\n",
            "iter 4540: loss 2.3471, time 332.60ms, mfu 2.67%\n",
            "iter 4550: loss 2.2737, time 331.78ms, mfu 2.69%\n",
            "iter 4560: loss 2.4069, time 383.12ms, mfu 2.68%\n",
            "iter 4570: loss 2.3469, time 329.81ms, mfu 2.71%\n",
            "iter 4580: loss 2.3236, time 337.44ms, mfu 2.73%\n",
            "iter 4590: loss 2.3712, time 361.28ms, mfu 2.73%\n",
            "iter 4600: loss 2.2570, time 344.02ms, mfu 2.74%\n",
            "iter 4610: loss 2.3069, time 341.04ms, mfu 2.75%\n",
            "iter 4620: loss 2.2466, time 343.12ms, mfu 2.76%\n",
            "iter 4630: loss 2.3715, time 342.56ms, mfu 2.77%\n",
            "iter 4640: loss 2.3100, time 338.19ms, mfu 2.79%\n",
            "iter 4650: loss 2.3261, time 344.05ms, mfu 2.79%\n",
            "iter 4660: loss 2.3371, time 339.59ms, mfu 2.80%\n",
            "iter 4670: loss 2.2243, time 341.60ms, mfu 2.81%\n",
            "iter 4680: loss 2.3410, time 344.18ms, mfu 2.81%\n",
            "iter 4690: loss 2.3590, time 343.19ms, mfu 2.82%\n",
            "iter 4700: loss 2.4093, time 338.41ms, mfu 2.83%\n",
            "iter 4710: loss 2.2988, time 347.63ms, mfu 2.82%\n",
            "iter 4720: loss 2.3024, time 341.24ms, mfu 2.83%\n",
            "iter 4730: loss 2.2776, time 338.89ms, mfu 2.84%\n",
            "iter 4740: loss 2.3120, time 340.16ms, mfu 2.84%\n",
            "step 4750: train loss 2.1980, val loss 2.4259\n",
            "saving checkpoint to out-lyrics\n",
            "iter 4750: loss 2.3379, time 52950.11ms, mfu 2.56%\n",
            "iter 4760: loss 2.2481, time 349.20ms, mfu 2.58%\n",
            "iter 4770: loss 2.1959, time 333.11ms, mfu 2.62%\n",
            "iter 4780: loss 2.3828, time 342.32ms, mfu 2.64%\n",
            "iter 4790: loss 2.3048, time 343.28ms, mfu 2.67%\n",
            "iter 4800: loss 2.3291, time 344.05ms, mfu 2.68%\n",
            "iter 4810: loss 2.2977, time 346.13ms, mfu 2.70%\n",
            "iter 4820: loss 2.3371, time 344.62ms, mfu 2.71%\n",
            "iter 4830: loss 2.3974, time 345.84ms, mfu 2.73%\n",
            "iter 4840: loss 2.1874, time 347.14ms, mfu 2.74%\n",
            "iter 4850: loss 2.2257, time 347.98ms, mfu 2.74%\n",
            "iter 4860: loss 2.2316, time 341.08ms, mfu 2.76%\n",
            "iter 4870: loss 2.2087, time 344.20ms, mfu 2.77%\n",
            "iter 4880: loss 2.3135, time 341.88ms, mfu 2.78%\n",
            "iter 4890: loss 2.1776, time 343.44ms, mfu 2.78%\n",
            "iter 4900: loss 2.3075, time 343.58ms, mfu 2.79%\n",
            "iter 4910: loss 2.2952, time 339.19ms, mfu 2.80%\n",
            "iter 4920: loss 2.2418, time 342.54ms, mfu 2.81%\n",
            "iter 4930: loss 2.4373, time 338.91ms, mfu 2.82%\n",
            "iter 4940: loss 2.2409, time 340.87ms, mfu 2.82%\n",
            "iter 4950: loss 2.4084, time 342.15ms, mfu 2.83%\n",
            "iter 4960: loss 2.3244, time 340.19ms, mfu 2.83%\n",
            "iter 4970: loss 2.3220, time 331.56ms, mfu 2.84%\n",
            "iter 4980: loss 2.3931, time 337.90ms, mfu 2.85%\n",
            "iter 4990: loss 2.1452, time 336.07ms, mfu 2.86%\n",
            "step 5000: train loss 2.2103, val loss 2.4255\n",
            "saving checkpoint to out-lyrics\n",
            "iter 5000: loss 2.2833, time 47964.41ms, mfu 2.57%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python sample.py --out_dir=out-lyrics --start=\"fade \""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJxKjt4cXCkX",
        "outputId": "de904f48-49a3-43b3-8697-f63fe90d573a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: out_dir = out-lyrics\n",
            "Overriding: start = fade \n",
            "number of parameters: 29.94M\n",
            "No meta.pkl found, assuming GPT-2 encodings...\n",
            "fade  \n",
            "Make sure that they have to be  \n",
            "And it's been called to you  \n",
            "  \n",
            "[Chorus]  \n",
            "Do what I say  \n",
            "  \n",
            "[Chorus]  \n",
            "I will be the one who knows what it means  \n",
            "I will be the one who knows what it means  \n",
            "I will be the one who knows what it means  \n",
            "'Cause all I want is to be with you  \n",
            "  \n",
            "[Chorus]  \n",
            "  \n",
            "Ah  \n",
            "[Chorus]  \n",
            "Do what I say  \n",
            "I will be the one who knows what it means  \n",
            "I will be the one who knows what it means  \n",
            "I will be the one who knows what it means  \n",
            "I will be the one who knows what it means  \n",
            "I will be the one who knows what it means  \n",
            "I will be the one who knows what it means  \n",
            "I will be the one who knows what it means  \n",
            "I will be the one who knows what it means  \n",
            "I will be the one who knows what it means  \n",
            "I will be the one who knows what it means  \n",
            "I will be the one who knows what it means  \n",
            "I will be the one who knows what it means  \n",
            "I will be the one who knows what it means  \n",
            "I will be the one who knows what it means  \n",
            "I will be the one who knows what it means  \n",
            "I will be the one who knows what it means  \n",
            "I will be the one who knows what it means  \n",
            "I will be the one who knows what it means  \n",
            "I will be the one who knows what it means  \n",
            "I will be the one who knows what it means  \n",
            "I will be the one who knows what it means  \n",
            "I will be the one who knows what it means  \n",
            "I will be the one who knows what it means  \n",
            "I will be the one who knows what it means  \n",
            "I will be the one who knows what it means  \n",
            "\n",
            "---------------\n",
            "fade  \n",
            "You'll never realize  \n",
            "You'll never know  \n",
            "You've never felt this way before  \n",
            "  \n",
            "You're so unsure, so what you mean  \n",
            "You're so unsure, so what you mean  \n",
            "You're so tired, so what you mean  \n",
            "You're so tired, so what you mean\n",
            "\n",
            "\n",
            "I'm gonna be your freak in my car  \n",
            "I'm gonna be your freak in my car  \n",
            "I'm gonna be your freak in my car  \n",
            "I'm gonna be your freak in my car  \n",
            "I'm gonna be your freak in my car  \n",
            "I'm gonna be your freak in my car  \n",
            "I'm gonna be your freak in my car  \n",
            "I'm gonna be your freak in my car  \n",
            "I'm gonna be your freak in my car  \n",
            "I'm gonna be your freak in my car  \n",
            "I'm gonna be your freak in my car  \n",
            "I'm gonna be your freak in my car  \n",
            "I'm gonna be your freak in my car  \n",
            "I'm gonna be your freak in my car  \n",
            "I'm gonna be your freak in my car  \n",
            "I'm gonna be your freak in my car  \n",
            "I'm gonna be your freak in my car  \n",
            "I'm gonna be your freak in my car  \n",
            "I'm gonna be your freak in my car  \n",
            "I'm gonna be your freak in my car  \n",
            "I'm gonna be your freak in my car  \n",
            "I'm gonna be your freak in my car  \n",
            "I'm gonna be your freak in my car  \n",
            "  \n",
            "I'm gonna be your freak in my car  \n",
            "I'm gonna be your freak in my car  \n",
            "  \n",
            "I'm gonna be your freak in my car  \n",
            "I'm gonna be your freak in my car  \n",
            "I'm gonna be your freak in my car  \n",
            "I'm gonna be your freak in my car  \n",
            "I'm gonna be your freak in my car  \n",
            "I'm gonna be your freak in my car  \n",
            "I'm\n",
            "---------------\n",
            "fade  \n",
            "(Yeah, yeah)  \n",
            "  \n",
            "Well, it's a new day comin  \n",
            "It's a new day comin  \n",
            "It's a new day comin'  \n",
            "A new day comin  \n",
            "It's a new day comin  \n",
            "It's a new day comin  \n",
            "It's a new day comin  \n",
            "It's a new day comin, it's a new day comin  \n",
            "  \n",
            "It's a new day comin, it's a new day comin  \n",
            "It's a new day comin  \n",
            "It's a new day comin  \n",
            "It's a new day comin  \n",
            "It's a new day comin  \n",
            "It's a new day comin  \n",
            "It's a new day comin\n",
            "\n",
            "\n",
            "I looked at you  \n",
            "You opened your eyes  \n",
            "But you gave me the one to me  \n",
            "You gave me the one to me  \n",
            "And you gave me the one to me  \n",
            "You gave me the one to me  \n",
            "And you gave me the one to me  \n",
            "  \n",
            "How long did I think you  \n",
            "I tried to hold on to you  \n",
            "I tried to hold on to you  \n",
            "  \n",
            "How long did I think you  \n",
            "I tried to hold on to you  \n",
            "I tried to hold on to you  \n",
            "  \n",
            "How long did I think you  \n",
            "I tried to hold on to you  \n",
            "How long did I think you  \n",
            "I cried to hold on to you  \n",
            "  \n",
            "So I turned away  \n",
            "Still I thought you  \n",
            "I didn't cry to hold on to you\n",
            "\n",
            "\n",
            "I can't get down  \n",
            "I can't make a sound  \n",
            "Till it's cold  \n",
            "I'm feelin' good  \n",
            "When I can't see no more  \n",
            "And I'm walkin'  \n",
            "  \n",
            "There's no feeling that I can't\n",
            "---------------\n",
            "fade  \n",
            "We live, we live  \n",
            "We live  \n",
            "We live, we live, we live  \n",
            "We live, we live  \n",
            "We live, we live  \n",
            "We live  \n",
            "We live, we live  \n",
            "We live, we live, we live  \n",
            "We live, we live, we live  \n",
            "We live, we live  \n",
            "We live, we live, we live\n",
            "\n",
            "\n",
            "I feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like we're in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I\n",
            "---------------\n",
            "fade  \n",
            "  \n",
            "Hate the piper,  \n",
            "Keep it alive  \n",
            "(it's really not that)  \n",
            "Them niggaz think you're a super bitch  \n",
            "And they all got the money for me on the TV  \n",
            "And yyall know what it is  \n",
            "Throw it down, throw it down  \n",
            "They gonna be with it, yeah yeah yeah  \n",
            "  \n",
            "[Chorus]  \n",
            "  \n",
            "[Verse 2]  \n",
            "Call my name, ain't it a shame  \n",
            "Kiss me like a gun to the  \n",
            "Yall know what I'm doing is  \n",
            "And that's why I brought my head up up  \n",
            "You know what I'm doing is  \n",
            "And that's why I brought my head up  \n",
            "You know what I'm doing is  \n",
            "And that's why I brought my head up  \n",
            "You know how I came to be with it  \n",
            "With I'm back in this bitch nigga nigga, don't leave it  \n",
            "They wanna be with it  \n",
            "And that's why I brought my head up  \n",
            "You know how I came to be with it  \n",
            "Cause that's why I came to be with it  \n",
            "With I'm back in this bitch nigga  \n",
            "This is the house that I hit the ground  \n",
            "And that's why I came to be with it  \n",
            "With I'm back in this bitch nigga  \n",
            "This is the house that I hit the ground  \n",
            "And that's why I came to be with it  \n",
            "Back in this bitch I hit the ground  \n",
            "  \n",
            "[Bridge]  \n",
            "  \n",
            "[Verse 2]  \n",
            "  \n",
            "[Bridge]  \n",
            "See you lookin' at me smile  \n",
            "This is the house that I hit the ground  \n",
            "And that's why I came to be with it  \n",
            "Cause that's why I came to be with it  \n",
            "Cause that's why I came to be with it  \n",
            "---------------\n",
            "fade  \n",
            "You thought you were here to stay  \n",
            "Now you're a star like you used to me  \n",
            "And now you're a star like you used to me  \n",
            "Whoa, whoa  \n",
            "  \n",
            "Love was a dreamer  \n",
            "And it was once in an end  \n",
            "I was here again  \n",
            "I was here again  \n",
            "I was here again  \n",
            "Well I caught a wall  \n",
            "The way you look at me  \n",
            "And I saw your picture  \n",
            "And all your friends  \n",
            "And your friends  \n",
            "You make them cry  \n",
            "  \n",
            "I was here again  \n",
            "I was here again  \n",
            "I was here again  \n",
            "I was here again  \n",
            "I was here again  \n",
            "I was here again  \n",
            "I was here again  \n",
            "I was here again  \n",
            "I was here again  \n",
            "I was here again  \n",
            "I was here again  \n",
            "I was here again\n",
            "\n",
            "\n",
            "Don't seem to know the pain  \n",
            "Doesn't seem to change  \n",
            "You're so close to my heart  \n",
            "Is so wrong, I know it  \n",
            "I'm not so wrong  \n",
            "It's time for me to change the time  \n",
            "  \n",
            "You're so far away  \n",
            "I've spent the night without you  \n",
            "I'm not your kind of fooling  \n",
            "Sometimes I think I need you  \n",
            "But it's not quite the same  \n",
            "I'm not so wrong  \n",
            "It's time for me to change the time  \n",
            "  \n",
            "Don't seem to know the pain  \n",
            "Doesn't seem to change  \n",
            "You're so far away  \n",
            "I've spent the night without you  \n",
            "I'm not so wrong, I know it  \n",
            "I'm not so wrong, I know it  \n",
            "I'm not so wrong, I know it  \n",
            "I'm not so wrong  \n",
            "It's time for me to change the time  \n",
            "---------------\n",
            "fade  \n",
            "  \n",
            "One, and one, and one, and a  \n",
            "One, and one, and a  \n",
            "One, and one, and a  \n",
            "One, and one, and a  \n",
            "One, and two, and a  \n",
            "One, and one, and a  \n",
            "One, and a  \n",
            "One, and a  \n",
            "One, and one, and a  \n",
            "\n",
            "\n",
            "One, and two, two, three, two, three, three  \n",
            "One, and two, three, three, four, three, one  \n",
            "One, and two, and two, four, four, four, one  \n",
            "The one and one, and one, and one, and one, and one  \n",
            "  \n",
            "One, and the one, and the one, and the one  \n",
            "One, and a  \n",
            "One, and a, and the one\n",
            "\n",
            "\n",
            "Cick  \n",
            "A hot kiss, like a cherry  \n",
            "A hot kiss, like a cherry  \n",
            "All I need is a ball  \n",
            "I'm a hot kiss  \n",
            "  \n",
            "I'm a hot kiss, like a cherry  \n",
            "Shoo  \n",
            "I'm a hot kiss, like a cherry  \n",
            "Shoo, woo  \n",
            "  \n",
            "I'm a hot kiss, like a cherry  \n",
            "Shoo, woo  \n",
            "  \n",
            "I'm a hot kiss, like a cherry  \n",
            "Shoo, woo, woo  \n",
            "I'm a hot kiss  \n",
            "  \n",
            "I'm a hot kiss, like a cherry  \n",
            "Shoo, woo  \n",
            "  \n",
            "Gonna shove you up, let will ride you down  \n",
            "I'm a hot kiss, like a cherry  \n",
            "Shoo, woo, woo  \n",
            "  \n",
            "I'm a hot kiss, like a cherry  \n",
            "Shoo, woo, woo  \n",
            "Gonna shove you up, let will ride you down  \n",
            "I'm a hot kiss, like\n",
            "---------------\n",
            "fade  \n",
            "If he wasn't worried  \n",
            "He was still not what he heard  \n",
            "Sometimes he was sick  \n",
            "He was still not what he heard  \n",
            "Sometimes he was sick  \n",
            "He was still not what he heard  \n",
            "Sometimes he was sick  \n",
            "He was still not what he heard  \n",
            "Sometimes he was sick  \n",
            "He was still not what he heard  \n",
            "Sometimes he was sick  \n",
            "He was still not what he heard  \n",
            "Sometimes he was sick  \n",
            "He was still not what he heard  \n",
            "Sometimes he was sick  \n",
            "He was still not what he heard  \n",
            "Sometimes he was sick  \n",
            "He was still not what he heard  \n",
            "Sometimes he was sick  \n",
            "He was still not what he heard  \n",
            "Sometimes he was sick  \n",
            "He was still not what he heard  \n",
            "Sometimes he was sick  \n",
            "He was still not what he heard  \n",
            "Sometimes he was sick  \n",
            "He was still not what he heard  \n",
            "Sometimes he was sick  \n",
            "He was still not what he heard  \n",
            "Sometimes he was sick  \n",
            "He was still not what he heard  \n",
            "Sometimes he was sick  \n",
            "He was still not what he heard  \n",
            "Sometimes he was sick  \n",
            "He was still not what he heard  \n",
            "Sometimes he was sick  \n",
            "He was still not what he heard  \n",
            "Sometimes he was sick  \n",
            "He was still not what he heard  \n",
            "Sometimes he was sick  \n",
            "He was still not what he heard  \n",
            "Sometimes he was sick  \n",
            "He was still not what he heard  \n",
            "Sometimes he was sick  \n",
            "He was still not what he heard  \n",
            "Sometimes he was sick  \n",
            "He was still not what he heard  \n",
            "Sometimes he was sick  \n",
            "He was still not what he heard  \n",
            "Sometimes he was sick  \n",
            "He was still not what he heard  \n",
            "Sometimes he was sick  \n",
            "He was still not what he heard  \n",
            "Sometimes he\n",
            "---------------\n",
            "fade  \n",
            "Come with me, come with me, come with me  \n",
            "Come with me, come with me\n",
            "\n",
            "\n",
            "Do you know?  \n",
            "Do you know?  \n",
            "Do you know?  \n",
            "Do you know?)  \n",
            "Do you know?  \n",
            "Do you know?  \n",
            "Do you know?  \n",
            "Do you know?  \n",
            "Do you know?  \n",
            "Do you know?  \n",
            "Do you know?  \n",
            "Do you know?  \n",
            "Do you know?  \n",
            "Do you know?  \n",
            "Do you know?  \n",
            "Do you know?  \n",
            "Do you know?  \n",
            "Do you know?  \n",
            "Do you know?  \n",
            "Do you know?  \n",
            "Do you know?  \n",
            "Do you know?\n",
            "\n",
            "\n",
            "Breathing, let's go  \n",
            "Breathing, let's go  \n",
            "Breathing, let's go  \n",
            "Breathing, let's go  \n",
            "  \n",
            "Breathing, let's go\n",
            "\n",
            "\n",
            "Walk that trigger off, turn it up  \n",
            "Let's go round and round and round  \n",
            "Gimme a body, so makin' it stop  \n",
            "Breathing, let's go round and round  \n",
            "Gimme a body, so makin' it stop  \n",
            "Breathing, let's go round and round  \n",
            "Gimme a body, so makin' it stop  \n",
            "Breathing, it, let's go round and round  \n",
            "  \n",
            "Breathing, let's go round and round  \n",
            "Gimme a body, so makin' it stop  \n",
            "Breathing, let's go round and round  \n",
            "Gimme a body, so makin' it stop  \n",
            "Breathing, let's go round and round  \n",
            "Gimme a body, so makin' it stop  \n",
            "Breathing, let's go round and round  \n",
            "  \n",
            "Breathing, let\n",
            "---------------\n",
            "fade  \n",
            "Here we go again  \n",
            "  \n",
            "[Bridge:]  \n",
            "You can't hide from me, you can't hide from me  \n",
            "You can't hide from me, you can't hide from me  \n",
            "I can't believe you're in love  \n",
            "But I can't believe you're in love  \n",
            "  \n",
            "[Chorus]  \n",
            "You can't hide from me  \n",
            "You can't hide from me  \n",
            "You can't hide from me  \n",
            "I can't believe you're in love  \n",
            "But I can't believe you're in love  \n",
            "So close to me  \n",
            "I can't believe you're in love  \n",
            "But I can't believe you're in love  \n",
            "  \n",
            "[Bridge]  \n",
            "I can't believe you're in love\n",
            "\n",
            "\n",
            "I want to take your hand  \n",
            "I want to take you to your hand  \n",
            "I want to take you to your hand  \n",
            "I want to take your hand  \n",
            "I want to take you to your hand  \n",
            "I want to take your hand  \n",
            "I want to take you to your hand  \n",
            "I want to take you to your hand  \n",
            "I want to take your hand  \n",
            "I want to take you to your hand  \n",
            "I want to take your hand  \n",
            "I want to take your hand  \n",
            "I want to take your hand  \n",
            "I want to take your hand  \n",
            "I want to take your hand  \n",
            "I want to make you love me  \n",
            "  \n",
            "I want to take your hand  \n",
            "I want to take your hand  \n",
            "I want to take your hand  \n",
            "I want to take your hand  \n",
            "I want to take your hand  \n",
            "I want to take your hand  \n",
            "I want to take your hand\n",
            "\n",
            "\n",
            "Listen to the laughter  \n",
            "It's in the laughter  \n",
            "The morning in the winter sun  \n",
            "The sun is shining on  \n",
            "With the sun to shine  \n",
            "You can\n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tang Poem Writer"
      ],
      "metadata": {
        "id": "Cp4NyTeIiJ5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python data/TangPoems/prepare.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dx4WcJgrhqw-",
        "outputId": "678a6a96-54a3-40cb-f0c4-c47b91f704e3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train has 47,451 tokens\n",
            "val has 5,304 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py config/train_TangPoems.py --n_layer=4 --n_head=4 --n_embd=128 --max_iters=2000 --lr_decay_iters=2000 --dropout=0.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avTc2Rlbif1n",
        "outputId": "6e3c1f92-5f50-400b-f4ff-115d1345648e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding config with config/train_TangPoems.py:\n",
            "# train a miniature character-level shakespeare model\n",
            "# good for debugging and playing on macbooks and such\n",
            "\n",
            "out_dir = 'out-TangPoems'\n",
            "eval_interval = 250 # keep frequent because we'll overfit\n",
            "eval_iters = 200\n",
            "log_interval = 10 # don't print too too often\n",
            "\n",
            "# we expect to overfit on this small dataset, so only save when val improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "wandb_log = False # override via command line if you like\n",
            "wandb_project = 'TangPoems'\n",
            "wandb_run_name = 'mini-gpt'\n",
            "\n",
            "dataset = 'TangPoems'\n",
            "gradient_accumulation_steps = 1\n",
            "batch_size = 64\n",
            "block_size = 256 # context of up to 256 previous characters\n",
            "\n",
            "# baby GPT model :)\n",
            "n_layer = 6\n",
            "n_head = 6\n",
            "n_embd = 384\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
            "max_iters = 5000\n",
            "lr_decay_iters = 5000 # make equal to max_iters usually\n",
            "min_lr = 1e-4 # learning_rate / 10 usually\n",
            "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
            "\n",
            "warmup_iters = 100 # not super necessary potentially\n",
            "\n",
            "# on macbook also add\n",
            "# device = 'cpu'  # run on cpu only\n",
            "# compile = False # do not torch compile the model\n",
            "\n",
            "Overriding: n_layer = 4\n",
            "Overriding: n_head = 4\n",
            "Overriding: n_embd = 128\n",
            "Overriding: max_iters = 2000\n",
            "Overriding: lr_decay_iters = 2000\n",
            "Overriding: dropout = 0.2\n",
            "tokens per iteration will be: 16,384\n",
            "Initializing a new model from scratch\n",
            "defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\n",
            "number of parameters: 7.23M\n",
            "num decayed parameter tensors: 18, with 7,258,112 parameters\n",
            "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
            "using fused AdamW: True\n",
            "compiling the model... (takes a ~minute)\n",
            "step 0: train loss 10.8594, val loss 10.8649\n",
            "[2023-09-18 09:09:21,803] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-18 09:09:22,263] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-18 09:09:23,169] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-18 09:09:23,440] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-18 09:09:23,840] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-18 09:09:24,106] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-18 09:09:24,520] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-18 09:09:24,785] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "iter 0: loss 10.8579, time 37909.16ms, mfu -100.00%\n",
            "iter 10: loss 10.5672, time 161.70ms, mfu 1.46%\n",
            "iter 20: loss 10.1698, time 161.81ms, mfu 1.46%\n",
            "iter 30: loss 9.5158, time 162.77ms, mfu 1.46%\n",
            "iter 40: loss 8.5928, time 161.45ms, mfu 1.46%\n",
            "iter 50: loss 7.4337, time 160.96ms, mfu 1.46%\n",
            "iter 60: loss 6.1956, time 161.49ms, mfu 1.46%\n",
            "iter 70: loss 5.2608, time 162.52ms, mfu 1.46%\n",
            "iter 80: loss 4.7538, time 161.53ms, mfu 1.46%\n",
            "iter 90: loss 4.5256, time 161.52ms, mfu 1.46%\n",
            "iter 100: loss 4.3756, time 160.90ms, mfu 1.46%\n",
            "iter 110: loss 4.1795, time 161.38ms, mfu 1.46%\n",
            "iter 120: loss 4.0643, time 162.08ms, mfu 1.46%\n",
            "iter 130: loss 3.9726, time 162.27ms, mfu 1.46%\n",
            "iter 140: loss 3.8823, time 161.69ms, mfu 1.46%\n",
            "iter 150: loss 3.8211, time 162.07ms, mfu 1.46%\n",
            "iter 160: loss 3.7391, time 163.20ms, mfu 1.46%\n",
            "iter 170: loss 3.7134, time 161.86ms, mfu 1.46%\n",
            "iter 180: loss 3.5929, time 162.16ms, mfu 1.46%\n",
            "iter 190: loss 3.5620, time 162.04ms, mfu 1.46%\n",
            "iter 200: loss 3.5835, time 162.82ms, mfu 1.46%\n",
            "iter 210: loss 3.5345, time 162.97ms, mfu 1.46%\n",
            "iter 220: loss 3.5082, time 162.31ms, mfu 1.46%\n",
            "iter 230: loss 3.4835, time 162.03ms, mfu 1.46%\n",
            "iter 240: loss 3.4356, time 162.78ms, mfu 1.45%\n",
            "step 250: train loss 3.3618, val loss 3.6229\n",
            "saving checkpoint to out-TangPoems\n",
            "iter 250: loss 3.4161, time 29819.67ms, mfu 1.31%\n",
            "iter 260: loss 3.3923, time 166.04ms, mfu 1.32%\n",
            "iter 270: loss 3.3693, time 165.36ms, mfu 1.33%\n",
            "iter 280: loss 3.3313, time 166.62ms, mfu 1.34%\n",
            "iter 290: loss 3.3339, time 162.35ms, mfu 1.35%\n",
            "iter 300: loss 3.3219, time 167.61ms, mfu 1.36%\n",
            "iter 310: loss 3.3354, time 167.49ms, mfu 1.36%\n",
            "iter 320: loss 3.3018, time 168.82ms, mfu 1.37%\n",
            "iter 330: loss 3.3374, time 167.17ms, mfu 1.37%\n",
            "iter 340: loss 3.2644, time 173.91ms, mfu 1.37%\n",
            "iter 350: loss 3.2580, time 175.54ms, mfu 1.37%\n",
            "iter 360: loss 3.2448, time 172.99ms, mfu 1.37%\n",
            "iter 370: loss 3.2143, time 165.84ms, mfu 1.37%\n",
            "iter 380: loss 3.2128, time 170.62ms, mfu 1.37%\n",
            "iter 390: loss 3.1942, time 168.57ms, mfu 1.38%\n",
            "iter 400: loss 3.1926, time 169.13ms, mfu 1.38%\n",
            "iter 410: loss 3.1705, time 171.38ms, mfu 1.38%\n",
            "iter 420: loss 3.1500, time 170.32ms, mfu 1.38%\n",
            "iter 430: loss 3.1246, time 166.78ms, mfu 1.38%\n",
            "iter 440: loss 3.1354, time 168.03ms, mfu 1.38%\n",
            "iter 450: loss 3.1411, time 166.00ms, mfu 1.39%\n",
            "iter 460: loss 3.0720, time 167.84ms, mfu 1.39%\n",
            "iter 470: loss 3.0885, time 168.42ms, mfu 1.39%\n",
            "iter 480: loss 3.0106, time 167.43ms, mfu 1.39%\n",
            "iter 490: loss 3.0465, time 168.44ms, mfu 1.39%\n",
            "step 500: train loss 2.8601, val loss 3.2777\n",
            "saving checkpoint to out-TangPoems\n",
            "iter 500: loss 3.0872, time 27354.66ms, mfu 1.25%\n",
            "iter 510: loss 3.0209, time 162.48ms, mfu 1.27%\n",
            "iter 520: loss 2.9913, time 164.14ms, mfu 1.29%\n",
            "iter 530: loss 3.0040, time 165.22ms, mfu 1.30%\n",
            "iter 540: loss 2.9947, time 164.87ms, mfu 1.32%\n",
            "iter 550: loss 2.9712, time 167.19ms, mfu 1.33%\n",
            "iter 560: loss 2.9539, time 165.50ms, mfu 1.34%\n",
            "iter 570: loss 2.9590, time 164.12ms, mfu 1.35%\n",
            "iter 580: loss 2.8984, time 165.64ms, mfu 1.35%\n",
            "iter 590: loss 2.9251, time 165.01ms, mfu 1.36%\n",
            "iter 600: loss 2.9158, time 166.64ms, mfu 1.37%\n",
            "iter 610: loss 2.8693, time 164.43ms, mfu 1.37%\n",
            "iter 620: loss 2.8807, time 166.27ms, mfu 1.38%\n",
            "iter 630: loss 2.9206, time 166.03ms, mfu 1.38%\n",
            "iter 640: loss 2.8823, time 165.60ms, mfu 1.39%\n",
            "iter 650: loss 2.8702, time 165.17ms, mfu 1.39%\n",
            "iter 660: loss 2.8482, time 166.27ms, mfu 1.39%\n",
            "iter 670: loss 2.8009, time 166.24ms, mfu 1.40%\n",
            "iter 680: loss 2.7810, time 164.86ms, mfu 1.40%\n",
            "iter 690: loss 2.8291, time 165.33ms, mfu 1.40%\n",
            "iter 700: loss 2.7669, time 166.20ms, mfu 1.40%\n",
            "iter 710: loss 2.7555, time 166.65ms, mfu 1.41%\n",
            "iter 720: loss 2.8102, time 166.48ms, mfu 1.41%\n",
            "iter 730: loss 2.7641, time 168.72ms, mfu 1.41%\n",
            "iter 740: loss 2.7520, time 165.74ms, mfu 1.41%\n",
            "step 750: train loss 2.5240, val loss 3.2095\n",
            "saving checkpoint to out-TangPoems\n",
            "iter 750: loss 2.7551, time 27650.19ms, mfu 1.27%\n",
            "iter 760: loss 2.7364, time 163.64ms, mfu 1.29%\n",
            "iter 770: loss 2.6905, time 166.20ms, mfu 1.30%\n",
            "iter 780: loss 2.7174, time 166.00ms, mfu 1.31%\n",
            "iter 790: loss 2.6831, time 164.26ms, mfu 1.32%\n",
            "iter 800: loss 2.7143, time 166.96ms, mfu 1.33%\n",
            "iter 810: loss 2.6860, time 164.91ms, mfu 1.34%\n",
            "iter 820: loss 2.6752, time 165.21ms, mfu 1.35%\n",
            "iter 830: loss 2.6348, time 165.28ms, mfu 1.36%\n",
            "iter 840: loss 2.6536, time 167.88ms, mfu 1.36%\n",
            "iter 850: loss 2.6647, time 164.08ms, mfu 1.37%\n",
            "iter 860: loss 2.6362, time 166.07ms, mfu 1.38%\n",
            "iter 870: loss 2.6415, time 164.20ms, mfu 1.38%\n",
            "iter 880: loss 2.6449, time 164.19ms, mfu 1.39%\n",
            "iter 890: loss 2.6123, time 163.74ms, mfu 1.39%\n",
            "iter 900: loss 2.6442, time 166.04ms, mfu 1.40%\n",
            "iter 910: loss 2.6129, time 164.87ms, mfu 1.40%\n",
            "iter 920: loss 2.5699, time 167.88ms, mfu 1.40%\n",
            "iter 930: loss 2.5888, time 165.12ms, mfu 1.40%\n",
            "iter 940: loss 2.5728, time 164.59ms, mfu 1.41%\n",
            "iter 950: loss 2.5752, time 164.72ms, mfu 1.41%\n",
            "iter 960: loss 2.5330, time 164.29ms, mfu 1.41%\n",
            "iter 970: loss 2.5729, time 166.44ms, mfu 1.41%\n",
            "iter 980: loss 2.5505, time 165.27ms, mfu 1.41%\n",
            "iter 990: loss 2.5170, time 166.21ms, mfu 1.41%\n",
            "step 1000: train loss 2.2061, val loss 3.2815\n",
            "iter 1000: loss 2.5494, time 27336.30ms, mfu 1.27%\n",
            "iter 1010: loss 2.5240, time 164.53ms, mfu 1.29%\n",
            "iter 1020: loss 2.4712, time 166.17ms, mfu 1.30%\n",
            "iter 1030: loss 2.5142, time 164.45ms, mfu 1.32%\n",
            "iter 1040: loss 2.4799, time 166.72ms, mfu 1.33%\n",
            "iter 1050: loss 2.4660, time 164.40ms, mfu 1.34%\n",
            "iter 1060: loss 2.4619, time 164.82ms, mfu 1.35%\n",
            "iter 1070: loss 2.4550, time 165.37ms, mfu 1.35%\n",
            "iter 1080: loss 2.4490, time 165.12ms, mfu 1.36%\n",
            "iter 1090: loss 2.4548, time 165.07ms, mfu 1.37%\n",
            "iter 1100: loss 2.4286, time 166.70ms, mfu 1.37%\n",
            "iter 1110: loss 2.3911, time 166.80ms, mfu 1.38%\n",
            "iter 1120: loss 2.4250, time 166.16ms, mfu 1.38%\n",
            "iter 1130: loss 2.3954, time 165.65ms, mfu 1.39%\n",
            "iter 1140: loss 2.3787, time 164.34ms, mfu 1.39%\n",
            "iter 1150: loss 2.3947, time 164.92ms, mfu 1.39%\n",
            "iter 1160: loss 2.3619, time 164.62ms, mfu 1.40%\n",
            "iter 1170: loss 2.3613, time 165.01ms, mfu 1.40%\n",
            "iter 1180: loss 2.3644, time 163.75ms, mfu 1.41%\n",
            "iter 1190: loss 2.3605, time 166.49ms, mfu 1.41%\n",
            "iter 1200: loss 2.3561, time 165.50ms, mfu 1.41%\n",
            "iter 1210: loss 2.3713, time 165.23ms, mfu 1.41%\n",
            "iter 1220: loss 2.3132, time 164.35ms, mfu 1.41%\n",
            "iter 1230: loss 2.3298, time 164.71ms, mfu 1.42%\n",
            "iter 1240: loss 2.3095, time 164.79ms, mfu 1.42%\n",
            "step 1250: train loss 1.9030, val loss 3.4059\n",
            "iter 1250: loss 2.3099, time 27362.86ms, mfu 1.28%\n",
            "iter 1260: loss 2.3017, time 165.45ms, mfu 1.29%\n",
            "iter 1270: loss 2.3225, time 165.62ms, mfu 1.30%\n",
            "iter 1280: loss 2.2943, time 164.60ms, mfu 1.32%\n",
            "iter 1290: loss 2.2848, time 163.46ms, mfu 1.33%\n",
            "iter 1300: loss 2.2773, time 164.33ms, mfu 1.34%\n",
            "iter 1310: loss 2.2803, time 165.62ms, mfu 1.35%\n",
            "iter 1320: loss 2.2532, time 164.31ms, mfu 1.36%\n",
            "iter 1330: loss 2.2504, time 164.49ms, mfu 1.37%\n",
            "iter 1340: loss 2.2330, time 164.21ms, mfu 1.37%\n",
            "iter 1350: loss 2.2333, time 165.01ms, mfu 1.38%\n",
            "iter 1360: loss 2.2416, time 165.36ms, mfu 1.38%\n",
            "iter 1370: loss 2.2246, time 163.85ms, mfu 1.39%\n",
            "iter 1380: loss 2.2196, time 165.26ms, mfu 1.39%\n",
            "iter 1390: loss 2.2242, time 164.13ms, mfu 1.40%\n",
            "iter 1400: loss 2.2362, time 164.78ms, mfu 1.40%\n",
            "iter 1410: loss 2.2216, time 165.31ms, mfu 1.40%\n",
            "iter 1420: loss 2.1816, time 164.40ms, mfu 1.41%\n",
            "iter 1430: loss 2.1991, time 164.12ms, mfu 1.41%\n",
            "iter 1440: loss 2.1935, time 164.82ms, mfu 1.41%\n",
            "iter 1450: loss 2.2021, time 163.10ms, mfu 1.42%\n",
            "iter 1460: loss 2.1772, time 163.89ms, mfu 1.42%\n",
            "iter 1470: loss 2.1799, time 164.51ms, mfu 1.42%\n",
            "iter 1480: loss 2.1595, time 164.45ms, mfu 1.42%\n",
            "iter 1490: loss 2.1835, time 165.78ms, mfu 1.42%\n",
            "step 1500: train loss 1.6772, val loss 3.5105\n",
            "iter 1500: loss 2.1604, time 27262.50ms, mfu 1.28%\n",
            "iter 1510: loss 2.1617, time 164.42ms, mfu 1.30%\n",
            "iter 1520: loss 2.1550, time 164.99ms, mfu 1.31%\n",
            "iter 1530: loss 2.1285, time 163.51ms, mfu 1.32%\n",
            "iter 1540: loss 2.1609, time 163.57ms, mfu 1.33%\n",
            "iter 1550: loss 2.1672, time 164.14ms, mfu 1.34%\n",
            "iter 1560: loss 2.1227, time 163.98ms, mfu 1.35%\n",
            "iter 1570: loss 2.1282, time 164.18ms, mfu 1.36%\n",
            "iter 1580: loss 2.1265, time 164.00ms, mfu 1.37%\n",
            "iter 1590: loss 2.1234, time 165.51ms, mfu 1.38%\n",
            "iter 1600: loss 2.1276, time 164.30ms, mfu 1.38%\n",
            "iter 1610: loss 2.1166, time 164.06ms, mfu 1.39%\n",
            "iter 1620: loss 2.1008, time 163.50ms, mfu 1.39%\n",
            "iter 1630: loss 2.1211, time 163.42ms, mfu 1.40%\n",
            "iter 1640: loss 2.1145, time 164.21ms, mfu 1.40%\n",
            "iter 1650: loss 2.1123, time 163.72ms, mfu 1.41%\n",
            "iter 1660: loss 2.1009, time 163.99ms, mfu 1.41%\n",
            "iter 1670: loss 2.1000, time 164.99ms, mfu 1.41%\n",
            "iter 1680: loss 2.0929, time 164.13ms, mfu 1.41%\n",
            "iter 1690: loss 2.0853, time 163.91ms, mfu 1.42%\n",
            "iter 1700: loss 2.0887, time 164.57ms, mfu 1.42%\n",
            "iter 1710: loss 2.0830, time 163.25ms, mfu 1.42%\n",
            "iter 1720: loss 2.0402, time 163.11ms, mfu 1.42%\n",
            "iter 1730: loss 2.0653, time 164.69ms, mfu 1.42%\n",
            "iter 1740: loss 2.0857, time 163.89ms, mfu 1.43%\n",
            "step 1750: train loss 1.5441, val loss 3.5911\n",
            "iter 1750: loss 2.0636, time 27222.27ms, mfu 1.28%\n",
            "iter 1760: loss 2.1149, time 164.92ms, mfu 1.30%\n",
            "iter 1770: loss 2.0908, time 163.57ms, mfu 1.31%\n",
            "iter 1780: loss 2.0643, time 163.65ms, mfu 1.33%\n",
            "iter 1790: loss 2.0518, time 163.45ms, mfu 1.34%\n",
            "iter 1800: loss 2.0508, time 163.65ms, mfu 1.35%\n",
            "iter 1810: loss 2.0598, time 164.31ms, mfu 1.36%\n",
            "iter 1820: loss 2.0446, time 163.46ms, mfu 1.37%\n",
            "iter 1830: loss 2.0401, time 163.80ms, mfu 1.37%\n",
            "iter 1840: loss 2.0575, time 164.20ms, mfu 1.38%\n",
            "iter 1850: loss 2.0351, time 164.32ms, mfu 1.39%\n",
            "iter 1860: loss 2.0426, time 163.86ms, mfu 1.39%\n",
            "iter 1870: loss 2.0322, time 164.16ms, mfu 1.40%\n",
            "iter 1880: loss 2.0464, time 163.25ms, mfu 1.40%\n",
            "iter 1890: loss 2.0286, time 164.96ms, mfu 1.40%\n",
            "iter 1900: loss 2.0303, time 164.56ms, mfu 1.41%\n",
            "iter 1910: loss 2.0226, time 164.77ms, mfu 1.41%\n",
            "iter 1920: loss 2.0013, time 165.06ms, mfu 1.41%\n",
            "iter 1930: loss 2.0474, time 163.61ms, mfu 1.41%\n",
            "iter 1940: loss 2.0265, time 163.71ms, mfu 1.42%\n",
            "iter 1950: loss 2.0503, time 163.25ms, mfu 1.42%\n",
            "iter 1960: loss 2.0070, time 164.80ms, mfu 1.42%\n",
            "iter 1970: loss 1.9987, time 164.98ms, mfu 1.42%\n",
            "iter 1980: loss 2.0347, time 165.30ms, mfu 1.42%\n",
            "iter 1990: loss 2.0223, time 163.87ms, mfu 1.42%\n",
            "step 2000: train loss 1.4528, val loss 3.6412\n",
            "iter 2000: loss 1.9964, time 27331.51ms, mfu 1.28%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python sample.py --out_dir=out-TangPoems --start=\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxKVcN9Si9Uu",
        "outputId": "44332e25-8bb1-4057-f84b-57703d4a4033"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: out_dir = out-TangPoems\n",
            "Overriding: start = \n",
            "number of parameters: 7.23M\n",
            "No meta.pkl found, assuming GPT-2 encodings...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "---------------\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "---------------\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "---------------\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "---------------\n",
            "\n",
            "\n",
            "\n",
            "---------------\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "---------------\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "---------------\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "---------------\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "---------------\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eYTomIgaluI1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}