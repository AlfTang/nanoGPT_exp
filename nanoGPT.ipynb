{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNnY+K0RlYYE1oNR6QEmQ8D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlfTang/nanoGPT_exp/blob/main/nanoGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9Fpv4z3X_tFn",
        "outputId": "42cfd318-2a5a-4c46-c922-061e3c1cf96d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nanoGPT_exp'...\n",
            "remote: Enumerating objects: 700, done.\u001b[K\n",
            "remote: Counting objects: 100% (21/21), done.\u001b[K\n",
            "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "remote: Total 700 (delta 8), reused 15 (delta 6), pack-reused 679\u001b[K\n",
            "Receiving objects: 100% (700/700), 51.95 MiB | 20.72 MiB/s, done.\n",
            "Resolving deltas: 100% (402/402), done.\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.33.2-py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-2.14.5-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.6/519.6 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken\n",
            "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wandb\n",
            "  Downloading wandb-0.15.10-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.2-py3-none-any.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.36-py3-none-any.whl (189 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.5/189.5 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.31.0-py2.py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.8/224.8 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting pathtools (from wandb)\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=e6aac37276750c325cf0e4221d1da6f9cb70ec9e37b99c33a20dd4c922ef4d4a\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built pathtools\n",
            "Installing collected packages: tokenizers, safetensors, pathtools, xxhash, smmap, setproctitle, sentry-sdk, docker-pycreds, dill, tiktoken, multiprocess, huggingface-hub, gitdb, transformers, GitPython, wandb, datasets\n",
            "Successfully installed GitPython-3.1.36 datasets-2.14.5 dill-0.3.7 docker-pycreds-0.4.0 gitdb-4.0.10 huggingface-hub-0.17.2 multiprocess-0.70.15 pathtools-0.1.2 safetensors-0.3.3 sentry-sdk-1.31.0 setproctitle-1.3.2 smmap-5.0.1 tiktoken-0.5.1 tokenizers-0.13.3 transformers-4.33.2 wandb-0.15.10 xxhash-3.3.0\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/AlfTang/nanoGPT_exp.git\n",
        "!pip install torch numpy transformers datasets tiktoken wandb tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd nanoGPT_exp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZ2rItPLCrkE",
        "outputId": "3fadd432-03d4-4fc5-f7c2-5d0b9c8fb49c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nanoGPT_exp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Shakespear Writer"
      ],
      "metadata": {
        "id": "Zvh0YKhcTCZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python data/shakespeare_char/prepare.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7vEbq86C5RH",
        "outputId": "555e7517-b9b4-4adc-eb98-ef366564add6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters: 1,115,394\n",
            "all the unique characters: \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "vocab size: 65\n",
            "train has 1,003,854 tokens\n",
            "val has 111,540 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py config/train_shakespeare_char.py"
      ],
      "metadata": {
        "id": "meEdjlssEDlE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a00c3b5c-da84-4a52-d353-51cd0da0de08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding config with config/train_shakespeare_char.py:\n",
            "# train a miniature character-level shakespeare model\n",
            "# good for debugging and playing on macbooks and such\n",
            "\n",
            "out_dir = 'out-shakespeare-char'\n",
            "eval_interval = 250 # keep frequent because we'll overfit\n",
            "eval_iters = 200\n",
            "log_interval = 10 # don't print too too often\n",
            "\n",
            "# we expect to overfit on this small dataset, so only save when val improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "wandb_log = False # override via command line if you like\n",
            "wandb_project = 'shakespeare-char'\n",
            "wandb_run_name = 'mini-gpt'\n",
            "\n",
            "dataset = 'shakespeare_char'\n",
            "gradient_accumulation_steps = 1\n",
            "batch_size = 64\n",
            "block_size = 256 # context of up to 256 previous characters\n",
            "\n",
            "# baby GPT model :)\n",
            "n_layer = 6\n",
            "n_head = 6\n",
            "n_embd = 384\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
            "max_iters = 5000\n",
            "lr_decay_iters = 5000 # make equal to max_iters usually\n",
            "min_lr = 1e-4 # learning_rate / 10 usually\n",
            "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
            "\n",
            "warmup_iters = 100 # not super necessary potentially\n",
            "\n",
            "# on macbook also add\n",
            "# device = 'cpu'  # run on cpu only\n",
            "# compile = False # do not torch compile the model\n",
            "\n",
            "tokens per iteration will be: 16,384\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 10.65M\n",
            "num decayed parameter tensors: 26, with 10,740,096 parameters\n",
            "num non-decayed parameter tensors: 13, with 4,992 parameters\n",
            "using fused AdamW: True\n",
            "compiling the model... (takes a ~minute)\n",
            "step 0: train loss 4.2874, val loss 4.2823\n",
            "[2023-09-18 07:57:03,006] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-18 07:57:03,776] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-18 07:57:05,461] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-18 07:57:05,927] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-18 07:57:06,456] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-18 07:57:06,728] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-18 07:57:07,144] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-18 07:57:07,423] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-18 07:57:07,851] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-18 07:57:08,142] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-18 07:57:08,566] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-18 07:57:08,850] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "iter 0: loss 4.2649, time 36305.07ms, mfu -100.00%\n",
            "iter 10: loss 3.2438, time 107.23ms, mfu 3.47%\n",
            "iter 20: loss 2.7899, time 104.89ms, mfu 3.48%\n",
            "iter 30: loss 2.6383, time 107.23ms, mfu 3.48%\n",
            "iter 40: loss 2.5763, time 108.27ms, mfu 3.48%\n",
            "iter 50: loss 2.5261, time 106.81ms, mfu 3.48%\n",
            "iter 60: loss 2.5136, time 105.65ms, mfu 3.48%\n",
            "iter 70: loss 2.4921, time 108.45ms, mfu 3.48%\n",
            "iter 80: loss 2.4932, time 109.52ms, mfu 3.47%\n",
            "iter 90: loss 2.4696, time 108.95ms, mfu 3.47%\n",
            "iter 100: loss 2.4526, time 107.70ms, mfu 3.47%\n",
            "iter 110: loss 2.4543, time 109.37ms, mfu 3.46%\n",
            "iter 120: loss 2.4223, time 110.35ms, mfu 3.45%\n",
            "iter 130: loss 2.4059, time 107.55ms, mfu 3.45%\n",
            "iter 140: loss 2.3925, time 109.02ms, mfu 3.45%\n",
            "iter 150: loss 2.4098, time 108.77ms, mfu 3.45%\n",
            "iter 160: loss 2.3675, time 108.53ms, mfu 3.45%\n",
            "iter 170: loss 2.3382, time 107.92ms, mfu 3.45%\n",
            "iter 180: loss 2.3011, time 113.72ms, mfu 3.43%\n",
            "iter 190: loss 2.2278, time 107.59ms, mfu 3.43%\n",
            "iter 200: loss 2.2004, time 110.78ms, mfu 3.43%\n",
            "iter 210: loss 2.1244, time 111.36ms, mfu 3.42%\n",
            "iter 220: loss 2.1338, time 110.30ms, mfu 3.41%\n",
            "iter 230: loss 2.0709, time 111.06ms, mfu 3.41%\n",
            "iter 240: loss 2.0742, time 110.34ms, mfu 3.40%\n",
            "step 250: train loss 1.9616, val loss 2.0647\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 250: loss 2.0277, time 14637.90ms, mfu 3.07%\n",
            "iter 260: loss 1.9685, time 113.12ms, mfu 3.09%\n",
            "iter 270: loss 1.9776, time 116.58ms, mfu 3.10%\n",
            "iter 280: loss 1.9798, time 117.16ms, mfu 3.11%\n",
            "iter 290: loss 1.9237, time 115.02ms, mfu 3.12%\n",
            "iter 300: loss 1.8944, time 112.71ms, mfu 3.14%\n",
            "iter 310: loss 1.8637, time 114.86ms, mfu 3.15%\n",
            "iter 320: loss 1.8569, time 117.15ms, mfu 3.15%\n",
            "iter 330: loss 1.8088, time 114.07ms, mfu 3.16%\n",
            "iter 340: loss 1.7812, time 117.67ms, mfu 3.16%\n",
            "iter 350: loss 1.8272, time 115.07ms, mfu 3.17%\n",
            "iter 360: loss 1.7745, time 114.09ms, mfu 3.18%\n",
            "iter 370: loss 1.7414, time 116.56ms, mfu 3.18%\n",
            "iter 380: loss 1.7304, time 117.72ms, mfu 3.18%\n",
            "iter 390: loss 1.7372, time 115.67ms, mfu 3.19%\n",
            "iter 400: loss 1.7640, time 120.02ms, mfu 3.18%\n",
            "iter 410: loss 1.6959, time 116.59ms, mfu 3.18%\n",
            "iter 420: loss 1.7088, time 118.24ms, mfu 3.18%\n",
            "iter 430: loss 1.6815, time 120.59ms, mfu 3.17%\n",
            "iter 440: loss 1.6462, time 118.84ms, mfu 3.16%\n",
            "iter 450: loss 1.6511, time 118.46ms, mfu 3.16%\n",
            "iter 460: loss 1.6024, time 117.42ms, mfu 3.16%\n",
            "iter 470: loss 1.6554, time 117.84ms, mfu 3.16%\n",
            "iter 480: loss 1.6165, time 120.45ms, mfu 3.16%\n",
            "iter 490: loss 1.6016, time 120.66ms, mfu 3.15%\n",
            "step 500: train loss 1.5285, val loss 1.7362\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 500: loss 1.6016, time 12959.67ms, mfu 2.84%\n",
            "iter 510: loss 1.6162, time 123.16ms, mfu 2.86%\n",
            "iter 520: loss 1.6020, time 124.84ms, mfu 2.87%\n",
            "iter 530: loss 1.5657, time 122.27ms, mfu 2.89%\n",
            "iter 540: loss 1.6203, time 123.01ms, mfu 2.90%\n",
            "iter 550: loss 1.5671, time 121.31ms, mfu 2.92%\n",
            "iter 560: loss 1.5651, time 126.45ms, mfu 2.92%\n",
            "iter 570: loss 1.5745, time 121.43ms, mfu 2.94%\n",
            "iter 580: loss 1.5401, time 121.72ms, mfu 2.95%\n",
            "iter 590: loss 1.5016, time 121.48ms, mfu 2.96%\n",
            "iter 600: loss 1.5180, time 124.28ms, mfu 2.96%\n",
            "iter 610: loss 1.5552, time 122.67ms, mfu 2.97%\n",
            "iter 620: loss 1.5292, time 124.04ms, mfu 2.97%\n",
            "iter 630: loss 1.5179, time 123.08ms, mfu 2.98%\n",
            "iter 640: loss 1.4779, time 126.19ms, mfu 2.98%\n",
            "iter 650: loss 1.5043, time 123.69ms, mfu 2.98%\n",
            "iter 660: loss 1.5145, time 128.38ms, mfu 2.97%\n",
            "iter 670: loss 1.4491, time 127.36ms, mfu 2.97%\n",
            "iter 680: loss 1.5118, time 124.03ms, mfu 2.97%\n",
            "iter 690: loss 1.4611, time 121.75ms, mfu 2.98%\n",
            "iter 700: loss 1.4802, time 123.50ms, mfu 2.98%\n",
            "iter 710: loss 1.4568, time 122.95ms, mfu 2.99%\n",
            "iter 720: loss 1.4481, time 123.15ms, mfu 2.99%\n",
            "iter 730: loss 1.4214, time 125.33ms, mfu 2.99%\n",
            "iter 740: loss 1.4311, time 121.50ms, mfu 3.00%\n",
            "step 750: train loss 1.3611, val loss 1.5957\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 750: loss 1.4226, time 13188.13ms, mfu 2.70%\n",
            "iter 760: loss 1.4461, time 120.47ms, mfu 2.74%\n",
            "iter 770: loss 1.4283, time 121.75ms, mfu 2.77%\n",
            "iter 780: loss 1.4123, time 121.14ms, mfu 2.80%\n",
            "iter 790: loss 1.4221, time 121.97ms, mfu 2.83%\n",
            "iter 800: loss 1.4286, time 121.92ms, mfu 2.85%\n",
            "iter 810: loss 1.4068, time 121.17ms, mfu 2.87%\n",
            "iter 820: loss 1.4105, time 121.11ms, mfu 2.89%\n",
            "iter 830: loss 1.3939, time 120.52ms, mfu 2.91%\n",
            "iter 840: loss 1.4056, time 120.32ms, mfu 2.93%\n",
            "iter 850: loss 1.3911, time 121.37ms, mfu 2.95%\n",
            "iter 860: loss 1.4010, time 125.04ms, mfu 2.95%\n",
            "iter 870: loss 1.4061, time 120.33ms, mfu 2.96%\n",
            "iter 880: loss 1.3758, time 118.71ms, mfu 2.98%\n",
            "iter 890: loss 1.3920, time 120.66ms, mfu 2.99%\n",
            "iter 900: loss 1.3728, time 120.05ms, mfu 3.00%\n",
            "iter 910: loss 1.3245, time 122.86ms, mfu 3.01%\n",
            "iter 920: loss 1.3671, time 120.66ms, mfu 3.01%\n",
            "iter 930: loss 1.3670, time 122.11ms, mfu 3.02%\n",
            "iter 940: loss 1.3524, time 122.55ms, mfu 3.02%\n",
            "iter 950: loss 1.3538, time 122.08ms, mfu 3.02%\n",
            "iter 960: loss 1.3668, time 122.25ms, mfu 3.03%\n",
            "iter 970: loss 1.3636, time 119.31ms, mfu 3.04%\n",
            "iter 980: loss 1.3566, time 120.61ms, mfu 3.04%\n",
            "iter 990: loss 1.3400, time 119.38ms, mfu 3.05%\n",
            "step 1000: train loss 1.2762, val loss 1.5256\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1000: loss 1.3415, time 13063.89ms, mfu 2.75%\n",
            "iter 1010: loss 1.3431, time 120.38ms, mfu 2.78%\n",
            "iter 1020: loss 1.3133, time 121.23ms, mfu 2.81%\n",
            "iter 1030: loss 1.3367, time 121.05ms, mfu 2.84%\n",
            "iter 1040: loss 1.3645, time 122.25ms, mfu 2.86%\n",
            "iter 1050: loss 1.2936, time 121.71ms, mfu 2.88%\n",
            "iter 1060: loss 1.3426, time 122.25ms, mfu 2.90%\n",
            "iter 1070: loss 1.3305, time 121.24ms, mfu 2.91%\n",
            "iter 1080: loss 1.3386, time 122.10ms, mfu 2.93%\n",
            "iter 1090: loss 1.3539, time 121.28ms, mfu 2.94%\n",
            "iter 1100: loss 1.3168, time 123.30ms, mfu 2.95%\n",
            "iter 1110: loss 1.3008, time 122.05ms, mfu 2.96%\n",
            "iter 1120: loss 1.3026, time 119.82ms, mfu 2.98%\n",
            "iter 1130: loss 1.3015, time 121.97ms, mfu 2.98%\n",
            "iter 1140: loss 1.2992, time 122.62ms, mfu 2.99%\n",
            "iter 1150: loss 1.3126, time 121.40ms, mfu 3.00%\n",
            "iter 1160: loss 1.3269, time 120.83ms, mfu 3.01%\n",
            "iter 1170: loss 1.3064, time 120.75ms, mfu 3.01%\n",
            "iter 1180: loss 1.3226, time 120.21ms, mfu 3.02%\n",
            "iter 1190: loss 1.2668, time 125.08ms, mfu 3.02%\n",
            "iter 1200: loss 1.2964, time 125.87ms, mfu 3.01%\n",
            "iter 1210: loss 1.2739, time 121.51ms, mfu 3.02%\n",
            "iter 1220: loss 1.3009, time 122.27ms, mfu 3.02%\n",
            "iter 1230: loss 1.2977, time 120.39ms, mfu 3.03%\n",
            "iter 1240: loss 1.3042, time 120.73ms, mfu 3.03%\n",
            "step 1250: train loss 1.2079, val loss 1.4969\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1250: loss 1.2753, time 13273.29ms, mfu 2.73%\n",
            "iter 1260: loss 1.2867, time 121.03ms, mfu 2.77%\n",
            "iter 1270: loss 1.2715, time 121.83ms, mfu 2.80%\n",
            "iter 1280: loss 1.2605, time 120.73ms, mfu 2.83%\n",
            "iter 1290: loss 1.2806, time 122.08ms, mfu 2.85%\n",
            "iter 1300: loss 1.2990, time 120.92ms, mfu 2.87%\n",
            "iter 1310: loss 1.2402, time 121.61ms, mfu 2.89%\n",
            "iter 1320: loss 1.3072, time 121.46ms, mfu 2.91%\n",
            "iter 1330: loss 1.2705, time 122.93ms, mfu 2.92%\n",
            "iter 1340: loss 1.2992, time 121.51ms, mfu 2.94%\n",
            "iter 1350: loss 1.2529, time 122.51ms, mfu 2.95%\n",
            "iter 1360: loss 1.2679, time 120.49ms, mfu 2.96%\n",
            "iter 1370: loss 1.2591, time 120.33ms, mfu 2.97%\n",
            "iter 1380: loss 1.2695, time 120.73ms, mfu 2.99%\n",
            "iter 1390: loss 1.2550, time 120.53ms, mfu 3.00%\n",
            "iter 1400: loss 1.2619, time 122.80ms, mfu 3.00%\n",
            "iter 1410: loss 1.2513, time 122.52ms, mfu 3.00%\n",
            "iter 1420: loss 1.2738, time 121.25ms, mfu 3.01%\n",
            "iter 1430: loss 1.2424, time 122.51ms, mfu 3.01%\n",
            "iter 1440: loss 1.2576, time 122.26ms, mfu 3.02%\n",
            "iter 1450: loss 1.2388, time 119.86ms, mfu 3.03%\n",
            "iter 1460: loss 1.2448, time 121.03ms, mfu 3.03%\n",
            "iter 1470: loss 1.2223, time 123.85ms, mfu 3.03%\n",
            "iter 1480: loss 1.2091, time 120.06ms, mfu 3.04%\n",
            "iter 1490: loss 1.2391, time 121.55ms, mfu 3.04%\n",
            "step 1500: train loss 1.1533, val loss 1.4693\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1500: loss 1.1903, time 13238.89ms, mfu 2.74%\n",
            "iter 1510: loss 1.2334, time 122.95ms, mfu 2.77%\n",
            "iter 1520: loss 1.2242, time 120.43ms, mfu 2.80%\n",
            "iter 1530: loss 1.2554, time 123.84ms, mfu 2.82%\n",
            "iter 1540: loss 1.1952, time 120.15ms, mfu 2.85%\n",
            "iter 1550: loss 1.2376, time 120.18ms, mfu 2.87%\n",
            "iter 1560: loss 1.2124, time 121.45ms, mfu 2.89%\n",
            "iter 1570: loss 1.2289, time 120.46ms, mfu 2.91%\n",
            "iter 1580: loss 1.2126, time 123.89ms, mfu 2.92%\n",
            "iter 1590: loss 1.1923, time 121.30ms, mfu 2.94%\n",
            "iter 1600: loss 1.2019, time 121.49ms, mfu 2.95%\n",
            "iter 1610: loss 1.2397, time 121.13ms, mfu 2.96%\n",
            "iter 1620: loss 1.1842, time 122.54ms, mfu 2.97%\n",
            "iter 1630: loss 1.2066, time 122.58ms, mfu 2.98%\n",
            "iter 1640: loss 1.2057, time 123.01ms, mfu 2.98%\n",
            "iter 1650: loss 1.1848, time 120.93ms, mfu 2.99%\n",
            "iter 1660: loss 1.2201, time 121.90ms, mfu 3.00%\n",
            "iter 1670: loss 1.2026, time 121.10ms, mfu 3.01%\n",
            "iter 1680: loss 1.2037, time 121.69ms, mfu 3.01%\n",
            "iter 1690: loss 1.2076, time 124.09ms, mfu 3.01%\n",
            "iter 1700: loss 1.1913, time 120.48ms, mfu 3.02%\n",
            "iter 1710: loss 1.1841, time 126.35ms, mfu 3.01%\n",
            "iter 1720: loss 1.1854, time 121.06ms, mfu 3.02%\n",
            "iter 1730: loss 1.1986, time 121.60ms, mfu 3.02%\n",
            "iter 1740: loss 1.1763, time 121.31ms, mfu 3.03%\n",
            "step 1750: train loss 1.1028, val loss 1.4603\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1750: loss 1.1805, time 13301.18ms, mfu 2.73%\n",
            "iter 1760: loss 1.1936, time 123.64ms, mfu 2.76%\n",
            "iter 1770: loss 1.1979, time 122.22ms, mfu 2.79%\n",
            "iter 1780: loss 1.1955, time 122.93ms, mfu 2.81%\n",
            "iter 1790: loss 1.1912, time 121.10ms, mfu 2.84%\n",
            "iter 1800: loss 1.1752, time 122.01ms, mfu 2.86%\n",
            "iter 1810: loss 1.1647, time 121.01ms, mfu 2.88%\n",
            "iter 1820: loss 1.1709, time 121.68ms, mfu 2.90%\n",
            "iter 1830: loss 1.1703, time 121.30ms, mfu 2.92%\n",
            "iter 1840: loss 1.1592, time 121.84ms, mfu 2.93%\n",
            "iter 1850: loss 1.1671, time 120.02ms, mfu 2.95%\n",
            "iter 1860: loss 1.1751, time 120.73ms, mfu 2.96%\n",
            "iter 1870: loss 1.1444, time 120.90ms, mfu 2.97%\n",
            "iter 1880: loss 1.1820, time 120.02ms, mfu 2.99%\n",
            "iter 1890: loss 1.1850, time 123.06ms, mfu 2.99%\n",
            "iter 1900: loss 1.1317, time 123.47ms, mfu 2.99%\n",
            "iter 1910: loss 1.1653, time 121.58ms, mfu 3.00%\n",
            "iter 1920: loss 1.1680, time 122.95ms, mfu 3.00%\n",
            "iter 1930: loss 1.1419, time 120.38ms, mfu 3.01%\n",
            "iter 1940: loss 1.1259, time 122.75ms, mfu 3.02%\n",
            "iter 1950: loss 1.1378, time 122.84ms, mfu 3.02%\n",
            "iter 1960: loss 1.1573, time 123.07ms, mfu 3.02%\n",
            "iter 1970: loss 1.1574, time 125.50ms, mfu 3.01%\n",
            "iter 1980: loss 1.1518, time 125.13ms, mfu 3.01%\n",
            "iter 1990: loss 1.1529, time 121.67ms, mfu 3.01%\n",
            "step 2000: train loss 1.0560, val loss 1.4723\n",
            "iter 2000: loss 1.1313, time 13005.28ms, mfu 2.72%\n",
            "iter 2010: loss 1.1371, time 122.28ms, mfu 2.75%\n",
            "iter 2020: loss 1.1267, time 119.81ms, mfu 2.79%\n",
            "iter 2030: loss 1.1575, time 121.77ms, mfu 2.81%\n",
            "iter 2040: loss 1.1369, time 125.14ms, mfu 2.83%\n",
            "iter 2050: loss 1.1200, time 123.03ms, mfu 2.85%\n",
            "iter 2060: loss 1.1054, time 126.84ms, mfu 2.86%\n",
            "iter 2070: loss 1.1241, time 120.48ms, mfu 2.88%\n",
            "iter 2080: loss 1.1240, time 124.90ms, mfu 2.89%\n",
            "iter 2090: loss 1.1352, time 120.73ms, mfu 2.91%\n",
            "iter 2100: loss 1.1349, time 124.25ms, mfu 2.92%\n",
            "iter 2110: loss 1.1359, time 121.22ms, mfu 2.94%\n",
            "iter 2120: loss 1.1305, time 122.62ms, mfu 2.95%\n",
            "iter 2130: loss 1.1397, time 121.77ms, mfu 2.96%\n",
            "iter 2140: loss 1.1365, time 121.61ms, mfu 2.97%\n",
            "iter 2150: loss 1.1284, time 124.15ms, mfu 2.97%\n",
            "iter 2160: loss 1.1414, time 124.20ms, mfu 2.97%\n",
            "iter 2170: loss 1.1367, time 121.75ms, mfu 2.98%\n",
            "iter 2180: loss 1.1187, time 122.74ms, mfu 2.99%\n",
            "iter 2190: loss 1.1095, time 122.84ms, mfu 2.99%\n",
            "iter 2200: loss 1.1194, time 120.61ms, mfu 3.00%\n",
            "iter 2210: loss 1.1110, time 122.93ms, mfu 3.01%\n",
            "iter 2220: loss 1.1232, time 122.97ms, mfu 3.01%\n",
            "iter 2230: loss 1.1231, time 121.92ms, mfu 3.01%\n",
            "iter 2240: loss 1.1356, time 123.70ms, mfu 3.01%\n",
            "step 2250: train loss 1.0100, val loss 1.4719\n",
            "iter 2250: loss 1.1040, time 12966.83ms, mfu 2.71%\n",
            "iter 2260: loss 1.1040, time 125.12ms, mfu 2.74%\n",
            "iter 2270: loss 1.1425, time 121.68ms, mfu 2.77%\n",
            "iter 2280: loss 1.0969, time 121.66ms, mfu 2.80%\n",
            "iter 2290: loss 1.1535, time 121.30ms, mfu 2.83%\n",
            "iter 2300: loss 1.1172, time 120.93ms, mfu 2.85%\n",
            "iter 2310: loss 1.0925, time 122.41ms, mfu 2.87%\n",
            "iter 2320: loss 1.0992, time 121.58ms, mfu 2.89%\n",
            "iter 2330: loss 1.1028, time 121.99ms, mfu 2.91%\n",
            "iter 2340: loss 1.1236, time 123.12ms, mfu 2.92%\n",
            "iter 2350: loss 1.1000, time 121.09ms, mfu 2.94%\n",
            "iter 2360: loss 1.1014, time 122.79ms, mfu 2.95%\n",
            "iter 2370: loss 1.0974, time 123.26ms, mfu 2.95%\n",
            "iter 2380: loss 1.0846, time 123.26ms, mfu 2.96%\n",
            "iter 2390: loss 1.0864, time 123.69ms, mfu 2.97%\n",
            "iter 2400: loss 1.0765, time 120.98ms, mfu 2.98%\n",
            "iter 2410: loss 1.0720, time 123.09ms, mfu 2.98%\n",
            "iter 2420: loss 1.0792, time 121.26ms, mfu 2.99%\n",
            "iter 2430: loss 1.0588, time 120.39ms, mfu 3.00%\n",
            "iter 2440: loss 1.0537, time 123.87ms, mfu 3.00%\n",
            "iter 2450: loss 1.0758, time 118.88ms, mfu 3.02%\n",
            "iter 2460: loss 1.0872, time 121.75ms, mfu 3.02%\n",
            "iter 2470: loss 1.0853, time 121.85ms, mfu 3.02%\n",
            "iter 2480: loss 1.0859, time 122.96ms, mfu 3.02%\n",
            "iter 2490: loss 1.0610, time 124.76ms, mfu 3.02%\n",
            "step 2500: train loss 0.9590, val loss 1.4886\n",
            "iter 2500: loss 1.0800, time 12929.91ms, mfu 2.72%\n",
            "iter 2510: loss 1.0775, time 122.30ms, mfu 2.75%\n",
            "iter 2520: loss 1.0480, time 121.09ms, mfu 2.79%\n",
            "iter 2530: loss 1.0608, time 122.65ms, mfu 2.81%\n",
            "iter 2540: loss 1.0559, time 121.62ms, mfu 2.84%\n",
            "iter 2550: loss 1.0655, time 121.72ms, mfu 2.86%\n",
            "iter 2560: loss 1.0615, time 122.53ms, mfu 2.88%\n",
            "iter 2570: loss 1.0752, time 122.13ms, mfu 2.89%\n",
            "iter 2580: loss 1.0818, time 121.66ms, mfu 2.91%\n",
            "iter 2590: loss 1.0710, time 122.21ms, mfu 2.93%\n",
            "iter 2600: loss 1.0741, time 121.86ms, mfu 2.94%\n",
            "iter 2610: loss 1.0476, time 120.33ms, mfu 2.95%\n",
            "iter 2620: loss 1.0413, time 121.39ms, mfu 2.97%\n",
            "iter 2630: loss 1.0266, time 120.92ms, mfu 2.98%\n",
            "iter 2640: loss 1.0461, time 121.46ms, mfu 2.99%\n",
            "iter 2650: loss 1.0662, time 121.42ms, mfu 2.99%\n",
            "iter 2660: loss 1.0466, time 121.40ms, mfu 3.00%\n",
            "iter 2670: loss 1.0205, time 121.30ms, mfu 3.01%\n",
            "iter 2680: loss 1.0463, time 120.95ms, mfu 3.02%\n",
            "iter 2690: loss 1.0574, time 121.67ms, mfu 3.02%\n",
            "iter 2700: loss 1.0213, time 120.01ms, mfu 3.03%\n",
            "iter 2710: loss 1.0412, time 122.08ms, mfu 3.03%\n",
            "iter 2720: loss 1.0447, time 120.66ms, mfu 3.04%\n",
            "iter 2730: loss 1.0544, time 122.41ms, mfu 3.04%\n",
            "iter 2740: loss 1.0265, time 122.57ms, mfu 3.04%\n",
            "step 2750: train loss 0.9135, val loss 1.5104\n",
            "iter 2750: loss 1.0351, time 12905.70ms, mfu 2.74%\n",
            "iter 2760: loss 1.0271, time 124.15ms, mfu 2.76%\n",
            "iter 2770: loss 1.0300, time 120.65ms, mfu 2.80%\n",
            "iter 2780: loss 1.0212, time 122.75ms, mfu 2.82%\n",
            "iter 2790: loss 1.0406, time 122.15ms, mfu 2.84%\n",
            "iter 2800: loss 1.0119, time 120.00ms, mfu 2.87%\n",
            "iter 2810: loss 1.0451, time 122.05ms, mfu 2.89%\n",
            "iter 2820: loss 1.0255, time 123.84ms, mfu 2.90%\n",
            "iter 2830: loss 1.0377, time 123.82ms, mfu 2.91%\n",
            "iter 2840: loss 1.0019, time 118.94ms, mfu 2.93%\n",
            "iter 2850: loss 1.0306, time 121.94ms, mfu 2.95%\n",
            "iter 2860: loss 1.0237, time 120.33ms, mfu 2.96%\n",
            "iter 2870: loss 1.0016, time 121.80ms, mfu 2.97%\n",
            "iter 2880: loss 1.0270, time 122.32ms, mfu 2.98%\n",
            "iter 2890: loss 1.0150, time 122.13ms, mfu 2.99%\n",
            "iter 2900: loss 0.9963, time 121.72ms, mfu 2.99%\n",
            "iter 2910: loss 1.0427, time 122.31ms, mfu 3.00%\n",
            "iter 2920: loss 1.0190, time 120.50ms, mfu 3.01%\n",
            "iter 2930: loss 0.9971, time 124.17ms, mfu 3.01%\n",
            "iter 2940: loss 0.9860, time 120.15ms, mfu 3.02%\n",
            "iter 2950: loss 1.0231, time 120.79ms, mfu 3.02%\n",
            "iter 2960: loss 1.0000, time 119.78ms, mfu 3.03%\n",
            "iter 2970: loss 0.9940, time 121.10ms, mfu 3.04%\n",
            "iter 2980: loss 1.0014, time 122.55ms, mfu 3.04%\n",
            "iter 2990: loss 0.9899, time 119.77ms, mfu 3.04%\n",
            "step 3000: train loss 0.8673, val loss 1.5194\n",
            "iter 3000: loss 0.9866, time 12940.94ms, mfu 2.74%\n",
            "iter 3010: loss 0.9940, time 121.46ms, mfu 2.78%\n",
            "iter 3020: loss 0.9946, time 121.08ms, mfu 2.81%\n",
            "iter 3030: loss 1.0067, time 120.71ms, mfu 2.83%\n",
            "iter 3040: loss 1.0279, time 122.45ms, mfu 2.85%\n",
            "iter 3050: loss 0.9855, time 122.58ms, mfu 2.87%\n",
            "iter 3060: loss 0.9980, time 121.41ms, mfu 2.89%\n",
            "iter 3070: loss 1.0166, time 121.69ms, mfu 2.91%\n",
            "iter 3080: loss 1.0064, time 121.23ms, mfu 2.93%\n",
            "iter 3090: loss 0.9795, time 122.85ms, mfu 2.94%\n",
            "iter 3100: loss 0.9986, time 120.63ms, mfu 2.95%\n",
            "iter 3110: loss 0.9806, time 121.03ms, mfu 2.96%\n",
            "iter 3120: loss 0.9921, time 120.84ms, mfu 2.98%\n",
            "iter 3130: loss 0.9779, time 119.69ms, mfu 2.99%\n",
            "iter 3140: loss 0.9753, time 123.08ms, mfu 2.99%\n",
            "iter 3150: loss 0.9882, time 122.93ms, mfu 3.00%\n",
            "iter 3160: loss 1.0155, time 122.71ms, mfu 3.00%\n",
            "iter 3170: loss 0.9620, time 123.46ms, mfu 3.00%\n",
            "iter 3180: loss 0.9784, time 122.91ms, mfu 3.01%\n",
            "iter 3190: loss 0.9972, time 121.45ms, mfu 3.01%\n",
            "iter 3200: loss 0.9690, time 122.42ms, mfu 3.02%\n",
            "iter 3210: loss 0.9701, time 124.39ms, mfu 3.01%\n",
            "iter 3220: loss 0.9613, time 123.26ms, mfu 3.01%\n",
            "iter 3230: loss 0.9596, time 121.88ms, mfu 3.02%\n",
            "iter 3240: loss 0.9608, time 120.59ms, mfu 3.03%\n",
            "step 3250: train loss 0.8239, val loss 1.5611\n",
            "iter 3250: loss 0.9846, time 13005.14ms, mfu 2.73%\n",
            "iter 3260: loss 0.9668, time 121.73ms, mfu 2.76%\n",
            "iter 3270: loss 0.9772, time 121.06ms, mfu 2.79%\n",
            "iter 3280: loss 0.9445, time 122.32ms, mfu 2.82%\n",
            "iter 3290: loss 0.9432, time 121.26ms, mfu 2.84%\n",
            "iter 3300: loss 0.9451, time 122.29ms, mfu 2.86%\n",
            "iter 3310: loss 0.9571, time 121.51ms, mfu 2.88%\n",
            "iter 3320: loss 0.9727, time 123.57ms, mfu 2.90%\n",
            "iter 3330: loss 0.9581, time 122.78ms, mfu 2.91%\n",
            "iter 3340: loss 0.9502, time 123.32ms, mfu 2.92%\n",
            "iter 3350: loss 0.9538, time 123.11ms, mfu 2.93%\n",
            "iter 3360: loss 0.9362, time 120.13ms, mfu 2.95%\n",
            "iter 3370: loss 0.9606, time 121.77ms, mfu 2.96%\n",
            "iter 3380: loss 0.9514, time 119.99ms, mfu 2.97%\n",
            "iter 3390: loss 0.9561, time 121.31ms, mfu 2.98%\n",
            "iter 3400: loss 0.9596, time 120.91ms, mfu 2.99%\n",
            "iter 3410: loss 0.9444, time 121.94ms, mfu 3.00%\n",
            "iter 3420: loss 0.9479, time 123.41ms, mfu 3.00%\n",
            "iter 3430: loss 0.9430, time 120.38ms, mfu 3.01%\n",
            "iter 3440: loss 0.9693, time 122.02ms, mfu 3.02%\n",
            "iter 3450: loss 0.9509, time 123.12ms, mfu 3.02%\n",
            "iter 3460: loss 0.9453, time 121.70ms, mfu 3.02%\n",
            "iter 3470: loss 0.9385, time 121.83ms, mfu 3.03%\n",
            "iter 3480: loss 0.9504, time 120.78ms, mfu 3.03%\n",
            "iter 3490: loss 0.9106, time 120.70ms, mfu 3.04%\n",
            "step 3500: train loss 0.7790, val loss 1.5720\n",
            "iter 3500: loss 0.9011, time 12956.42ms, mfu 2.74%\n",
            "iter 3510: loss 0.9180, time 122.15ms, mfu 2.77%\n",
            "iter 3520: loss 0.9251, time 122.11ms, mfu 2.80%\n",
            "iter 3530: loss 0.9584, time 122.07ms, mfu 2.82%\n",
            "iter 3540: loss 0.9311, time 123.39ms, mfu 2.84%\n",
            "iter 3550: loss 0.9215, time 119.72ms, mfu 2.87%\n",
            "iter 3560: loss 0.9519, time 120.47ms, mfu 2.89%\n",
            "iter 3570: loss 0.9363, time 122.66ms, mfu 2.91%\n",
            "iter 3580: loss 0.9419, time 120.34ms, mfu 2.92%\n",
            "iter 3590: loss 0.9228, time 123.74ms, mfu 2.93%\n",
            "iter 3600: loss 0.9237, time 123.51ms, mfu 2.94%\n",
            "iter 3610: loss 0.9118, time 120.61ms, mfu 2.96%\n",
            "iter 3620: loss 0.9171, time 122.06ms, mfu 2.97%\n",
            "iter 3630: loss 0.9199, time 122.82ms, mfu 2.97%\n",
            "iter 3640: loss 0.9228, time 122.49ms, mfu 2.98%\n",
            "iter 3650: loss 0.9075, time 121.52ms, mfu 2.99%\n",
            "iter 3660: loss 0.9391, time 122.02ms, mfu 2.99%\n",
            "iter 3670: loss 0.9375, time 119.96ms, mfu 3.01%\n",
            "iter 3680: loss 0.9084, time 120.19ms, mfu 3.02%\n",
            "iter 3690: loss 0.9350, time 120.63ms, mfu 3.02%\n",
            "iter 3700: loss 0.8690, time 121.88ms, mfu 3.03%\n",
            "iter 3710: loss 0.8807, time 120.77ms, mfu 3.03%\n",
            "iter 3720: loss 0.9150, time 120.97ms, mfu 3.04%\n",
            "iter 3730: loss 0.9002, time 120.78ms, mfu 3.04%\n",
            "iter 3740: loss 0.9056, time 120.84ms, mfu 3.05%\n",
            "step 3750: train loss 0.7414, val loss 1.6029\n",
            "iter 3750: loss 0.9066, time 12930.37ms, mfu 2.74%\n",
            "iter 3760: loss 0.9350, time 120.16ms, mfu 2.78%\n",
            "iter 3770: loss 0.9320, time 121.75ms, mfu 2.81%\n",
            "iter 3780: loss 0.9233, time 120.87ms, mfu 2.84%\n",
            "iter 3790: loss 0.9007, time 121.67ms, mfu 2.86%\n",
            "iter 3800: loss 0.9027, time 122.74ms, mfu 2.88%\n",
            "iter 3810: loss 0.9152, time 122.55ms, mfu 2.89%\n",
            "iter 3820: loss 0.8884, time 120.54ms, mfu 2.91%\n",
            "iter 3830: loss 0.9023, time 124.12ms, mfu 2.92%\n",
            "iter 3840: loss 0.8860, time 123.29ms, mfu 2.93%\n",
            "iter 3850: loss 0.8922, time 120.07ms, mfu 2.95%\n",
            "iter 3860: loss 0.8694, time 123.09ms, mfu 2.96%\n",
            "iter 3870: loss 0.8979, time 122.02ms, mfu 2.97%\n",
            "iter 3880: loss 0.8883, time 119.82ms, mfu 2.98%\n",
            "iter 3890: loss 0.8921, time 123.36ms, mfu 2.98%\n",
            "iter 3900: loss 0.8796, time 120.61ms, mfu 3.00%\n",
            "iter 3910: loss 0.8910, time 123.93ms, mfu 3.00%\n",
            "iter 3920: loss 0.8786, time 121.49ms, mfu 3.00%\n",
            "iter 3930: loss 0.8860, time 122.38ms, mfu 3.01%\n",
            "iter 3940: loss 0.8757, time 120.69ms, mfu 3.02%\n",
            "iter 3950: loss 0.8747, time 122.06ms, mfu 3.02%\n",
            "iter 3960: loss 0.9120, time 121.97ms, mfu 3.02%\n",
            "iter 3970: loss 0.8914, time 120.87ms, mfu 3.03%\n",
            "iter 3980: loss 0.9046, time 119.26ms, mfu 3.04%\n",
            "iter 3990: loss 0.8761, time 122.49ms, mfu 3.04%\n",
            "step 4000: train loss 0.7089, val loss 1.6181\n",
            "iter 4000: loss 0.8590, time 12920.68ms, mfu 2.74%\n",
            "iter 4010: loss 0.8836, time 122.83ms, mfu 2.77%\n",
            "iter 4020: loss 0.8819, time 120.97ms, mfu 2.80%\n",
            "iter 4030: loss 0.8776, time 121.17ms, mfu 2.83%\n",
            "iter 4040: loss 0.8846, time 123.56ms, mfu 2.85%\n",
            "iter 4050: loss 0.8734, time 121.22ms, mfu 2.87%\n",
            "iter 4060: loss 0.8648, time 120.50ms, mfu 2.89%\n",
            "iter 4070: loss 0.8631, time 123.24ms, mfu 2.90%\n",
            "iter 4080: loss 0.8867, time 122.60ms, mfu 2.92%\n",
            "iter 4090: loss 0.8479, time 122.79ms, mfu 2.93%\n",
            "iter 4100: loss 0.8987, time 121.98ms, mfu 2.94%\n",
            "iter 4110: loss 0.8641, time 124.10ms, mfu 2.95%\n",
            "iter 4120: loss 0.8797, time 119.98ms, mfu 2.96%\n",
            "iter 4130: loss 0.8548, time 122.44ms, mfu 2.97%\n",
            "iter 4140: loss 0.8778, time 120.62ms, mfu 2.98%\n",
            "iter 4150: loss 0.8723, time 124.48ms, mfu 2.98%\n",
            "iter 4160: loss 0.8512, time 121.46ms, mfu 2.99%\n",
            "iter 4170: loss 0.8705, time 119.55ms, mfu 3.01%\n",
            "iter 4180: loss 0.8739, time 120.53ms, mfu 3.01%\n",
            "iter 4190: loss 0.8654, time 120.10ms, mfu 3.02%\n",
            "iter 4200: loss 0.8579, time 121.41ms, mfu 3.03%\n",
            "iter 4210: loss 0.8754, time 122.02ms, mfu 3.03%\n",
            "iter 4220: loss 0.8595, time 120.13ms, mfu 3.04%\n",
            "iter 4230: loss 0.8805, time 121.27ms, mfu 3.04%\n",
            "iter 4240: loss 0.8654, time 125.00ms, mfu 3.03%\n",
            "step 4250: train loss 0.6784, val loss 1.6442\n",
            "iter 4250: loss 0.8753, time 12971.90ms, mfu 2.73%\n",
            "iter 4260: loss 0.8598, time 121.98ms, mfu 2.77%\n",
            "iter 4270: loss 0.8634, time 123.85ms, mfu 2.79%\n",
            "iter 4280: loss 0.8539, time 120.59ms, mfu 2.82%\n",
            "iter 4290: loss 0.8318, time 124.14ms, mfu 2.84%\n",
            "iter 4300: loss 0.8307, time 119.36ms, mfu 2.87%\n",
            "iter 4310: loss 0.8535, time 122.13ms, mfu 2.89%\n",
            "iter 4320: loss 0.8466, time 122.94ms, mfu 2.90%\n",
            "iter 4330: loss 0.8621, time 121.72ms, mfu 2.92%\n",
            "iter 4340: loss 0.8286, time 120.55ms, mfu 2.93%\n",
            "iter 4350: loss 0.8388, time 122.68ms, mfu 2.94%\n",
            "iter 4360: loss 0.8612, time 120.94ms, mfu 2.96%\n",
            "iter 4370: loss 0.8579, time 121.67ms, mfu 2.97%\n",
            "iter 4380: loss 0.8347, time 122.28ms, mfu 2.98%\n",
            "iter 4390: loss 0.8672, time 122.80ms, mfu 2.98%\n",
            "iter 4400: loss 0.8440, time 122.04ms, mfu 2.99%\n",
            "iter 4410: loss 0.8630, time 120.71ms, mfu 3.00%\n",
            "iter 4420: loss 0.8611, time 120.82ms, mfu 3.01%\n",
            "iter 4430: loss 0.8399, time 121.04ms, mfu 3.01%\n",
            "iter 4440: loss 0.8514, time 123.13ms, mfu 3.02%\n",
            "iter 4450: loss 0.8549, time 125.24ms, mfu 3.01%\n",
            "iter 4460: loss 0.8366, time 120.26ms, mfu 3.02%\n",
            "iter 4470: loss 0.8530, time 123.37ms, mfu 3.02%\n",
            "iter 4480: loss 0.8332, time 120.94ms, mfu 3.03%\n",
            "iter 4490: loss 0.8419, time 125.16ms, mfu 3.02%\n",
            "step 4500: train loss 0.6544, val loss 1.6613\n",
            "iter 4500: loss 0.8533, time 13010.56ms, mfu 2.72%\n",
            "iter 4510: loss 0.8460, time 121.52ms, mfu 2.76%\n",
            "iter 4520: loss 0.8332, time 124.06ms, mfu 2.78%\n",
            "iter 4530: loss 0.8519, time 120.21ms, mfu 2.81%\n",
            "iter 4540: loss 0.8431, time 121.88ms, mfu 2.84%\n",
            "iter 4550: loss 0.8768, time 120.44ms, mfu 2.86%\n",
            "iter 4560: loss 0.8361, time 121.86ms, mfu 2.88%\n",
            "iter 4570: loss 0.8384, time 121.69ms, mfu 2.90%\n",
            "iter 4580: loss 0.8528, time 122.46ms, mfu 2.91%\n",
            "iter 4590: loss 0.8554, time 121.13ms, mfu 2.93%\n",
            "iter 4600: loss 0.8282, time 123.90ms, mfu 2.94%\n",
            "iter 4610: loss 0.8602, time 124.43ms, mfu 2.94%\n",
            "iter 4620: loss 0.8341, time 121.98ms, mfu 2.96%\n",
            "iter 4630: loss 0.8136, time 119.68ms, mfu 2.97%\n",
            "iter 4640: loss 0.8465, time 120.13ms, mfu 2.98%\n",
            "iter 4650: loss 0.8606, time 121.41ms, mfu 2.99%\n",
            "iter 4660: loss 0.8533, time 123.75ms, mfu 2.99%\n",
            "iter 4670: loss 0.8358, time 125.38ms, mfu 2.99%\n",
            "iter 4680: loss 0.8533, time 125.01ms, mfu 2.99%\n",
            "iter 4690: loss 0.8411, time 121.54ms, mfu 3.00%\n",
            "iter 4700: loss 0.8206, time 122.71ms, mfu 3.00%\n",
            "iter 4710: loss 0.7934, time 124.31ms, mfu 3.00%\n",
            "iter 4720: loss 0.8374, time 121.47ms, mfu 3.01%\n",
            "iter 4730: loss 0.8223, time 120.44ms, mfu 3.02%\n",
            "iter 4740: loss 0.8268, time 119.99ms, mfu 3.03%\n",
            "step 4750: train loss 0.6359, val loss 1.6802\n",
            "iter 4750: loss 0.8078, time 12964.86ms, mfu 2.73%\n",
            "iter 4760: loss 0.8218, time 120.94ms, mfu 2.76%\n",
            "iter 4770: loss 0.8055, time 122.22ms, mfu 2.79%\n",
            "iter 4780: loss 0.8098, time 122.11ms, mfu 2.82%\n",
            "iter 4790: loss 0.8343, time 120.34ms, mfu 2.84%\n",
            "iter 4800: loss 0.8281, time 121.64ms, mfu 2.87%\n",
            "iter 4810: loss 0.8343, time 122.82ms, mfu 2.88%\n",
            "iter 4820: loss 0.8217, time 120.77ms, mfu 2.90%\n",
            "iter 4830: loss 0.8183, time 118.91ms, mfu 2.93%\n",
            "iter 4840: loss 0.8372, time 121.27ms, mfu 2.94%\n",
            "iter 4850: loss 0.8202, time 122.64ms, mfu 2.95%\n",
            "iter 4860: loss 0.8241, time 120.26ms, mfu 2.97%\n",
            "iter 4870: loss 0.8103, time 121.76ms, mfu 2.97%\n",
            "iter 4880: loss 0.8312, time 121.85ms, mfu 2.98%\n",
            "iter 4890: loss 0.8079, time 120.45ms, mfu 2.99%\n",
            "iter 4900: loss 0.8054, time 122.71ms, mfu 3.00%\n",
            "iter 4910: loss 0.8333, time 121.00ms, mfu 3.01%\n",
            "iter 4920: loss 0.8248, time 121.29ms, mfu 3.01%\n",
            "iter 4930: loss 0.8068, time 123.53ms, mfu 3.01%\n",
            "iter 4940: loss 0.8003, time 122.40ms, mfu 3.02%\n",
            "iter 4950: loss 0.8276, time 124.20ms, mfu 3.01%\n",
            "iter 4960: loss 0.8309, time 122.10ms, mfu 3.02%\n",
            "iter 4970: loss 0.7909, time 120.75ms, mfu 3.03%\n",
            "iter 4980: loss 0.7935, time 122.88ms, mfu 3.03%\n",
            "iter 4990: loss 0.8205, time 123.50ms, mfu 3.03%\n",
            "step 5000: train loss 0.6210, val loss 1.7005\n",
            "iter 5000: loss 0.8196, time 12917.23ms, mfu 2.73%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python sample.py --out_dir=out-shakespeare-char --start=\"star \""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWb71dfyOe7e",
        "outputId": "0f68ed36-fb03-4425-cc6c-15ad8a758a17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: out_dir = out-shakespeare-char\n",
            "Overriding: start = star \n",
            "number of parameters: 10.65M\n",
            "Loading meta from data/shakespeare_char/meta.pkl...\n",
            "star the time of whom and is not\n",
            "three servants and all ready that hangs art\n",
            "that uses the tracterlessand are now an answer'd of mine\n",
            "what to the heart of hell.\n",
            "\n",
            "ROMEO:\n",
            "Yet I have late made overthrow of love,\n",
            "Will I see love to ear.\n",
            "\n",
            "Nurse:\n",
            "I have been a soldier, and too much\n",
            "To this despised sword. Good morrow, good Capitol!\n",
            "Thou dost say it, it is poor great to\n",
            "That dangerous for traitor! must I do along.\n",
            "\n",
            "Nurse:\n",
            "Well, how thou wert!\n",
            "\n",
            "JULIET:\n",
            "What a curse of the dishonour?\n",
            "\n",
            "Nurse:\n",
            "Go see the frown,\n",
            "---------------\n",
            "star with the king shall be so graced of all.\n",
            "Where is my heart is ready? when he has the world\n",
            "Is no more free than a case, more in my thumb.\n",
            "\n",
            "SLY:\n",
            "See the heavens of my soul words in meeting.\n",
            "\n",
            "DUKE OF AUMERLE:\n",
            "He do not commit me to the ground.\n",
            "\n",
            "KING RICHARD II:\n",
            "But a woman from the walls of his self;\n",
            "For the wolf have in himself I see thee.\n",
            "\n",
            "KING RICHARD II:\n",
            "Call your grace in impostion.\n",
            "\n",
            "DUKE OF YORK:\n",
            "I thank you. thou wilt say this most man carried\n",
            "For this man that it will be set upon thee,\n",
            "And\n",
            "---------------\n",
            "star we roar, will we are,\n",
            "We two conceive to the very hear of the cold\n",
            "That thou hast put our sweet wis sweet words.\n",
            "\n",
            "MARIANA:\n",
            "No, my lord,\n",
            "Henry must be so.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "This is a polly that ever known him!\n",
            "\n",
            "ISABELLA:\n",
            "I hide you, not my hearted to grieve a determined.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Ay, true of the world.\n",
            "\n",
            "ANGELO:\n",
            "So much he is here but slain in his head;\n",
            "And there late you come so.\n",
            "\n",
            "ISABELLA:\n",
            "Here in the gates villanor horse;\n",
            "And the number of mine enemy.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "My soul, wholesomet\n",
            "---------------\n",
            "star not so far\n",
            "In the king, and he seems and sweet as virtuous\n",
            "Could prove of when he was not to revenge\n",
            "But right on my Lartius.\n",
            "\n",
            "MARIANA:\n",
            "'Tis dead; a royal marvellous maid.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Marry, sir, I have seen him a land of lest\n",
            "A wife in his majesty: he could have here, here's\n",
            "a tale work of her banishments, I have but relied\n",
            "Unto 'like her unscorn.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Then will say, thy prosperous wars,\n",
            "That Jove hath been a king so here.\n",
            "\n",
            "Nurse:\n",
            "So shall I fight him so much for her be a half?\n",
            "---------------\n",
            "star site,\n",
            "And three the bloody of Tybalt's last.\n",
            "\n",
            "KING RICHARD II:\n",
            "Why she, the two executioners hath been slain,\n",
            "That stroke to conjure this adversaries\n",
            "Are offended in this docted more\n",
            "And bear the people is come to make his most\n",
            "Than presently of charge, that chamber'd\n",
            "All many that which a something does\n",
            "To anciently have touch'd. Alas, noble lord!\n",
            "Here is shame my father's son and honour\n",
            "That wayward i' the air: she was overboard\n",
            "A piper of her death, and her defills,\n",
            "Her best officers, and for\n",
            "---------------\n",
            "star in him, like to bear him for the rest\n",
            "Affection and make him talk of his peace; he would\n",
            "Have deserved of death shall be thought that run or more.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "I would you got you well, sir, if you have\n",
            "Hold that every more, you have been a guard of this,\n",
            "Which noise the issue of the more provided.\n",
            "\n",
            "Provost:\n",
            "I will not be not a feed of my brow, and the\n",
            "servant of God's name word will answer with the Capitol.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "I say, it is the gods of it, and therefore\n",
            "to see him for Rome is t\n",
            "---------------\n",
            "star shall be well appeard'd,\n",
            "And filling she, when in the royal issue,\n",
            "He is all the else other shall the air.\n",
            "\n",
            "GLOUCESTER:\n",
            "No, not to the Lord Warwick's title of our bahk;\n",
            "But she shall be long like that love of death;\n",
            "And her with a noble contract when they have but a cause,\n",
            "To save war frown'd interchance of their worship.\n",
            "\n",
            "GLOUCESTER:\n",
            "\n",
            "HASTINGS:\n",
            "In the oracle them on the speech or happiness.\n",
            "\n",
            "YORK:\n",
            "Here comes, my lord, your majesty\n",
            "Is not so grown to our fortune: be it strange all.\n",
            "\n",
            "GEORGE:\n",
            "But \n",
            "---------------\n",
            "star late,\n",
            "Hold me long not fellows to the saucy of death,\n",
            "I say 'twere a part of the east, and they are born\n",
            "And play'd your power in place, beholding in enterials;\n",
            "And to use your guilty and obidious\n",
            "Show your highness' honour, and to such a brother\n",
            "With silker blood as reason tears their heads,\n",
            "And here was quitted withal! Against the varlets\n",
            "Of their own throne with the power. The thoughts\n",
            "And his eye the forehead the same other there\n",
            "Within the advantage of those stranged war: but, in the worls\n",
            "\n",
            "---------------\n",
            "star have purchased\n",
            "My gracious master and play'd them like the sky\n",
            "Endured with her! Go, go, girls! well have heed\n",
            "The statue of the hope of his soldiers,\n",
            "Strength of his foul may have put upon them\n",
            "To the dangerous man. This silent and reason\n",
            "To have sure of many words: but it were my noble lesser\n",
            "Shall not be cause to first, and, as if they great\n",
            "I have touch'd him with their grief, and all the gooses\n",
            "That nest they will not love them. As I am so,\n",
            "As I can under him, as I can desire\n",
            "The parts in t\n",
            "---------------\n",
            "star with her body:\n",
            "I respect not; my lord, for that I am here is\n",
            "to enbroke my son: I do not know the tears of such a taste\n",
            "as best your terril\n",
            "Is thus. If it be not so a woman's fear,\n",
            "As you are not but your doubt is a world,\n",
            "I am of royal disloyal.\n",
            "\n",
            "MENENIUS:\n",
            "This is offence.\n",
            "\n",
            "MENENIUS:\n",
            "You have said the people, it is in war\n",
            "To the name of yourselves; but the varlet-winds be\n",
            "As the consent your times and speak of the parties\n",
            "With remove of his side. He thinks you, sir,\n",
            "The vaultain of Polixenes\n",
            "An\n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Song Writer"
      ],
      "metadata": {
        "id": "88DqCYjYWUMP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python data/lyrics/prepare.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQTryJ-_SJ4C",
        "outputId": "eb9ff4ce-eb61-47bf-e3bf-d98fd8d8331b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train has 22,308,928 tokens\n",
            "val has 2,456,916 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py config/train_lyrics.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0XI4S2tWvoI",
        "outputId": "bd9fe051-3238-456c-8b04-a2870e062999"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding config with config/train_lyrics.py:\n",
            "# train a miniature character-level shakespeare model\n",
            "# good for debugging and playing on macbooks and such\n",
            "\n",
            "out_dir = 'out-lyrics'\n",
            "eval_interval = 250 # keep frequent because we'll overfit\n",
            "eval_iters = 200\n",
            "log_interval = 10 # don't print too too often\n",
            "\n",
            "# we expect to overfit on this small dataset, so only save when val improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "wandb_log = False # override via command line if you like\n",
            "wandb_project = 'lyrics'\n",
            "wandb_run_name = 'mini-gpt'\n",
            "\n",
            "dataset = 'lyrics'\n",
            "gradient_accumulation_steps = 1\n",
            "batch_size = 64\n",
            "block_size = 256 # context of up to 256 previous characters\n",
            "\n",
            "# baby GPT model :)\n",
            "n_layer = 6\n",
            "n_head = 6\n",
            "n_embd = 384\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
            "max_iters = 5000\n",
            "lr_decay_iters = 5000 # make equal to max_iters usually\n",
            "min_lr = 1e-4 # learning_rate / 10 usually\n",
            "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
            "\n",
            "warmup_iters = 100 # not super necessary potentially\n",
            "\n",
            "# on macbook also add\n",
            "# device = 'cpu'  # run on cpu only\n",
            "# compile = False # do not torch compile the model\n",
            "\n",
            "tokens per iteration will be: 16,384\n",
            "Initializing a new model from scratch\n",
            "defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\n",
            "number of parameters: 29.94M\n",
            "num decayed parameter tensors: 26, with 30,031,872 parameters\n",
            "num non-decayed parameter tensors: 13, with 4,992 parameters\n",
            "using fused AdamW: True\n",
            "compiling the model... (takes a ~minute)\n",
            "step 0: train loss 10.7094, val loss 10.7105\n",
            "[2023-09-18 08:16:30,237] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-18 08:16:30,520] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-18 08:16:30,971] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-18 08:16:31,245] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-18 08:16:31,649] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-18 08:16:31,910] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-18 08:16:32,316] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-18 08:16:32,580] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-18 08:16:32,978] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-18 08:16:33,257] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-18 08:16:33,779] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-18 08:16:34,212] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "iter 0: loss 10.7344, time 41046.71ms, mfu -100.00%\n",
            "iter 10: loss 8.8351, time 320.85ms, mfu 3.06%\n",
            "iter 20: loss 7.9036, time 325.02ms, mfu 3.05%\n",
            "iter 30: loss 6.5586, time 318.31ms, mfu 3.05%\n",
            "iter 40: loss 5.4099, time 334.40ms, mfu 3.04%\n",
            "iter 50: loss 4.6167, time 327.02ms, mfu 3.04%\n",
            "iter 60: loss 4.4833, time 324.08ms, mfu 3.04%\n",
            "iter 70: loss 4.3515, time 324.92ms, mfu 3.03%\n",
            "iter 80: loss 4.2454, time 334.37ms, mfu 3.02%\n",
            "iter 90: loss 4.0503, time 327.50ms, mfu 3.02%\n",
            "iter 100: loss 4.0226, time 338.52ms, mfu 3.01%\n",
            "iter 110: loss 3.9475, time 331.29ms, mfu 3.00%\n",
            "iter 120: loss 3.8612, time 339.31ms, mfu 2.99%\n",
            "iter 130: loss 3.8511, time 338.07ms, mfu 2.98%\n",
            "iter 140: loss 3.9758, time 340.25ms, mfu 2.97%\n",
            "iter 150: loss 3.8317, time 346.89ms, mfu 2.96%\n",
            "iter 160: loss 3.6626, time 341.15ms, mfu 2.95%\n",
            "iter 170: loss 3.6629, time 344.59ms, mfu 2.94%\n",
            "iter 180: loss 3.6852, time 344.30ms, mfu 2.93%\n",
            "iter 190: loss 3.7087, time 356.28ms, mfu 2.91%\n",
            "iter 200: loss 3.7163, time 348.57ms, mfu 2.90%\n",
            "iter 210: loss 3.6334, time 347.42ms, mfu 2.89%\n",
            "iter 220: loss 3.6333, time 346.64ms, mfu 2.89%\n",
            "iter 230: loss 3.6447, time 355.06ms, mfu 2.88%\n",
            "iter 240: loss 3.7147, time 349.23ms, mfu 2.87%\n",
            "step 250: train loss 3.5135, val loss 3.5959\n",
            "saving checkpoint to out-lyrics\n",
            "iter 250: loss 3.5746, time 50061.95ms, mfu 2.58%\n",
            "iter 260: loss 3.6605, time 326.49ms, mfu 2.63%\n",
            "iter 270: loss 3.4628, time 380.18ms, mfu 2.62%\n",
            "iter 280: loss 3.6133, time 329.74ms, mfu 2.66%\n",
            "iter 290: loss 3.5141, time 327.69ms, mfu 2.69%\n",
            "iter 300: loss 3.4140, time 383.14ms, mfu 2.68%\n",
            "iter 310: loss 3.4340, time 327.70ms, mfu 2.71%\n",
            "iter 320: loss 3.4435, time 332.16ms, mfu 2.73%\n",
            "iter 330: loss 3.3476, time 357.10ms, mfu 2.73%\n",
            "iter 340: loss 3.4221, time 338.78ms, mfu 2.75%\n",
            "iter 350: loss 3.3921, time 344.20ms, mfu 2.76%\n",
            "iter 360: loss 3.3331, time 373.93ms, mfu 2.75%\n",
            "iter 370: loss 3.2886, time 325.09ms, mfu 2.77%\n",
            "iter 380: loss 3.2997, time 341.37ms, mfu 2.78%\n",
            "iter 390: loss 3.3286, time 344.35ms, mfu 2.79%\n",
            "iter 400: loss 3.3993, time 339.82ms, mfu 2.80%\n",
            "iter 410: loss 3.3082, time 341.00ms, mfu 2.81%\n",
            "iter 420: loss 3.1560, time 343.43ms, mfu 2.81%\n",
            "iter 430: loss 3.3345, time 340.16ms, mfu 2.82%\n",
            "iter 440: loss 3.2852, time 335.07ms, mfu 2.83%\n",
            "iter 450: loss 3.1445, time 366.40ms, mfu 2.81%\n",
            "iter 460: loss 3.2252, time 331.76ms, mfu 2.83%\n",
            "iter 470: loss 3.3621, time 326.08ms, mfu 2.85%\n",
            "iter 480: loss 3.3060, time 367.75ms, mfu 2.83%\n",
            "iter 490: loss 3.2731, time 331.21ms, mfu 2.84%\n",
            "step 500: train loss 3.1541, val loss 3.2681\n",
            "saving checkpoint to out-lyrics\n",
            "iter 500: loss 3.2678, time 51567.99ms, mfu 2.56%\n",
            "iter 510: loss 3.1221, time 350.06ms, mfu 2.58%\n",
            "iter 520: loss 3.2343, time 328.55ms, mfu 2.62%\n",
            "iter 530: loss 3.2545, time 339.64ms, mfu 2.65%\n",
            "iter 540: loss 3.1669, time 346.22ms, mfu 2.67%\n",
            "iter 550: loss 3.1681, time 367.37ms, mfu 2.67%\n",
            "iter 560: loss 3.1808, time 340.11ms, mfu 2.69%\n",
            "iter 570: loss 2.9993, time 338.49ms, mfu 2.71%\n",
            "iter 580: loss 3.0684, time 340.58ms, mfu 2.73%\n",
            "iter 590: loss 3.1672, time 355.79ms, mfu 2.73%\n",
            "iter 600: loss 3.1063, time 336.05ms, mfu 2.75%\n",
            "iter 610: loss 3.1796, time 349.95ms, mfu 2.75%\n",
            "iter 620: loss 3.1487, time 336.14ms, mfu 2.77%\n",
            "iter 630: loss 3.2595, time 333.89ms, mfu 2.79%\n",
            "iter 640: loss 3.1039, time 353.38ms, mfu 2.79%\n",
            "iter 650: loss 3.1260, time 328.52ms, mfu 2.81%\n",
            "iter 660: loss 3.1743, time 336.96ms, mfu 2.82%\n",
            "iter 670: loss 3.0398, time 338.50ms, mfu 2.82%\n",
            "iter 680: loss 3.0256, time 339.70ms, mfu 2.83%\n",
            "iter 690: loss 3.1837, time 340.62ms, mfu 2.84%\n",
            "iter 700: loss 3.1311, time 339.45ms, mfu 2.84%\n",
            "iter 710: loss 3.0578, time 344.98ms, mfu 2.84%\n",
            "iter 720: loss 2.9843, time 339.52ms, mfu 2.85%\n",
            "iter 730: loss 2.8871, time 346.02ms, mfu 2.84%\n",
            "iter 740: loss 3.1386, time 336.84ms, mfu 2.85%\n",
            "step 750: train loss 2.9509, val loss 3.0794\n",
            "saving checkpoint to out-lyrics\n",
            "iter 750: loss 3.0699, time 52654.11ms, mfu 2.57%\n",
            "iter 760: loss 3.1355, time 334.29ms, mfu 2.60%\n",
            "iter 770: loss 3.0461, time 371.87ms, mfu 2.61%\n",
            "iter 780: loss 3.0548, time 341.79ms, mfu 2.63%\n",
            "iter 790: loss 3.0981, time 338.30ms, mfu 2.66%\n",
            "iter 800: loss 2.9545, time 344.28ms, mfu 2.68%\n",
            "iter 810: loss 2.9950, time 340.53ms, mfu 2.70%\n",
            "iter 820: loss 3.0006, time 339.67ms, mfu 2.72%\n",
            "iter 830: loss 3.0107, time 342.46ms, mfu 2.73%\n",
            "iter 840: loss 2.9669, time 345.30ms, mfu 2.74%\n",
            "iter 850: loss 2.9816, time 344.66ms, mfu 2.75%\n",
            "iter 860: loss 2.9119, time 345.32ms, mfu 2.76%\n",
            "iter 870: loss 2.8690, time 342.44ms, mfu 2.77%\n",
            "iter 880: loss 2.8908, time 342.15ms, mfu 2.78%\n",
            "iter 890: loss 3.0243, time 338.79ms, mfu 2.79%\n",
            "iter 900: loss 2.8693, time 334.63ms, mfu 2.81%\n",
            "iter 910: loss 2.8886, time 341.04ms, mfu 2.81%\n",
            "iter 920: loss 2.7711, time 351.01ms, mfu 2.81%\n",
            "iter 930: loss 2.7693, time 335.38ms, mfu 2.82%\n",
            "iter 940: loss 2.9965, time 344.23ms, mfu 2.82%\n",
            "iter 950: loss 2.9714, time 346.26ms, mfu 2.83%\n",
            "iter 960: loss 2.8569, time 348.87ms, mfu 2.82%\n",
            "iter 970: loss 2.8247, time 337.08ms, mfu 2.83%\n",
            "iter 980: loss 2.8812, time 340.41ms, mfu 2.84%\n",
            "iter 990: loss 2.8017, time 336.47ms, mfu 2.84%\n",
            "step 1000: train loss 2.7322, val loss 2.8888\n",
            "saving checkpoint to out-lyrics\n",
            "iter 1000: loss 2.9430, time 52591.21ms, mfu 2.56%\n",
            "iter 1010: loss 2.9367, time 339.19ms, mfu 2.60%\n",
            "iter 1020: loss 2.8849, time 341.91ms, mfu 2.62%\n",
            "iter 1030: loss 3.0091, time 338.93ms, mfu 2.65%\n",
            "iter 1040: loss 2.7551, time 342.43ms, mfu 2.67%\n",
            "iter 1050: loss 2.8007, time 346.48ms, mfu 2.69%\n",
            "iter 1060: loss 2.7279, time 346.88ms, mfu 2.70%\n",
            "iter 1070: loss 2.7244, time 347.01ms, mfu 2.71%\n",
            "iter 1080: loss 2.8975, time 342.70ms, mfu 2.73%\n",
            "iter 1090: loss 2.8577, time 346.12ms, mfu 2.74%\n",
            "iter 1100: loss 2.8054, time 342.48ms, mfu 2.75%\n",
            "iter 1110: loss 2.7002, time 342.24ms, mfu 2.76%\n",
            "iter 1120: loss 2.8135, time 346.81ms, mfu 2.77%\n",
            "iter 1130: loss 2.7613, time 339.38ms, mfu 2.78%\n",
            "iter 1140: loss 2.8399, time 344.25ms, mfu 2.79%\n",
            "iter 1150: loss 2.7849, time 338.62ms, mfu 2.80%\n",
            "iter 1160: loss 2.7821, time 341.98ms, mfu 2.81%\n",
            "iter 1170: loss 2.8328, time 339.99ms, mfu 2.81%\n",
            "iter 1180: loss 2.5968, time 350.28ms, mfu 2.81%\n",
            "iter 1190: loss 2.8828, time 328.63ms, mfu 2.83%\n",
            "iter 1200: loss 2.8196, time 335.99ms, mfu 2.84%\n",
            "iter 1210: loss 2.7753, time 340.04ms, mfu 2.84%\n",
            "iter 1220: loss 2.7540, time 340.14ms, mfu 2.85%\n",
            "iter 1230: loss 2.8093, time 335.83ms, mfu 2.85%\n",
            "iter 1240: loss 2.6669, time 342.96ms, mfu 2.85%\n",
            "step 1250: train loss 2.5986, val loss 2.7550\n",
            "saving checkpoint to out-lyrics\n",
            "iter 1250: loss 2.7173, time 52657.26ms, mfu 2.57%\n",
            "iter 1260: loss 2.7580, time 342.77ms, mfu 2.60%\n",
            "iter 1270: loss 2.7758, time 344.02ms, mfu 2.62%\n",
            "iter 1280: loss 2.9503, time 327.64ms, mfu 2.66%\n",
            "iter 1290: loss 2.6431, time 332.63ms, mfu 2.69%\n",
            "iter 1300: loss 2.6463, time 376.61ms, mfu 2.68%\n",
            "iter 1310: loss 2.7660, time 341.64ms, mfu 2.70%\n",
            "iter 1320: loss 2.5831, time 342.23ms, mfu 2.72%\n",
            "iter 1330: loss 2.6424, time 350.03ms, mfu 2.73%\n",
            "iter 1340: loss 2.6138, time 346.37ms, mfu 2.74%\n",
            "iter 1350: loss 2.5950, time 343.32ms, mfu 2.75%\n",
            "iter 1360: loss 2.5652, time 342.45ms, mfu 2.76%\n",
            "iter 1370: loss 2.7021, time 339.14ms, mfu 2.77%\n",
            "iter 1380: loss 2.7181, time 337.32ms, mfu 2.79%\n",
            "iter 1390: loss 2.5449, time 344.94ms, mfu 2.79%\n",
            "iter 1400: loss 2.7111, time 333.75ms, mfu 2.81%\n",
            "iter 1410: loss 2.7260, time 331.97ms, mfu 2.82%\n",
            "iter 1420: loss 2.7248, time 331.86ms, mfu 2.83%\n",
            "iter 1430: loss 2.5817, time 330.74ms, mfu 2.85%\n",
            "iter 1440: loss 2.6341, time 361.60ms, mfu 2.83%\n",
            "iter 1450: loss 2.6330, time 336.46ms, mfu 2.84%\n",
            "iter 1460: loss 2.6728, time 343.19ms, mfu 2.84%\n",
            "iter 1470: loss 2.6687, time 363.81ms, mfu 2.83%\n",
            "iter 1480: loss 2.6855, time 334.75ms, mfu 2.84%\n",
            "iter 1490: loss 2.7519, time 334.96ms, mfu 2.85%\n",
            "step 1500: train loss 2.5136, val loss 2.6913\n",
            "saving checkpoint to out-lyrics\n",
            "iter 1500: loss 2.8112, time 52749.73ms, mfu 2.56%\n",
            "iter 1510: loss 2.7038, time 339.18ms, mfu 2.60%\n",
            "iter 1520: loss 2.7971, time 350.13ms, mfu 2.62%\n",
            "iter 1530: loss 2.6913, time 333.03ms, mfu 2.65%\n",
            "iter 1540: loss 2.6950, time 335.23ms, mfu 2.68%\n",
            "iter 1550: loss 2.7061, time 340.85ms, mfu 2.70%\n",
            "iter 1560: loss 2.5247, time 340.83ms, mfu 2.72%\n",
            "iter 1570: loss 2.5860, time 340.63ms, mfu 2.73%\n",
            "iter 1580: loss 2.6237, time 345.14ms, mfu 2.74%\n",
            "iter 1590: loss 2.6162, time 340.75ms, mfu 2.76%\n",
            "iter 1600: loss 2.6719, time 346.69ms, mfu 2.76%\n",
            "iter 1610: loss 2.4868, time 337.59ms, mfu 2.78%\n",
            "iter 1620: loss 2.7224, time 344.90ms, mfu 2.78%\n",
            "iter 1630: loss 2.6351, time 339.21ms, mfu 2.79%\n",
            "iter 1640: loss 2.5558, time 343.06ms, mfu 2.80%\n",
            "iter 1650: loss 2.3924, time 337.70ms, mfu 2.81%\n",
            "iter 1660: loss 2.5250, time 340.07ms, mfu 2.82%\n",
            "iter 1670: loss 2.3957, time 333.57ms, mfu 2.83%\n",
            "iter 1680: loss 2.4747, time 342.43ms, mfu 2.83%\n",
            "iter 1690: loss 2.6202, time 343.94ms, mfu 2.84%\n",
            "iter 1700: loss 2.6609, time 344.06ms, mfu 2.84%\n",
            "iter 1710: loss 2.6317, time 334.03ms, mfu 2.85%\n",
            "iter 1720: loss 2.5957, time 331.79ms, mfu 2.86%\n",
            "iter 1730: loss 2.6875, time 336.71ms, mfu 2.86%\n",
            "iter 1740: loss 2.4648, time 333.40ms, mfu 2.87%\n",
            "step 1750: train loss 2.4607, val loss 2.6298\n",
            "saving checkpoint to out-lyrics\n",
            "iter 1750: loss 2.6471, time 52730.87ms, mfu 2.59%\n",
            "iter 1760: loss 2.5462, time 340.36ms, mfu 2.62%\n",
            "iter 1770: loss 2.6703, time 345.97ms, mfu 2.64%\n",
            "iter 1780: loss 2.4086, time 330.44ms, mfu 2.67%\n",
            "iter 1790: loss 2.4923, time 328.17ms, mfu 2.70%\n",
            "iter 1800: loss 2.4133, time 345.62ms, mfu 2.72%\n",
            "iter 1810: loss 2.7201, time 345.17ms, mfu 2.73%\n",
            "iter 1820: loss 2.7000, time 341.85ms, mfu 2.74%\n",
            "iter 1830: loss 2.5246, time 348.55ms, mfu 2.75%\n",
            "iter 1840: loss 2.6345, time 341.84ms, mfu 2.76%\n",
            "iter 1850: loss 2.4357, time 347.15ms, mfu 2.77%\n",
            "iter 1860: loss 2.5837, time 342.41ms, mfu 2.78%\n",
            "iter 1870: loss 2.5488, time 344.43ms, mfu 2.78%\n",
            "iter 1880: loss 2.4707, time 342.31ms, mfu 2.79%\n",
            "iter 1890: loss 2.4749, time 341.65ms, mfu 2.80%\n",
            "iter 1900: loss 2.5066, time 338.02ms, mfu 2.81%\n",
            "iter 1910: loss 2.5349, time 340.34ms, mfu 2.82%\n",
            "iter 1920: loss 2.5040, time 343.27ms, mfu 2.82%\n",
            "iter 1930: loss 2.4854, time 353.21ms, mfu 2.82%\n",
            "iter 1940: loss 2.5770, time 340.94ms, mfu 2.82%\n",
            "iter 1950: loss 2.5570, time 337.03ms, mfu 2.83%\n",
            "iter 1960: loss 2.5127, time 330.83ms, mfu 2.84%\n",
            "iter 1970: loss 2.4670, time 350.94ms, mfu 2.84%\n",
            "iter 1980: loss 2.4477, time 350.25ms, mfu 2.84%\n",
            "iter 1990: loss 2.5474, time 343.27ms, mfu 2.84%\n",
            "step 2000: train loss 2.4091, val loss 2.5847\n",
            "saving checkpoint to out-lyrics\n",
            "iter 2000: loss 2.5101, time 48055.79ms, mfu 2.56%\n",
            "iter 2010: loss 2.4728, time 328.40ms, mfu 2.60%\n",
            "iter 2020: loss 2.4346, time 380.18ms, mfu 2.60%\n",
            "iter 2030: loss 2.5549, time 334.44ms, mfu 2.63%\n",
            "iter 2040: loss 2.3968, time 329.73ms, mfu 2.66%\n",
            "iter 2050: loss 2.4051, time 365.54ms, mfu 2.67%\n",
            "iter 2060: loss 2.4879, time 330.58ms, mfu 2.70%\n",
            "iter 2070: loss 2.5225, time 335.57ms, mfu 2.72%\n",
            "iter 2080: loss 2.6402, time 367.28ms, mfu 2.71%\n",
            "iter 2090: loss 2.4514, time 329.94ms, mfu 2.74%\n",
            "iter 2100: loss 2.4357, time 330.96ms, mfu 2.76%\n",
            "iter 2110: loss 2.5357, time 364.08ms, mfu 2.76%\n",
            "iter 2120: loss 2.4724, time 330.45ms, mfu 2.78%\n",
            "iter 2130: loss 2.4177, time 331.45ms, mfu 2.79%\n",
            "iter 2140: loss 2.4342, time 364.75ms, mfu 2.78%\n",
            "iter 2150: loss 2.5041, time 327.94ms, mfu 2.80%\n",
            "iter 2160: loss 2.4775, time 327.10ms, mfu 2.82%\n",
            "iter 2170: loss 2.4852, time 373.14ms, mfu 2.80%\n",
            "iter 2180: loss 2.4418, time 336.32ms, mfu 2.82%\n",
            "iter 2190: loss 2.4648, time 336.83ms, mfu 2.82%\n",
            "iter 2200: loss 2.5627, time 328.88ms, mfu 2.84%\n",
            "iter 2210: loss 2.5365, time 340.46ms, mfu 2.84%\n",
            "iter 2220: loss 2.5813, time 336.63ms, mfu 2.85%\n",
            "iter 2230: loss 2.4133, time 345.74ms, mfu 2.85%\n",
            "iter 2240: loss 2.3494, time 337.99ms, mfu 2.85%\n",
            "step 2250: train loss 2.3878, val loss 2.5725\n",
            "saving checkpoint to out-lyrics\n",
            "iter 2250: loss 2.5152, time 52707.70ms, mfu 2.57%\n",
            "iter 2260: loss 2.3801, time 337.96ms, mfu 2.60%\n",
            "iter 2270: loss 2.5045, time 336.70ms, mfu 2.63%\n",
            "iter 2280: loss 2.5189, time 338.83ms, mfu 2.66%\n",
            "iter 2290: loss 2.4246, time 343.08ms, mfu 2.68%\n",
            "iter 2300: loss 2.5223, time 346.20ms, mfu 2.70%\n",
            "iter 2310: loss 2.4659, time 344.22ms, mfu 2.71%\n",
            "iter 2320: loss 2.4438, time 350.52ms, mfu 2.72%\n",
            "iter 2330: loss 2.3799, time 344.84ms, mfu 2.73%\n",
            "iter 2340: loss 2.5604, time 346.97ms, mfu 2.74%\n",
            "iter 2350: loss 2.5582, time 342.58ms, mfu 2.75%\n",
            "iter 2360: loss 2.5558, time 348.11ms, mfu 2.76%\n",
            "iter 2370: loss 2.4780, time 339.85ms, mfu 2.77%\n",
            "iter 2380: loss 2.4739, time 344.64ms, mfu 2.78%\n",
            "iter 2390: loss 2.4760, time 342.57ms, mfu 2.79%\n",
            "iter 2400: loss 2.3934, time 343.71ms, mfu 2.79%\n",
            "iter 2410: loss 2.5330, time 340.32ms, mfu 2.80%\n",
            "iter 2420: loss 2.4029, time 345.84ms, mfu 2.81%\n",
            "iter 2430: loss 2.3752, time 343.11ms, mfu 2.81%\n",
            "iter 2440: loss 2.4331, time 332.81ms, mfu 2.82%\n",
            "iter 2450: loss 2.5200, time 339.60ms, mfu 2.83%\n",
            "iter 2460: loss 2.4267, time 325.26ms, mfu 2.85%\n",
            "iter 2470: loss 2.3631, time 336.66ms, mfu 2.86%\n",
            "iter 2480: loss 2.4071, time 348.18ms, mfu 2.85%\n",
            "iter 2490: loss 2.4460, time 331.06ms, mfu 2.86%\n",
            "step 2500: train loss 2.3664, val loss 2.5371\n",
            "saving checkpoint to out-lyrics\n",
            "iter 2500: loss 2.4451, time 52696.31ms, mfu 2.58%\n",
            "iter 2510: loss 2.5146, time 340.08ms, mfu 2.61%\n",
            "iter 2520: loss 2.3707, time 341.01ms, mfu 2.64%\n",
            "iter 2530: loss 2.4738, time 338.92ms, mfu 2.66%\n",
            "iter 2540: loss 2.4791, time 342.59ms, mfu 2.68%\n",
            "iter 2550: loss 2.4229, time 340.67ms, mfu 2.70%\n",
            "iter 2560: loss 2.3974, time 337.87ms, mfu 2.72%\n",
            "iter 2570: loss 2.3810, time 348.88ms, mfu 2.73%\n",
            "iter 2580: loss 2.4607, time 343.02ms, mfu 2.74%\n",
            "iter 2590: loss 2.3192, time 346.88ms, mfu 2.75%\n",
            "iter 2600: loss 2.3853, time 346.70ms, mfu 2.76%\n",
            "iter 2610: loss 2.5525, time 342.57ms, mfu 2.77%\n",
            "iter 2620: loss 2.5487, time 349.52ms, mfu 2.77%\n",
            "iter 2630: loss 2.3365, time 344.31ms, mfu 2.78%\n",
            "iter 2640: loss 2.4711, time 351.26ms, mfu 2.78%\n",
            "iter 2650: loss 2.4454, time 343.29ms, mfu 2.79%\n",
            "iter 2660: loss 2.5582, time 332.98ms, mfu 2.80%\n",
            "iter 2670: loss 2.4193, time 342.67ms, mfu 2.81%\n",
            "iter 2680: loss 2.4846, time 339.57ms, mfu 2.82%\n",
            "iter 2690: loss 2.3814, time 352.57ms, mfu 2.81%\n",
            "iter 2700: loss 2.3915, time 345.47ms, mfu 2.82%\n",
            "iter 2710: loss 2.3966, time 336.92ms, mfu 2.83%\n",
            "iter 2720: loss 2.4477, time 331.65ms, mfu 2.84%\n",
            "iter 2730: loss 2.3180, time 331.57ms, mfu 2.85%\n",
            "iter 2740: loss 2.4050, time 341.14ms, mfu 2.85%\n",
            "step 2750: train loss 2.3320, val loss 2.5207\n",
            "saving checkpoint to out-lyrics\n",
            "iter 2750: loss 2.5758, time 49085.87ms, mfu 2.57%\n",
            "iter 2760: loss 2.3995, time 332.54ms, mfu 2.61%\n",
            "iter 2770: loss 2.4657, time 339.53ms, mfu 2.64%\n",
            "iter 2780: loss 2.3412, time 341.68ms, mfu 2.66%\n",
            "iter 2790: loss 2.3966, time 338.57ms, mfu 2.68%\n",
            "iter 2800: loss 2.3481, time 337.87ms, mfu 2.70%\n",
            "iter 2810: loss 2.4980, time 336.91ms, mfu 2.73%\n",
            "iter 2820: loss 2.3630, time 344.31ms, mfu 2.74%\n",
            "iter 2830: loss 2.3556, time 334.91ms, mfu 2.76%\n",
            "iter 2840: loss 2.3044, time 339.59ms, mfu 2.77%\n",
            "iter 2850: loss 2.3906, time 343.52ms, mfu 2.78%\n",
            "iter 2860: loss 2.4173, time 343.51ms, mfu 2.79%\n",
            "iter 2870: loss 2.5768, time 345.11ms, mfu 2.79%\n",
            "iter 2880: loss 2.3751, time 340.35ms, mfu 2.80%\n",
            "iter 2890: loss 2.3224, time 338.67ms, mfu 2.81%\n",
            "iter 2900: loss 2.2767, time 337.17ms, mfu 2.82%\n",
            "iter 2910: loss 2.2965, time 339.70ms, mfu 2.83%\n",
            "iter 2920: loss 2.4717, time 339.31ms, mfu 2.83%\n",
            "iter 2930: loss 2.3871, time 335.35ms, mfu 2.84%\n",
            "iter 2940: loss 2.4296, time 338.98ms, mfu 2.85%\n",
            "iter 2950: loss 2.4247, time 338.28ms, mfu 2.85%\n",
            "iter 2960: loss 2.3369, time 332.63ms, mfu 2.86%\n",
            "iter 2970: loss 2.3806, time 343.18ms, mfu 2.86%\n",
            "iter 2980: loss 2.4354, time 337.10ms, mfu 2.87%\n",
            "iter 2990: loss 2.3650, time 336.26ms, mfu 2.87%\n",
            "step 3000: train loss 2.3007, val loss 2.4933\n",
            "saving checkpoint to out-lyrics\n",
            "iter 3000: loss 2.2788, time 52569.75ms, mfu 2.59%\n",
            "iter 3010: loss 2.4568, time 348.88ms, mfu 2.61%\n",
            "iter 3020: loss 2.2183, time 336.73ms, mfu 2.64%\n",
            "iter 3030: loss 2.3791, time 342.87ms, mfu 2.66%\n",
            "iter 3040: loss 2.3728, time 344.84ms, mfu 2.68%\n",
            "iter 3050: loss 2.4349, time 339.94ms, mfu 2.70%\n",
            "iter 3060: loss 2.3895, time 353.43ms, mfu 2.71%\n",
            "iter 3070: loss 2.3358, time 332.37ms, mfu 2.73%\n",
            "iter 3080: loss 2.4072, time 337.78ms, mfu 2.75%\n",
            "iter 3090: loss 2.2890, time 350.70ms, mfu 2.75%\n",
            "iter 3100: loss 2.5434, time 347.73ms, mfu 2.76%\n",
            "iter 3110: loss 2.2814, time 343.63ms, mfu 2.77%\n",
            "iter 3120: loss 2.3700, time 339.23ms, mfu 2.78%\n",
            "iter 3130: loss 2.5084, time 364.60ms, mfu 2.77%\n",
            "iter 3140: loss 2.3878, time 324.95ms, mfu 2.80%\n",
            "iter 3150: loss 2.3404, time 324.42ms, mfu 2.82%\n",
            "iter 3160: loss 2.3314, time 352.12ms, mfu 2.82%\n",
            "iter 3170: loss 2.5345, time 340.67ms, mfu 2.82%\n",
            "iter 3180: loss 2.3736, time 339.75ms, mfu 2.83%\n",
            "iter 3190: loss 2.2477, time 335.43ms, mfu 2.84%\n",
            "iter 3200: loss 2.4381, time 335.91ms, mfu 2.85%\n",
            "iter 3210: loss 2.3731, time 342.20ms, mfu 2.85%\n",
            "iter 3220: loss 2.4239, time 343.66ms, mfu 2.85%\n",
            "iter 3230: loss 2.2432, time 336.08ms, mfu 2.86%\n",
            "iter 3240: loss 2.4449, time 338.82ms, mfu 2.86%\n",
            "step 3250: train loss 2.2882, val loss 2.4783\n",
            "saving checkpoint to out-lyrics\n",
            "iter 3250: loss 2.2710, time 52696.69ms, mfu 2.58%\n",
            "iter 3260: loss 2.3837, time 353.36ms, mfu 2.59%\n",
            "iter 3270: loss 2.3993, time 336.65ms, mfu 2.63%\n",
            "iter 3280: loss 2.3047, time 344.03ms, mfu 2.65%\n",
            "iter 3290: loss 2.3227, time 346.72ms, mfu 2.67%\n",
            "iter 3300: loss 2.3401, time 344.28ms, mfu 2.69%\n",
            "iter 3310: loss 2.3444, time 341.17ms, mfu 2.70%\n",
            "iter 3320: loss 2.4347, time 343.98ms, mfu 2.72%\n",
            "iter 3330: loss 2.2958, time 344.42ms, mfu 2.73%\n",
            "iter 3340: loss 2.5331, time 340.25ms, mfu 2.75%\n",
            "iter 3350: loss 2.3780, time 346.44ms, mfu 2.75%\n",
            "iter 3360: loss 2.4691, time 342.30ms, mfu 2.77%\n",
            "iter 3370: loss 2.2231, time 339.66ms, mfu 2.78%\n",
            "iter 3380: loss 2.2843, time 340.48ms, mfu 2.79%\n",
            "iter 3390: loss 2.3953, time 348.17ms, mfu 2.79%\n",
            "iter 3400: loss 2.3953, time 340.01ms, mfu 2.80%\n",
            "iter 3410: loss 2.3212, time 333.91ms, mfu 2.81%\n",
            "iter 3420: loss 2.3381, time 324.87ms, mfu 2.83%\n",
            "iter 3430: loss 2.3927, time 337.24ms, mfu 2.84%\n",
            "iter 3440: loss 2.4512, time 338.02ms, mfu 2.85%\n",
            "iter 3450: loss 2.2376, time 332.28ms, mfu 2.86%\n",
            "iter 3460: loss 2.3723, time 351.73ms, mfu 2.85%\n",
            "iter 3470: loss 2.3345, time 338.62ms, mfu 2.86%\n",
            "iter 3480: loss 2.2223, time 335.85ms, mfu 2.86%\n",
            "iter 3490: loss 2.4080, time 340.72ms, mfu 2.86%\n",
            "step 3500: train loss 2.2657, val loss 2.4536\n",
            "saving checkpoint to out-lyrics\n",
            "iter 3500: loss 2.4362, time 52688.02ms, mfu 2.58%\n",
            "iter 3510: loss 2.3588, time 341.06ms, mfu 2.61%\n",
            "iter 3520: loss 2.2531, time 330.69ms, mfu 2.64%\n",
            "iter 3530: loss 2.3139, time 335.16ms, mfu 2.67%\n",
            "iter 3540: loss 2.4382, time 341.52ms, mfu 2.69%\n",
            "iter 3550: loss 2.3523, time 346.41ms, mfu 2.71%\n",
            "iter 3560: loss 2.4468, time 348.79ms, mfu 2.72%\n",
            "iter 3570: loss 2.2591, time 341.48ms, mfu 2.73%\n",
            "iter 3580: loss 2.2594, time 344.11ms, mfu 2.74%\n",
            "iter 3590: loss 2.3417, time 346.15ms, mfu 2.75%\n",
            "iter 3600: loss 2.3685, time 347.22ms, mfu 2.76%\n",
            "iter 3610: loss 2.3479, time 345.28ms, mfu 2.77%\n",
            "iter 3620: loss 2.3405, time 345.67ms, mfu 2.77%\n",
            "iter 3630: loss 2.3390, time 339.59ms, mfu 2.79%\n",
            "iter 3640: loss 2.2945, time 340.08ms, mfu 2.80%\n",
            "iter 3650: loss 2.2927, time 343.58ms, mfu 2.80%\n",
            "iter 3660: loss 2.3493, time 338.47ms, mfu 2.81%\n",
            "iter 3670: loss 2.3718, time 330.46ms, mfu 2.83%\n",
            "iter 3680: loss 2.2641, time 340.82ms, mfu 2.83%\n",
            "iter 3690: loss 2.4959, time 348.15ms, mfu 2.83%\n",
            "iter 3700: loss 2.4215, time 341.16ms, mfu 2.83%\n",
            "iter 3710: loss 2.3774, time 359.78ms, mfu 2.82%\n",
            "iter 3720: loss 2.2457, time 328.88ms, mfu 2.84%\n",
            "iter 3730: loss 2.4320, time 328.51ms, mfu 2.85%\n",
            "iter 3740: loss 2.4186, time 360.94ms, mfu 2.84%\n",
            "step 3750: train loss 2.2567, val loss 2.4630\n",
            "iter 3750: loss 2.2685, time 46853.66ms, mfu 2.56%\n",
            "iter 3760: loss 2.3285, time 341.58ms, mfu 2.59%\n",
            "iter 3770: loss 2.3250, time 337.03ms, mfu 2.62%\n",
            "iter 3780: loss 2.3046, time 345.98ms, mfu 2.64%\n",
            "iter 3790: loss 2.2507, time 338.04ms, mfu 2.67%\n",
            "iter 3800: loss 2.2795, time 342.73ms, mfu 2.69%\n",
            "iter 3810: loss 2.1308, time 343.79ms, mfu 2.70%\n",
            "iter 3820: loss 2.4022, time 342.00ms, mfu 2.72%\n",
            "iter 3830: loss 2.3298, time 343.36ms, mfu 2.73%\n",
            "iter 3840: loss 2.3269, time 336.20ms, mfu 2.75%\n",
            "iter 3850: loss 2.2980, time 336.68ms, mfu 2.77%\n",
            "iter 3860: loss 2.4052, time 340.15ms, mfu 2.78%\n",
            "iter 3870: loss 2.2445, time 336.15ms, mfu 2.79%\n",
            "iter 3880: loss 2.3945, time 343.15ms, mfu 2.80%\n",
            "iter 3890: loss 2.2817, time 338.30ms, mfu 2.81%\n",
            "iter 3900: loss 2.4346, time 342.12ms, mfu 2.82%\n",
            "iter 3910: loss 2.3689, time 341.93ms, mfu 2.82%\n",
            "iter 3920: loss 2.2826, time 341.18ms, mfu 2.83%\n",
            "iter 3930: loss 2.3952, time 343.00ms, mfu 2.83%\n",
            "iter 3940: loss 2.2317, time 338.43ms, mfu 2.84%\n",
            "iter 3950: loss 2.2419, time 343.39ms, mfu 2.84%\n",
            "iter 3960: loss 2.2938, time 340.10ms, mfu 2.84%\n",
            "iter 3970: loss 2.2400, time 335.30ms, mfu 2.85%\n",
            "iter 3980: loss 2.3386, time 338.81ms, mfu 2.85%\n",
            "iter 3990: loss 2.3240, time 338.00ms, mfu 2.86%\n",
            "step 4000: train loss 2.2333, val loss 2.4502\n",
            "saving checkpoint to out-lyrics\n",
            "iter 4000: loss 2.3176, time 52824.72ms, mfu 2.58%\n",
            "iter 4010: loss 2.3110, time 352.30ms, mfu 2.60%\n",
            "iter 4020: loss 2.3307, time 345.01ms, mfu 2.62%\n",
            "iter 4030: loss 2.3019, time 336.48ms, mfu 2.65%\n",
            "iter 4040: loss 2.2903, time 346.64ms, mfu 2.67%\n",
            "iter 4050: loss 2.3013, time 347.91ms, mfu 2.68%\n",
            "iter 4060: loss 2.3854, time 344.40ms, mfu 2.70%\n",
            "iter 4070: loss 2.3158, time 334.67ms, mfu 2.72%\n",
            "iter 4080: loss 2.3253, time 341.07ms, mfu 2.74%\n",
            "iter 4090: loss 2.2911, time 349.94ms, mfu 2.74%\n",
            "iter 4100: loss 2.3643, time 341.66ms, mfu 2.76%\n",
            "iter 4110: loss 2.1757, time 339.14ms, mfu 2.77%\n",
            "iter 4120: loss 2.2365, time 345.69ms, mfu 2.78%\n",
            "iter 4130: loss 2.3092, time 339.77ms, mfu 2.79%\n",
            "iter 4140: loss 2.3008, time 342.85ms, mfu 2.79%\n",
            "iter 4150: loss 2.2536, time 344.03ms, mfu 2.80%\n",
            "iter 4160: loss 2.2029, time 342.19ms, mfu 2.81%\n",
            "iter 4170: loss 2.2827, time 340.54ms, mfu 2.81%\n",
            "iter 4180: loss 2.2131, time 344.18ms, mfu 2.82%\n",
            "iter 4190: loss 2.2507, time 335.37ms, mfu 2.83%\n",
            "iter 4200: loss 2.2304, time 348.58ms, mfu 2.83%\n",
            "iter 4210: loss 2.3768, time 345.15ms, mfu 2.83%\n",
            "iter 4220: loss 2.2759, time 341.87ms, mfu 2.83%\n",
            "iter 4230: loss 2.3686, time 341.98ms, mfu 2.84%\n",
            "iter 4240: loss 2.2961, time 341.48ms, mfu 2.84%\n",
            "step 4250: train loss 2.2303, val loss 2.4328\n",
            "saving checkpoint to out-lyrics\n",
            "iter 4250: loss 2.2555, time 52469.31ms, mfu 2.56%\n",
            "iter 4260: loss 2.2934, time 342.39ms, mfu 2.59%\n",
            "iter 4270: loss 2.4800, time 335.40ms, mfu 2.62%\n",
            "iter 4280: loss 2.3877, time 347.24ms, mfu 2.64%\n",
            "iter 4290: loss 2.2404, time 345.10ms, mfu 2.66%\n",
            "iter 4300: loss 2.3015, time 343.90ms, mfu 2.68%\n",
            "iter 4310: loss 2.1782, time 343.21ms, mfu 2.70%\n",
            "iter 4320: loss 2.1431, time 346.38ms, mfu 2.71%\n",
            "iter 4330: loss 2.4082, time 350.74ms, mfu 2.72%\n",
            "iter 4340: loss 2.3827, time 344.45ms, mfu 2.73%\n",
            "iter 4350: loss 2.3902, time 349.54ms, mfu 2.74%\n",
            "iter 4360: loss 2.1985, time 342.49ms, mfu 2.75%\n",
            "iter 4370: loss 2.2183, time 343.64ms, mfu 2.76%\n",
            "iter 4380: loss 2.2657, time 333.14ms, mfu 2.78%\n",
            "iter 4390: loss 2.3537, time 325.37ms, mfu 2.80%\n",
            "iter 4400: loss 2.2324, time 353.12ms, mfu 2.80%\n",
            "iter 4410: loss 2.2498, time 336.36ms, mfu 2.81%\n",
            "iter 4420: loss 2.3375, time 323.93ms, mfu 2.83%\n",
            "iter 4430: loss 2.2884, time 355.94ms, mfu 2.83%\n",
            "iter 4440: loss 2.1987, time 337.82ms, mfu 2.83%\n",
            "iter 4450: loss 2.1151, time 334.84ms, mfu 2.84%\n",
            "iter 4460: loss 2.2974, time 341.29ms, mfu 2.85%\n",
            "iter 4470: loss 2.1902, time 337.86ms, mfu 2.85%\n",
            "iter 4480: loss 2.3092, time 352.55ms, mfu 2.84%\n",
            "iter 4490: loss 2.3998, time 339.66ms, mfu 2.85%\n",
            "step 4500: train loss 2.2269, val loss 2.4329\n",
            "iter 4500: loss 2.1581, time 46787.19ms, mfu 2.57%\n",
            "iter 4510: loss 2.3106, time 330.48ms, mfu 2.61%\n",
            "iter 4520: loss 2.2950, time 342.96ms, mfu 2.63%\n",
            "iter 4530: loss 2.1738, time 368.24ms, mfu 2.63%\n",
            "iter 4540: loss 2.3471, time 332.60ms, mfu 2.67%\n",
            "iter 4550: loss 2.2737, time 331.78ms, mfu 2.69%\n",
            "iter 4560: loss 2.4069, time 383.12ms, mfu 2.68%\n",
            "iter 4570: loss 2.3469, time 329.81ms, mfu 2.71%\n",
            "iter 4580: loss 2.3236, time 337.44ms, mfu 2.73%\n",
            "iter 4590: loss 2.3712, time 361.28ms, mfu 2.73%\n",
            "iter 4600: loss 2.2570, time 344.02ms, mfu 2.74%\n",
            "iter 4610: loss 2.3069, time 341.04ms, mfu 2.75%\n",
            "iter 4620: loss 2.2466, time 343.12ms, mfu 2.76%\n",
            "iter 4630: loss 2.3715, time 342.56ms, mfu 2.77%\n",
            "iter 4640: loss 2.3100, time 338.19ms, mfu 2.79%\n",
            "iter 4650: loss 2.3261, time 344.05ms, mfu 2.79%\n",
            "iter 4660: loss 2.3371, time 339.59ms, mfu 2.80%\n",
            "iter 4670: loss 2.2243, time 341.60ms, mfu 2.81%\n",
            "iter 4680: loss 2.3410, time 344.18ms, mfu 2.81%\n",
            "iter 4690: loss 2.3590, time 343.19ms, mfu 2.82%\n",
            "iter 4700: loss 2.4093, time 338.41ms, mfu 2.83%\n",
            "iter 4710: loss 2.2988, time 347.63ms, mfu 2.82%\n",
            "iter 4720: loss 2.3024, time 341.24ms, mfu 2.83%\n",
            "iter 4730: loss 2.2776, time 338.89ms, mfu 2.84%\n",
            "iter 4740: loss 2.3120, time 340.16ms, mfu 2.84%\n",
            "step 4750: train loss 2.1980, val loss 2.4259\n",
            "saving checkpoint to out-lyrics\n",
            "iter 4750: loss 2.3379, time 52950.11ms, mfu 2.56%\n",
            "iter 4760: loss 2.2481, time 349.20ms, mfu 2.58%\n",
            "iter 4770: loss 2.1959, time 333.11ms, mfu 2.62%\n",
            "iter 4780: loss 2.3828, time 342.32ms, mfu 2.64%\n",
            "iter 4790: loss 2.3048, time 343.28ms, mfu 2.67%\n",
            "iter 4800: loss 2.3291, time 344.05ms, mfu 2.68%\n",
            "iter 4810: loss 2.2977, time 346.13ms, mfu 2.70%\n",
            "iter 4820: loss 2.3371, time 344.62ms, mfu 2.71%\n",
            "iter 4830: loss 2.3974, time 345.84ms, mfu 2.73%\n",
            "iter 4840: loss 2.1874, time 347.14ms, mfu 2.74%\n",
            "iter 4850: loss 2.2257, time 347.98ms, mfu 2.74%\n",
            "iter 4860: loss 2.2316, time 341.08ms, mfu 2.76%\n",
            "iter 4870: loss 2.2087, time 344.20ms, mfu 2.77%\n",
            "iter 4880: loss 2.3135, time 341.88ms, mfu 2.78%\n",
            "iter 4890: loss 2.1776, time 343.44ms, mfu 2.78%\n",
            "iter 4900: loss 2.3075, time 343.58ms, mfu 2.79%\n",
            "iter 4910: loss 2.2952, time 339.19ms, mfu 2.80%\n",
            "iter 4920: loss 2.2418, time 342.54ms, mfu 2.81%\n",
            "iter 4930: loss 2.4373, time 338.91ms, mfu 2.82%\n",
            "iter 4940: loss 2.2409, time 340.87ms, mfu 2.82%\n",
            "iter 4950: loss 2.4084, time 342.15ms, mfu 2.83%\n",
            "iter 4960: loss 2.3244, time 340.19ms, mfu 2.83%\n",
            "iter 4970: loss 2.3220, time 331.56ms, mfu 2.84%\n",
            "iter 4980: loss 2.3931, time 337.90ms, mfu 2.85%\n",
            "iter 4990: loss 2.1452, time 336.07ms, mfu 2.86%\n",
            "step 5000: train loss 2.2103, val loss 2.4255\n",
            "saving checkpoint to out-lyrics\n",
            "iter 5000: loss 2.2833, time 47964.41ms, mfu 2.57%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python sample.py --out_dir=out-lyrics --start=\"fade \""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJxKjt4cXCkX",
        "outputId": "de904f48-49a3-43b3-8697-f63fe90d573a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: out_dir = out-lyrics\n",
            "Overriding: start = fade \n",
            "number of parameters: 29.94M\n",
            "No meta.pkl found, assuming GPT-2 encodings...\n",
            "fade  \n",
            "Make sure that they have to be  \n",
            "And it's been called to you  \n",
            "  \n",
            "[Chorus]  \n",
            "Do what I say  \n",
            "  \n",
            "[Chorus]  \n",
            "I will be the one who knows what it means  \n",
            "I will be the one who knows what it means  \n",
            "I will be the one who knows what it means  \n",
            "'Cause all I want is to be with you  \n",
            "  \n",
            "[Chorus]  \n",
            "  \n",
            "Ah  \n",
            "[Chorus]  \n",
            "Do what I say  \n",
            "I will be the one who knows what it means  \n",
            "I will be the one who knows what it means  \n",
            "I will be the one who knows what it means  \n",
            "I will be the one who knows what it means  \n",
            "I will be the one who knows what it means  \n",
            "I will be the one who knows what it means  \n",
            "I will be the one who knows what it means  \n",
            "I will be the one who knows what it means  \n",
            "I will be the one who knows what it means  \n",
            "I will be the one who knows what it means  \n",
            "I will be the one who knows what it means  \n",
            "I will be the one who knows what it means  \n",
            "I will be the one who knows what it means  \n",
            "I will be the one who knows what it means  \n",
            "I will be the one who knows what it means  \n",
            "I will be the one who knows what it means  \n",
            "I will be the one who knows what it means  \n",
            "I will be the one who knows what it means  \n",
            "I will be the one who knows what it means  \n",
            "I will be the one who knows what it means  \n",
            "I will be the one who knows what it means  \n",
            "I will be the one who knows what it means  \n",
            "I will be the one who knows what it means  \n",
            "I will be the one who knows what it means  \n",
            "I will be the one who knows what it means  \n",
            "\n",
            "---------------\n",
            "fade  \n",
            "You'll never realize  \n",
            "You'll never know  \n",
            "You've never felt this way before  \n",
            "  \n",
            "You're so unsure, so what you mean  \n",
            "You're so unsure, so what you mean  \n",
            "You're so tired, so what you mean  \n",
            "You're so tired, so what you mean\n",
            "\n",
            "\n",
            "I'm gonna be your freak in my car  \n",
            "I'm gonna be your freak in my car  \n",
            "I'm gonna be your freak in my car  \n",
            "I'm gonna be your freak in my car  \n",
            "I'm gonna be your freak in my car  \n",
            "I'm gonna be your freak in my car  \n",
            "I'm gonna be your freak in my car  \n",
            "I'm gonna be your freak in my car  \n",
            "I'm gonna be your freak in my car  \n",
            "I'm gonna be your freak in my car  \n",
            "I'm gonna be your freak in my car  \n",
            "I'm gonna be your freak in my car  \n",
            "I'm gonna be your freak in my car  \n",
            "I'm gonna be your freak in my car  \n",
            "I'm gonna be your freak in my car  \n",
            "I'm gonna be your freak in my car  \n",
            "I'm gonna be your freak in my car  \n",
            "I'm gonna be your freak in my car  \n",
            "I'm gonna be your freak in my car  \n",
            "I'm gonna be your freak in my car  \n",
            "I'm gonna be your freak in my car  \n",
            "I'm gonna be your freak in my car  \n",
            "I'm gonna be your freak in my car  \n",
            "  \n",
            "I'm gonna be your freak in my car  \n",
            "I'm gonna be your freak in my car  \n",
            "  \n",
            "I'm gonna be your freak in my car  \n",
            "I'm gonna be your freak in my car  \n",
            "I'm gonna be your freak in my car  \n",
            "I'm gonna be your freak in my car  \n",
            "I'm gonna be your freak in my car  \n",
            "I'm gonna be your freak in my car  \n",
            "I'm\n",
            "---------------\n",
            "fade  \n",
            "(Yeah, yeah)  \n",
            "  \n",
            "Well, it's a new day comin  \n",
            "It's a new day comin  \n",
            "It's a new day comin'  \n",
            "A new day comin  \n",
            "It's a new day comin  \n",
            "It's a new day comin  \n",
            "It's a new day comin  \n",
            "It's a new day comin, it's a new day comin  \n",
            "  \n",
            "It's a new day comin, it's a new day comin  \n",
            "It's a new day comin  \n",
            "It's a new day comin  \n",
            "It's a new day comin  \n",
            "It's a new day comin  \n",
            "It's a new day comin  \n",
            "It's a new day comin\n",
            "\n",
            "\n",
            "I looked at you  \n",
            "You opened your eyes  \n",
            "But you gave me the one to me  \n",
            "You gave me the one to me  \n",
            "And you gave me the one to me  \n",
            "You gave me the one to me  \n",
            "And you gave me the one to me  \n",
            "  \n",
            "How long did I think you  \n",
            "I tried to hold on to you  \n",
            "I tried to hold on to you  \n",
            "  \n",
            "How long did I think you  \n",
            "I tried to hold on to you  \n",
            "I tried to hold on to you  \n",
            "  \n",
            "How long did I think you  \n",
            "I tried to hold on to you  \n",
            "How long did I think you  \n",
            "I cried to hold on to you  \n",
            "  \n",
            "So I turned away  \n",
            "Still I thought you  \n",
            "I didn't cry to hold on to you\n",
            "\n",
            "\n",
            "I can't get down  \n",
            "I can't make a sound  \n",
            "Till it's cold  \n",
            "I'm feelin' good  \n",
            "When I can't see no more  \n",
            "And I'm walkin'  \n",
            "  \n",
            "There's no feeling that I can't\n",
            "---------------\n",
            "fade  \n",
            "We live, we live  \n",
            "We live  \n",
            "We live, we live, we live  \n",
            "We live, we live  \n",
            "We live, we live  \n",
            "We live  \n",
            "We live, we live  \n",
            "We live, we live, we live  \n",
            "We live, we live, we live  \n",
            "We live, we live  \n",
            "We live, we live, we live\n",
            "\n",
            "\n",
            "I feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like we're in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I'm in  \n",
            "Feel like I\n",
            "---------------\n",
            "fade  \n",
            "  \n",
            "Hate the piper,  \n",
            "Keep it alive  \n",
            "(it's really not that)  \n",
            "Them niggaz think you're a super bitch  \n",
            "And they all got the money for me on the TV  \n",
            "And yyall know what it is  \n",
            "Throw it down, throw it down  \n",
            "They gonna be with it, yeah yeah yeah  \n",
            "  \n",
            "[Chorus]  \n",
            "  \n",
            "[Verse 2]  \n",
            "Call my name, ain't it a shame  \n",
            "Kiss me like a gun to the  \n",
            "Yall know what I'm doing is  \n",
            "And that's why I brought my head up up  \n",
            "You know what I'm doing is  \n",
            "And that's why I brought my head up  \n",
            "You know what I'm doing is  \n",
            "And that's why I brought my head up  \n",
            "You know how I came to be with it  \n",
            "With I'm back in this bitch nigga nigga, don't leave it  \n",
            "They wanna be with it  \n",
            "And that's why I brought my head up  \n",
            "You know how I came to be with it  \n",
            "Cause that's why I came to be with it  \n",
            "With I'm back in this bitch nigga  \n",
            "This is the house that I hit the ground  \n",
            "And that's why I came to be with it  \n",
            "With I'm back in this bitch nigga  \n",
            "This is the house that I hit the ground  \n",
            "And that's why I came to be with it  \n",
            "Back in this bitch I hit the ground  \n",
            "  \n",
            "[Bridge]  \n",
            "  \n",
            "[Verse 2]  \n",
            "  \n",
            "[Bridge]  \n",
            "See you lookin' at me smile  \n",
            "This is the house that I hit the ground  \n",
            "And that's why I came to be with it  \n",
            "Cause that's why I came to be with it  \n",
            "Cause that's why I came to be with it  \n",
            "---------------\n",
            "fade  \n",
            "You thought you were here to stay  \n",
            "Now you're a star like you used to me  \n",
            "And now you're a star like you used to me  \n",
            "Whoa, whoa  \n",
            "  \n",
            "Love was a dreamer  \n",
            "And it was once in an end  \n",
            "I was here again  \n",
            "I was here again  \n",
            "I was here again  \n",
            "Well I caught a wall  \n",
            "The way you look at me  \n",
            "And I saw your picture  \n",
            "And all your friends  \n",
            "And your friends  \n",
            "You make them cry  \n",
            "  \n",
            "I was here again  \n",
            "I was here again  \n",
            "I was here again  \n",
            "I was here again  \n",
            "I was here again  \n",
            "I was here again  \n",
            "I was here again  \n",
            "I was here again  \n",
            "I was here again  \n",
            "I was here again  \n",
            "I was here again  \n",
            "I was here again\n",
            "\n",
            "\n",
            "Don't seem to know the pain  \n",
            "Doesn't seem to change  \n",
            "You're so close to my heart  \n",
            "Is so wrong, I know it  \n",
            "I'm not so wrong  \n",
            "It's time for me to change the time  \n",
            "  \n",
            "You're so far away  \n",
            "I've spent the night without you  \n",
            "I'm not your kind of fooling  \n",
            "Sometimes I think I need you  \n",
            "But it's not quite the same  \n",
            "I'm not so wrong  \n",
            "It's time for me to change the time  \n",
            "  \n",
            "Don't seem to know the pain  \n",
            "Doesn't seem to change  \n",
            "You're so far away  \n",
            "I've spent the night without you  \n",
            "I'm not so wrong, I know it  \n",
            "I'm not so wrong, I know it  \n",
            "I'm not so wrong, I know it  \n",
            "I'm not so wrong  \n",
            "It's time for me to change the time  \n",
            "---------------\n",
            "fade  \n",
            "  \n",
            "One, and one, and one, and a  \n",
            "One, and one, and a  \n",
            "One, and one, and a  \n",
            "One, and one, and a  \n",
            "One, and two, and a  \n",
            "One, and one, and a  \n",
            "One, and a  \n",
            "One, and a  \n",
            "One, and one, and a  \n",
            "\n",
            "\n",
            "One, and two, two, three, two, three, three  \n",
            "One, and two, three, three, four, three, one  \n",
            "One, and two, and two, four, four, four, one  \n",
            "The one and one, and one, and one, and one, and one  \n",
            "  \n",
            "One, and the one, and the one, and the one  \n",
            "One, and a  \n",
            "One, and a, and the one\n",
            "\n",
            "\n",
            "Cick  \n",
            "A hot kiss, like a cherry  \n",
            "A hot kiss, like a cherry  \n",
            "All I need is a ball  \n",
            "I'm a hot kiss  \n",
            "  \n",
            "I'm a hot kiss, like a cherry  \n",
            "Shoo  \n",
            "I'm a hot kiss, like a cherry  \n",
            "Shoo, woo  \n",
            "  \n",
            "I'm a hot kiss, like a cherry  \n",
            "Shoo, woo  \n",
            "  \n",
            "I'm a hot kiss, like a cherry  \n",
            "Shoo, woo, woo  \n",
            "I'm a hot kiss  \n",
            "  \n",
            "I'm a hot kiss, like a cherry  \n",
            "Shoo, woo  \n",
            "  \n",
            "Gonna shove you up, let will ride you down  \n",
            "I'm a hot kiss, like a cherry  \n",
            "Shoo, woo, woo  \n",
            "  \n",
            "I'm a hot kiss, like a cherry  \n",
            "Shoo, woo, woo  \n",
            "Gonna shove you up, let will ride you down  \n",
            "I'm a hot kiss, like\n",
            "---------------\n",
            "fade  \n",
            "If he wasn't worried  \n",
            "He was still not what he heard  \n",
            "Sometimes he was sick  \n",
            "He was still not what he heard  \n",
            "Sometimes he was sick  \n",
            "He was still not what he heard  \n",
            "Sometimes he was sick  \n",
            "He was still not what he heard  \n",
            "Sometimes he was sick  \n",
            "He was still not what he heard  \n",
            "Sometimes he was sick  \n",
            "He was still not what he heard  \n",
            "Sometimes he was sick  \n",
            "He was still not what he heard  \n",
            "Sometimes he was sick  \n",
            "He was still not what he heard  \n",
            "Sometimes he was sick  \n",
            "He was still not what he heard  \n",
            "Sometimes he was sick  \n",
            "He was still not what he heard  \n",
            "Sometimes he was sick  \n",
            "He was still not what he heard  \n",
            "Sometimes he was sick  \n",
            "He was still not what he heard  \n",
            "Sometimes he was sick  \n",
            "He was still not what he heard  \n",
            "Sometimes he was sick  \n",
            "He was still not what he heard  \n",
            "Sometimes he was sick  \n",
            "He was still not what he heard  \n",
            "Sometimes he was sick  \n",
            "He was still not what he heard  \n",
            "Sometimes he was sick  \n",
            "He was still not what he heard  \n",
            "Sometimes he was sick  \n",
            "He was still not what he heard  \n",
            "Sometimes he was sick  \n",
            "He was still not what he heard  \n",
            "Sometimes he was sick  \n",
            "He was still not what he heard  \n",
            "Sometimes he was sick  \n",
            "He was still not what he heard  \n",
            "Sometimes he was sick  \n",
            "He was still not what he heard  \n",
            "Sometimes he was sick  \n",
            "He was still not what he heard  \n",
            "Sometimes he was sick  \n",
            "He was still not what he heard  \n",
            "Sometimes he was sick  \n",
            "He was still not what he heard  \n",
            "Sometimes he was sick  \n",
            "He was still not what he heard  \n",
            "Sometimes he\n",
            "---------------\n",
            "fade  \n",
            "Come with me, come with me, come with me  \n",
            "Come with me, come with me\n",
            "\n",
            "\n",
            "Do you know?  \n",
            "Do you know?  \n",
            "Do you know?  \n",
            "Do you know?)  \n",
            "Do you know?  \n",
            "Do you know?  \n",
            "Do you know?  \n",
            "Do you know?  \n",
            "Do you know?  \n",
            "Do you know?  \n",
            "Do you know?  \n",
            "Do you know?  \n",
            "Do you know?  \n",
            "Do you know?  \n",
            "Do you know?  \n",
            "Do you know?  \n",
            "Do you know?  \n",
            "Do you know?  \n",
            "Do you know?  \n",
            "Do you know?  \n",
            "Do you know?  \n",
            "Do you know?\n",
            "\n",
            "\n",
            "Breathing, let's go  \n",
            "Breathing, let's go  \n",
            "Breathing, let's go  \n",
            "Breathing, let's go  \n",
            "  \n",
            "Breathing, let's go\n",
            "\n",
            "\n",
            "Walk that trigger off, turn it up  \n",
            "Let's go round and round and round  \n",
            "Gimme a body, so makin' it stop  \n",
            "Breathing, let's go round and round  \n",
            "Gimme a body, so makin' it stop  \n",
            "Breathing, let's go round and round  \n",
            "Gimme a body, so makin' it stop  \n",
            "Breathing, it, let's go round and round  \n",
            "  \n",
            "Breathing, let's go round and round  \n",
            "Gimme a body, so makin' it stop  \n",
            "Breathing, let's go round and round  \n",
            "Gimme a body, so makin' it stop  \n",
            "Breathing, let's go round and round  \n",
            "Gimme a body, so makin' it stop  \n",
            "Breathing, let's go round and round  \n",
            "  \n",
            "Breathing, let\n",
            "---------------\n",
            "fade  \n",
            "Here we go again  \n",
            "  \n",
            "[Bridge:]  \n",
            "You can't hide from me, you can't hide from me  \n",
            "You can't hide from me, you can't hide from me  \n",
            "I can't believe you're in love  \n",
            "But I can't believe you're in love  \n",
            "  \n",
            "[Chorus]  \n",
            "You can't hide from me  \n",
            "You can't hide from me  \n",
            "You can't hide from me  \n",
            "I can't believe you're in love  \n",
            "But I can't believe you're in love  \n",
            "So close to me  \n",
            "I can't believe you're in love  \n",
            "But I can't believe you're in love  \n",
            "  \n",
            "[Bridge]  \n",
            "I can't believe you're in love\n",
            "\n",
            "\n",
            "I want to take your hand  \n",
            "I want to take you to your hand  \n",
            "I want to take you to your hand  \n",
            "I want to take your hand  \n",
            "I want to take you to your hand  \n",
            "I want to take your hand  \n",
            "I want to take you to your hand  \n",
            "I want to take you to your hand  \n",
            "I want to take your hand  \n",
            "I want to take you to your hand  \n",
            "I want to take your hand  \n",
            "I want to take your hand  \n",
            "I want to take your hand  \n",
            "I want to take your hand  \n",
            "I want to take your hand  \n",
            "I want to make you love me  \n",
            "  \n",
            "I want to take your hand  \n",
            "I want to take your hand  \n",
            "I want to take your hand  \n",
            "I want to take your hand  \n",
            "I want to take your hand  \n",
            "I want to take your hand  \n",
            "I want to take your hand\n",
            "\n",
            "\n",
            "Listen to the laughter  \n",
            "It's in the laughter  \n",
            "The morning in the winter sun  \n",
            "The sun is shining on  \n",
            "With the sun to shine  \n",
            "You can\n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tang Poem Writer"
      ],
      "metadata": {
        "id": "Cp4NyTeIiJ5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python data/TangPoems/prepare.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dx4WcJgrhqw-",
        "outputId": "678a6a96-54a3-40cb-f0c4-c47b91f704e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train has 47,451 tokens\n",
            "val has 5,304 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py config/train_TangPoems.py --n_layer=4 --n_head=4 --n_embd=128 --max_iters=2000 --lr_decay_iters=2000 --dropout=0.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avTc2Rlbif1n",
        "outputId": "6e3c1f92-5f50-400b-f4ff-115d1345648e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding config with config/train_TangPoems.py:\n",
            "# train a miniature character-level shakespeare model\n",
            "# good for debugging and playing on macbooks and such\n",
            "\n",
            "out_dir = 'out-TangPoems'\n",
            "eval_interval = 250 # keep frequent because we'll overfit\n",
            "eval_iters = 200\n",
            "log_interval = 10 # don't print too too often\n",
            "\n",
            "# we expect to overfit on this small dataset, so only save when val improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "wandb_log = False # override via command line if you like\n",
            "wandb_project = 'TangPoems'\n",
            "wandb_run_name = 'mini-gpt'\n",
            "\n",
            "dataset = 'TangPoems'\n",
            "gradient_accumulation_steps = 1\n",
            "batch_size = 64\n",
            "block_size = 256 # context of up to 256 previous characters\n",
            "\n",
            "# baby GPT model :)\n",
            "n_layer = 6\n",
            "n_head = 6\n",
            "n_embd = 384\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
            "max_iters = 5000\n",
            "lr_decay_iters = 5000 # make equal to max_iters usually\n",
            "min_lr = 1e-4 # learning_rate / 10 usually\n",
            "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
            "\n",
            "warmup_iters = 100 # not super necessary potentially\n",
            "\n",
            "# on macbook also add\n",
            "# device = 'cpu'  # run on cpu only\n",
            "# compile = False # do not torch compile the model\n",
            "\n",
            "Overriding: n_layer = 4\n",
            "Overriding: n_head = 4\n",
            "Overriding: n_embd = 128\n",
            "Overriding: max_iters = 2000\n",
            "Overriding: lr_decay_iters = 2000\n",
            "Overriding: dropout = 0.2\n",
            "tokens per iteration will be: 16,384\n",
            "Initializing a new model from scratch\n",
            "defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\n",
            "number of parameters: 7.23M\n",
            "num decayed parameter tensors: 18, with 7,258,112 parameters\n",
            "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
            "using fused AdamW: True\n",
            "compiling the model... (takes a ~minute)\n",
            "step 0: train loss 10.8594, val loss 10.8649\n",
            "[2023-09-18 09:09:21,803] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-18 09:09:22,263] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-18 09:09:23,169] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-18 09:09:23,440] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-18 09:09:23,840] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-18 09:09:24,106] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-18 09:09:24,520] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-18 09:09:24,785] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "iter 0: loss 10.8579, time 37909.16ms, mfu -100.00%\n",
            "iter 10: loss 10.5672, time 161.70ms, mfu 1.46%\n",
            "iter 20: loss 10.1698, time 161.81ms, mfu 1.46%\n",
            "iter 30: loss 9.5158, time 162.77ms, mfu 1.46%\n",
            "iter 40: loss 8.5928, time 161.45ms, mfu 1.46%\n",
            "iter 50: loss 7.4337, time 160.96ms, mfu 1.46%\n",
            "iter 60: loss 6.1956, time 161.49ms, mfu 1.46%\n",
            "iter 70: loss 5.2608, time 162.52ms, mfu 1.46%\n",
            "iter 80: loss 4.7538, time 161.53ms, mfu 1.46%\n",
            "iter 90: loss 4.5256, time 161.52ms, mfu 1.46%\n",
            "iter 100: loss 4.3756, time 160.90ms, mfu 1.46%\n",
            "iter 110: loss 4.1795, time 161.38ms, mfu 1.46%\n",
            "iter 120: loss 4.0643, time 162.08ms, mfu 1.46%\n",
            "iter 130: loss 3.9726, time 162.27ms, mfu 1.46%\n",
            "iter 140: loss 3.8823, time 161.69ms, mfu 1.46%\n",
            "iter 150: loss 3.8211, time 162.07ms, mfu 1.46%\n",
            "iter 160: loss 3.7391, time 163.20ms, mfu 1.46%\n",
            "iter 170: loss 3.7134, time 161.86ms, mfu 1.46%\n",
            "iter 180: loss 3.5929, time 162.16ms, mfu 1.46%\n",
            "iter 190: loss 3.5620, time 162.04ms, mfu 1.46%\n",
            "iter 200: loss 3.5835, time 162.82ms, mfu 1.46%\n",
            "iter 210: loss 3.5345, time 162.97ms, mfu 1.46%\n",
            "iter 220: loss 3.5082, time 162.31ms, mfu 1.46%\n",
            "iter 230: loss 3.4835, time 162.03ms, mfu 1.46%\n",
            "iter 240: loss 3.4356, time 162.78ms, mfu 1.45%\n",
            "step 250: train loss 3.3618, val loss 3.6229\n",
            "saving checkpoint to out-TangPoems\n",
            "iter 250: loss 3.4161, time 29819.67ms, mfu 1.31%\n",
            "iter 260: loss 3.3923, time 166.04ms, mfu 1.32%\n",
            "iter 270: loss 3.3693, time 165.36ms, mfu 1.33%\n",
            "iter 280: loss 3.3313, time 166.62ms, mfu 1.34%\n",
            "iter 290: loss 3.3339, time 162.35ms, mfu 1.35%\n",
            "iter 300: loss 3.3219, time 167.61ms, mfu 1.36%\n",
            "iter 310: loss 3.3354, time 167.49ms, mfu 1.36%\n",
            "iter 320: loss 3.3018, time 168.82ms, mfu 1.37%\n",
            "iter 330: loss 3.3374, time 167.17ms, mfu 1.37%\n",
            "iter 340: loss 3.2644, time 173.91ms, mfu 1.37%\n",
            "iter 350: loss 3.2580, time 175.54ms, mfu 1.37%\n",
            "iter 360: loss 3.2448, time 172.99ms, mfu 1.37%\n",
            "iter 370: loss 3.2143, time 165.84ms, mfu 1.37%\n",
            "iter 380: loss 3.2128, time 170.62ms, mfu 1.37%\n",
            "iter 390: loss 3.1942, time 168.57ms, mfu 1.38%\n",
            "iter 400: loss 3.1926, time 169.13ms, mfu 1.38%\n",
            "iter 410: loss 3.1705, time 171.38ms, mfu 1.38%\n",
            "iter 420: loss 3.1500, time 170.32ms, mfu 1.38%\n",
            "iter 430: loss 3.1246, time 166.78ms, mfu 1.38%\n",
            "iter 440: loss 3.1354, time 168.03ms, mfu 1.38%\n",
            "iter 450: loss 3.1411, time 166.00ms, mfu 1.39%\n",
            "iter 460: loss 3.0720, time 167.84ms, mfu 1.39%\n",
            "iter 470: loss 3.0885, time 168.42ms, mfu 1.39%\n",
            "iter 480: loss 3.0106, time 167.43ms, mfu 1.39%\n",
            "iter 490: loss 3.0465, time 168.44ms, mfu 1.39%\n",
            "step 500: train loss 2.8601, val loss 3.2777\n",
            "saving checkpoint to out-TangPoems\n",
            "iter 500: loss 3.0872, time 27354.66ms, mfu 1.25%\n",
            "iter 510: loss 3.0209, time 162.48ms, mfu 1.27%\n",
            "iter 520: loss 2.9913, time 164.14ms, mfu 1.29%\n",
            "iter 530: loss 3.0040, time 165.22ms, mfu 1.30%\n",
            "iter 540: loss 2.9947, time 164.87ms, mfu 1.32%\n",
            "iter 550: loss 2.9712, time 167.19ms, mfu 1.33%\n",
            "iter 560: loss 2.9539, time 165.50ms, mfu 1.34%\n",
            "iter 570: loss 2.9590, time 164.12ms, mfu 1.35%\n",
            "iter 580: loss 2.8984, time 165.64ms, mfu 1.35%\n",
            "iter 590: loss 2.9251, time 165.01ms, mfu 1.36%\n",
            "iter 600: loss 2.9158, time 166.64ms, mfu 1.37%\n",
            "iter 610: loss 2.8693, time 164.43ms, mfu 1.37%\n",
            "iter 620: loss 2.8807, time 166.27ms, mfu 1.38%\n",
            "iter 630: loss 2.9206, time 166.03ms, mfu 1.38%\n",
            "iter 640: loss 2.8823, time 165.60ms, mfu 1.39%\n",
            "iter 650: loss 2.8702, time 165.17ms, mfu 1.39%\n",
            "iter 660: loss 2.8482, time 166.27ms, mfu 1.39%\n",
            "iter 670: loss 2.8009, time 166.24ms, mfu 1.40%\n",
            "iter 680: loss 2.7810, time 164.86ms, mfu 1.40%\n",
            "iter 690: loss 2.8291, time 165.33ms, mfu 1.40%\n",
            "iter 700: loss 2.7669, time 166.20ms, mfu 1.40%\n",
            "iter 710: loss 2.7555, time 166.65ms, mfu 1.41%\n",
            "iter 720: loss 2.8102, time 166.48ms, mfu 1.41%\n",
            "iter 730: loss 2.7641, time 168.72ms, mfu 1.41%\n",
            "iter 740: loss 2.7520, time 165.74ms, mfu 1.41%\n",
            "step 750: train loss 2.5240, val loss 3.2095\n",
            "saving checkpoint to out-TangPoems\n",
            "iter 750: loss 2.7551, time 27650.19ms, mfu 1.27%\n",
            "iter 760: loss 2.7364, time 163.64ms, mfu 1.29%\n",
            "iter 770: loss 2.6905, time 166.20ms, mfu 1.30%\n",
            "iter 780: loss 2.7174, time 166.00ms, mfu 1.31%\n",
            "iter 790: loss 2.6831, time 164.26ms, mfu 1.32%\n",
            "iter 800: loss 2.7143, time 166.96ms, mfu 1.33%\n",
            "iter 810: loss 2.6860, time 164.91ms, mfu 1.34%\n",
            "iter 820: loss 2.6752, time 165.21ms, mfu 1.35%\n",
            "iter 830: loss 2.6348, time 165.28ms, mfu 1.36%\n",
            "iter 840: loss 2.6536, time 167.88ms, mfu 1.36%\n",
            "iter 850: loss 2.6647, time 164.08ms, mfu 1.37%\n",
            "iter 860: loss 2.6362, time 166.07ms, mfu 1.38%\n",
            "iter 870: loss 2.6415, time 164.20ms, mfu 1.38%\n",
            "iter 880: loss 2.6449, time 164.19ms, mfu 1.39%\n",
            "iter 890: loss 2.6123, time 163.74ms, mfu 1.39%\n",
            "iter 900: loss 2.6442, time 166.04ms, mfu 1.40%\n",
            "iter 910: loss 2.6129, time 164.87ms, mfu 1.40%\n",
            "iter 920: loss 2.5699, time 167.88ms, mfu 1.40%\n",
            "iter 930: loss 2.5888, time 165.12ms, mfu 1.40%\n",
            "iter 940: loss 2.5728, time 164.59ms, mfu 1.41%\n",
            "iter 950: loss 2.5752, time 164.72ms, mfu 1.41%\n",
            "iter 960: loss 2.5330, time 164.29ms, mfu 1.41%\n",
            "iter 970: loss 2.5729, time 166.44ms, mfu 1.41%\n",
            "iter 980: loss 2.5505, time 165.27ms, mfu 1.41%\n",
            "iter 990: loss 2.5170, time 166.21ms, mfu 1.41%\n",
            "step 1000: train loss 2.2061, val loss 3.2815\n",
            "iter 1000: loss 2.5494, time 27336.30ms, mfu 1.27%\n",
            "iter 1010: loss 2.5240, time 164.53ms, mfu 1.29%\n",
            "iter 1020: loss 2.4712, time 166.17ms, mfu 1.30%\n",
            "iter 1030: loss 2.5142, time 164.45ms, mfu 1.32%\n",
            "iter 1040: loss 2.4799, time 166.72ms, mfu 1.33%\n",
            "iter 1050: loss 2.4660, time 164.40ms, mfu 1.34%\n",
            "iter 1060: loss 2.4619, time 164.82ms, mfu 1.35%\n",
            "iter 1070: loss 2.4550, time 165.37ms, mfu 1.35%\n",
            "iter 1080: loss 2.4490, time 165.12ms, mfu 1.36%\n",
            "iter 1090: loss 2.4548, time 165.07ms, mfu 1.37%\n",
            "iter 1100: loss 2.4286, time 166.70ms, mfu 1.37%\n",
            "iter 1110: loss 2.3911, time 166.80ms, mfu 1.38%\n",
            "iter 1120: loss 2.4250, time 166.16ms, mfu 1.38%\n",
            "iter 1130: loss 2.3954, time 165.65ms, mfu 1.39%\n",
            "iter 1140: loss 2.3787, time 164.34ms, mfu 1.39%\n",
            "iter 1150: loss 2.3947, time 164.92ms, mfu 1.39%\n",
            "iter 1160: loss 2.3619, time 164.62ms, mfu 1.40%\n",
            "iter 1170: loss 2.3613, time 165.01ms, mfu 1.40%\n",
            "iter 1180: loss 2.3644, time 163.75ms, mfu 1.41%\n",
            "iter 1190: loss 2.3605, time 166.49ms, mfu 1.41%\n",
            "iter 1200: loss 2.3561, time 165.50ms, mfu 1.41%\n",
            "iter 1210: loss 2.3713, time 165.23ms, mfu 1.41%\n",
            "iter 1220: loss 2.3132, time 164.35ms, mfu 1.41%\n",
            "iter 1230: loss 2.3298, time 164.71ms, mfu 1.42%\n",
            "iter 1240: loss 2.3095, time 164.79ms, mfu 1.42%\n",
            "step 1250: train loss 1.9030, val loss 3.4059\n",
            "iter 1250: loss 2.3099, time 27362.86ms, mfu 1.28%\n",
            "iter 1260: loss 2.3017, time 165.45ms, mfu 1.29%\n",
            "iter 1270: loss 2.3225, time 165.62ms, mfu 1.30%\n",
            "iter 1280: loss 2.2943, time 164.60ms, mfu 1.32%\n",
            "iter 1290: loss 2.2848, time 163.46ms, mfu 1.33%\n",
            "iter 1300: loss 2.2773, time 164.33ms, mfu 1.34%\n",
            "iter 1310: loss 2.2803, time 165.62ms, mfu 1.35%\n",
            "iter 1320: loss 2.2532, time 164.31ms, mfu 1.36%\n",
            "iter 1330: loss 2.2504, time 164.49ms, mfu 1.37%\n",
            "iter 1340: loss 2.2330, time 164.21ms, mfu 1.37%\n",
            "iter 1350: loss 2.2333, time 165.01ms, mfu 1.38%\n",
            "iter 1360: loss 2.2416, time 165.36ms, mfu 1.38%\n",
            "iter 1370: loss 2.2246, time 163.85ms, mfu 1.39%\n",
            "iter 1380: loss 2.2196, time 165.26ms, mfu 1.39%\n",
            "iter 1390: loss 2.2242, time 164.13ms, mfu 1.40%\n",
            "iter 1400: loss 2.2362, time 164.78ms, mfu 1.40%\n",
            "iter 1410: loss 2.2216, time 165.31ms, mfu 1.40%\n",
            "iter 1420: loss 2.1816, time 164.40ms, mfu 1.41%\n",
            "iter 1430: loss 2.1991, time 164.12ms, mfu 1.41%\n",
            "iter 1440: loss 2.1935, time 164.82ms, mfu 1.41%\n",
            "iter 1450: loss 2.2021, time 163.10ms, mfu 1.42%\n",
            "iter 1460: loss 2.1772, time 163.89ms, mfu 1.42%\n",
            "iter 1470: loss 2.1799, time 164.51ms, mfu 1.42%\n",
            "iter 1480: loss 2.1595, time 164.45ms, mfu 1.42%\n",
            "iter 1490: loss 2.1835, time 165.78ms, mfu 1.42%\n",
            "step 1500: train loss 1.6772, val loss 3.5105\n",
            "iter 1500: loss 2.1604, time 27262.50ms, mfu 1.28%\n",
            "iter 1510: loss 2.1617, time 164.42ms, mfu 1.30%\n",
            "iter 1520: loss 2.1550, time 164.99ms, mfu 1.31%\n",
            "iter 1530: loss 2.1285, time 163.51ms, mfu 1.32%\n",
            "iter 1540: loss 2.1609, time 163.57ms, mfu 1.33%\n",
            "iter 1550: loss 2.1672, time 164.14ms, mfu 1.34%\n",
            "iter 1560: loss 2.1227, time 163.98ms, mfu 1.35%\n",
            "iter 1570: loss 2.1282, time 164.18ms, mfu 1.36%\n",
            "iter 1580: loss 2.1265, time 164.00ms, mfu 1.37%\n",
            "iter 1590: loss 2.1234, time 165.51ms, mfu 1.38%\n",
            "iter 1600: loss 2.1276, time 164.30ms, mfu 1.38%\n",
            "iter 1610: loss 2.1166, time 164.06ms, mfu 1.39%\n",
            "iter 1620: loss 2.1008, time 163.50ms, mfu 1.39%\n",
            "iter 1630: loss 2.1211, time 163.42ms, mfu 1.40%\n",
            "iter 1640: loss 2.1145, time 164.21ms, mfu 1.40%\n",
            "iter 1650: loss 2.1123, time 163.72ms, mfu 1.41%\n",
            "iter 1660: loss 2.1009, time 163.99ms, mfu 1.41%\n",
            "iter 1670: loss 2.1000, time 164.99ms, mfu 1.41%\n",
            "iter 1680: loss 2.0929, time 164.13ms, mfu 1.41%\n",
            "iter 1690: loss 2.0853, time 163.91ms, mfu 1.42%\n",
            "iter 1700: loss 2.0887, time 164.57ms, mfu 1.42%\n",
            "iter 1710: loss 2.0830, time 163.25ms, mfu 1.42%\n",
            "iter 1720: loss 2.0402, time 163.11ms, mfu 1.42%\n",
            "iter 1730: loss 2.0653, time 164.69ms, mfu 1.42%\n",
            "iter 1740: loss 2.0857, time 163.89ms, mfu 1.43%\n",
            "step 1750: train loss 1.5441, val loss 3.5911\n",
            "iter 1750: loss 2.0636, time 27222.27ms, mfu 1.28%\n",
            "iter 1760: loss 2.1149, time 164.92ms, mfu 1.30%\n",
            "iter 1770: loss 2.0908, time 163.57ms, mfu 1.31%\n",
            "iter 1780: loss 2.0643, time 163.65ms, mfu 1.33%\n",
            "iter 1790: loss 2.0518, time 163.45ms, mfu 1.34%\n",
            "iter 1800: loss 2.0508, time 163.65ms, mfu 1.35%\n",
            "iter 1810: loss 2.0598, time 164.31ms, mfu 1.36%\n",
            "iter 1820: loss 2.0446, time 163.46ms, mfu 1.37%\n",
            "iter 1830: loss 2.0401, time 163.80ms, mfu 1.37%\n",
            "iter 1840: loss 2.0575, time 164.20ms, mfu 1.38%\n",
            "iter 1850: loss 2.0351, time 164.32ms, mfu 1.39%\n",
            "iter 1860: loss 2.0426, time 163.86ms, mfu 1.39%\n",
            "iter 1870: loss 2.0322, time 164.16ms, mfu 1.40%\n",
            "iter 1880: loss 2.0464, time 163.25ms, mfu 1.40%\n",
            "iter 1890: loss 2.0286, time 164.96ms, mfu 1.40%\n",
            "iter 1900: loss 2.0303, time 164.56ms, mfu 1.41%\n",
            "iter 1910: loss 2.0226, time 164.77ms, mfu 1.41%\n",
            "iter 1920: loss 2.0013, time 165.06ms, mfu 1.41%\n",
            "iter 1930: loss 2.0474, time 163.61ms, mfu 1.41%\n",
            "iter 1940: loss 2.0265, time 163.71ms, mfu 1.42%\n",
            "iter 1950: loss 2.0503, time 163.25ms, mfu 1.42%\n",
            "iter 1960: loss 2.0070, time 164.80ms, mfu 1.42%\n",
            "iter 1970: loss 1.9987, time 164.98ms, mfu 1.42%\n",
            "iter 1980: loss 2.0347, time 165.30ms, mfu 1.42%\n",
            "iter 1990: loss 2.0223, time 163.87ms, mfu 1.42%\n",
            "step 2000: train loss 1.4528, val loss 3.6412\n",
            "iter 2000: loss 1.9964, time 27331.51ms, mfu 1.28%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python sample.py --out_dir=out-TangPoems --start=\"星空\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxKVcN9Si9Uu",
        "outputId": "44332e25-8bb1-4057-f84b-57703d4a4033"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: out_dir = out-TangPoems\n",
            "Overriding: start = 星空\n",
            "number of parameters: 7.23M\n",
            "No meta.pkl found, assuming GPT-2 encodings...\n",
            "星空。夜頭夜將書，錦湘水天承山夜。照疑少欲節正夋，問裡如拂西東。\n",
            "�風悲曹春秋處，晚欲徒光濚落滿。顃路書�空，窮頯矃鶴返陽鄉。得見將絃因送與，燕客�貅�親此成頭。強歸來道，半頭皆鶴空。蔡�武來，生由來微事罷，結知散玉擁川。\n",
            "清故洲鳥，衣極紅紅鸨泥，堆涯帆君百事。海宮鳥今薄達，九�山夜雲盤白。雨袨雨，雲梁江深�東。處跡，處徒�深君照。高黨�忽風恥，舟苑天，藥�臨止澹書雨夜。\n",
            "今本薄涹，楚客高棲鐘萬木未邉聞。\n",
            "絃相本洞�，獨夜鳳�迴�\n",
            "---------------\n",
            "星空山中，山�中自聲，句洲湖紅。漢窮情臥，深來輕惟江臥自，青鳥籠宴髮。\n",
            "稅�飛離，一夜沙。春，西落日日秋送春頭，愁里山中家。梁舟煙羅�帝，沙深花有羅�。\n",
            "青飛黃春，少洞�飛�何。\n",
            "頭見樹東�無，驱未紅晱�。無金不見春滅，相名連空憶。\n",
            "秋青，江曾清葉芙賢。水楚，山�隨見紈寂�。\n",
            "苔劍�陽陽高樓�雙豈，髮雲處苑�自。秋恨�，萬金露碜殞彩曙。地馬野沙樹，清人不照�暮，楚晾恨君鳥裡秋費。\n",
            "名不萬里無，羊落寄欲江海�。\n",
            "洞�龍�同何國，託去藥，野�\n",
            "---------------\n",
            "星空將，上草未莫�何道。�朝�傷水，洛�至已。\n",
            "江樹落臥聞落秋羅。\n",
            "時鳥萬帝船，大落鳥風望風。\n",
            "對長啼夜舊，馬空山�雨花。\n",
            "青光斷此煙，清滿秋落聞。\n",
            "飒疑宮露簾紅涺，淚搘菨生。\n",
            "漠江洗明月洗冥，江上陽。\n",
            "楚�陵闊�多�海，照。清時之渼�暮，鏮來月挫�聲。\n",
            "深�清�。\n",
            "影�東不來關，對欲帷深不客，長客�酒樹。\n",
            "煙鐘色，柴�棄絃生渹�多，萬釣欲奩蘆深�隨，落奏空深�紅銙堆。\n",
            "春夢至，逢妾雲初雁客楚依。\n",
            "樹入，未白水�。\n",
            "迢情月時紅行，間遠臺。自朔別薄，�\n",
            "---------------\n",
            "星空陰，顧時有魂，夜雲倚雨。枕知煙水山池薄素。年落華山外，萬重去方問河。明王白鬢�雲相雪，漁我使想照。\n",
            "秋曉高淨�洞�一淚，悲好悠水�身手。\n",
            "陽陽風雪，望白日月陃洲息。渚雲清木，相朝處。身湖春夜，�劍�豈夕。落日未聞，風因柳息恨清。\n",
            "共�極見水自，行樹兩螗心苦下塵深悵。\n",
            "滄忽豻�雁裡，此深�千湘草東。春太碧煙，渡秋楓。\n",
            "秋憶�深渡，今�鳥照花門。燭�，知羅��臺。辭身異，灰雨欲。頭�楚山寒魂離飲。垂前，青慸�涯江洮年�如。\n",
            "慰窮武來盡，�\n",
            "---------------\n",
            "星空到色。何手還露，心邊天�江林。卻綠照言道，孤都數連行顏輕葉。笑問悲露洞�，海歸未紅關河。江上將罅�寂誰，陵琴清�對重。燈江�秋喧門高。殘雁漸送風泊�，雲離與歸未。露楓辭淚楚暗，階時無煙。天江奴清待東不動，樂音誰獨竹。深�時絃，山寒空未到。態下��清梁滿，詩��關鴛白魂�香玉。\n",
            "煙秋竟啼，塵琶青雪梁深波世楚。淺江絃金，黃花紅�。知露露玉一磬池，萬金門遠客黛獨葉前。頭釣嘶臨，來杯疑玉白。\n",
            "秋窺地，塵�页知姓�山�。可忽中亦林流，佳�\n",
            "---------------\n",
            "星空，風景場輕處風。\n",
            "書舟掃急煙馬，山中宮木。流幽白雨夜，路變�春長。歲來殿衣豟為，罷滿素滿。漠�高深�年魂日客，滿何起西山。各傜歸轉，深萬重�開。\n",
            "地飲抱裳聲，此雨澹田外風。寒芳苦面，一脰柳出�處毌薰。以葉�，今明有青江。竈猶辈寰，一送鳥大男蕭滴場。\n",
            "深�西渡，沉亂黃城江湖所。\n",
            "潴�一夜，摧��倒湖，持��江湖何趙�。\n",
            "將時迢�君�，窮玉白狂�遲自月。\n",
            "梅山。\n",
            "歸見處皇，美服�日意。\n",
            "寥西聞至，空鴛羽滴。青接，清漫梨�共鳥，鬥波大春洞��\n",
            "---------------\n",
            "星空村。古熟波人夜銀，朝兒長江後。田深不能�，霧。\n",
            "笑回可深流山下閨，流�上一聲朝。歸無，澹�露君座歸。迴傷歡，急來深未曲。\n",
            "羅人洰羅夜，得笑滄桃海。\n",
            "樹落童，碧墅，離�月東。\n",
            "深浸�澹，赽人芙�，莫待不歌西薪露。明未鳥，不悵長持。恨�暮情相思火，胡風啷草水。\n",
            "白黃楑�楊��處，邨�送川�天隱。\n",
            "自此簾�川江漠深。震��草正蠶，小山�交絃。空澸花難林此，離�波山�。\n",
            "朝�時遠煙秋霓�，獨青合暗鳥。\n",
            "雨，微雁王高江水�青棹。君客別君，相持高採�江好。游�窮所對宅人猶�\n",
            "---------------\n",
            "星空綠頭至去，清。\n",
            "恨星上馬。君蹭白黃風來，自音今邊。日至絕頭，白日十邊邊黃度�霧雨。陽陽毛長，天隔處哀，稹�雨阿，君復悟楚地敢人。\n",
            "雨老去，入當老平里高自雲，高樓。出盡人朝高樓。賊地腸，雲奏不後�。至今湖江定雲，稱�楚草杯。\n",
            "迱渡客�白葉對�荒，亂露洛玉息。山�少紅雲勒，行可艹�。盤深渡，夷書�下夢空空獨。落家美有聲遮，功\n",
            "此紩�峰山�波對，輕至千白日此東。野人使苦平玉，絃一欲贗後興。羍�秋豈，兒��僕佳清風泥洗。\n",
            "問有嫋，悲�弟曲齻�節嗟。\n",
            "---------------\n",
            "星空月。白高曉征行，亦憐相。滿至與近處，新雁疏自成長。陵人烽�絲瓢，五酒沙鄉紅。陽鴦�犀流萬壓，夜顧莫。悠上西老落，箭霜�鴻。晰坐白音�柳風，此栭烹�音。\n",
            "春飛烽�水有落蹭高，與玉老美�酒疏葟鳥。白風書覆，�綠花盡窮。與�洅貴晚，四下隔時行日歸�，涯拂�自見落連滿即洲帝。\n",
            "戍木眼落處，望紅樹酒雨恨曙。顧游�滿雲煙，自錅�樹紛精巖�夜。\n",
            "江沾眠帝病，西獨無長水稀。清來羿�，顑時邱�塘水。\n",
            "失對漸夜，何空火逢�暮時飛�。鏡鳥解，空青天�\n",
            "---------------\n",
            "星空羅。日寒風高渡殘草葉，葉漢落旤殘水。故角弟好空拂寢，無山已騎。\n",
            "羟露之雨雲紙山�漿�流，以窗�終。荒可塵草虛，辭朝。夜清羽�，雲誰處賤值霜。東花三鐘只楊時黯，絃葉。天憶關舉後鳥郎川到，魂來大四絃知。十�欲影，素洅書夢道。\n",
            "漫晴�流門相新，昏獨漸離沙時邊�蟭洞眠不逐�。\n",
            "玉淚又繸�日婋，今漁蒼訪豈夜，登舟青空需。驚為未久，仙石照聞。將軍無對落，樹星景江好草別，門未忽臗。\n",
            "四江上煙節�，秋高山�波。\n",
            "美青崔津，及猶�莫薄。語靜君不�\n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CEO's Avatar"
      ],
      "metadata": {
        "id": "iqLNm7E7NvuV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python data/CEO_words/prepare_char.py"
      ],
      "metadata": {
        "id": "eYTomIgaluI1",
        "outputId": "3adea6bc-0add-468d-bbc4-86875ed45dfe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all the unique characters: \n",
            " %&(),0234567ABCDEGIMPRSTVabdefghiklmnopqrstuvwy、。「」一三上下不且世並中主之乖也了事二些享人什仍他仗付代以件任仿估似但位低佔何作你佳使來供依保信修個倍們候值做停偵備傳僅像價億優元充兆先免入內全兩公共其具再冷出刃分切列別利到則前剖剩割創劃力功加助努勁動務勝勢勵包化匹升半卑卓即原去參及反取受只可台司各合同向君否含吸呈周味命和品哪唯商問善器嚴因困固圍在地均坡型域執基堅場境增壟外多夠夢大天失奧奮好如妥始姿嫌子存學它守安完定宜宣宰害家容寓察實寶將專尊尋對導小少尚就局屈展屢層屬山峯崗崛嶄工巧巨差己已市希席帶常幣幫平年序度廠廣建式引強影往征很後得從復循微徵心必志快念怕思急性恆恥息恰情惕想意感愧態慎慧慮慾懈應懲懷成我或戰所手才打扮找技抄抉把投折抱拆拓括持指按挑捨掌排採探接控推提揚握揭揮搬搶撓擁擅擇擎據擴擾攀收改放故效敏敗數料新斷方於旅早明易是時景智曉更最會有服望朝未本杜東松析果查核根案條楚業極概構樂標模樣機次欲止正此步歷死段毅每比毖毫水永求決沒油治況法注洞津活浪海消涉涵深淺清減測渴游湧源準滑滯滿演漲漸潛激為烈無然煉煙照熟熱燃營爭爲物特犯狀猛獨獲率王現球理環生產用由界畏留畢略異當發的益目直相省看真眾瞭瞻知短石研破硝確礎示社祕神科移程種稱積穩穫究空突窺立竟競符策算管節範篩精糾約納級紛素索細終結絕給統經維編緩練縮總繞繼續纔缺置美羞義習考者而耗聯聰背能臨自至致臺與興般良色若華著葛蒐處虛蜂行術衡表被裡複襲西要覆規視覺觀角解言訂計討記訣設許試該認誤說誰調談論諦諸謂謙謹證識警變讓象貌貳買費資賣質賴購走起超越趣趨足跌距路蹈蹤躍身躲車軍軟輕轉轍迅迎近述迷追透逐這通速造連進遊運過道達遜遠適遭遷選避邁還那部都配釋重量銳錯鍵長開間闊關闡防降除陷階隔障隨集雖離難雨電需露霸靈靜非靠面革響頂項順須預領頭題願顯飛飽養餘首驗驚髓體高鬆鬥鴻麼點龐％（），：；\n",
            "vocab size: 792\n",
            "train has 369,549 tokens\n",
            "val has 41,061 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py config/train_CEO_words.py --n_layer=4 --n_head=4 --n_embd=128 --max_iters=2000 --lr_decay_iters=2000 --dropout=0.2"
      ],
      "metadata": {
        "id": "_-_jfin3OB7x",
        "outputId": "eff63f5f-e70f-4c13-8442-0134beff0040",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding config with config/train_CEO_words.py:\n",
            "# train a miniature character-level shakespeare model\n",
            "# good for debugging and playing on macbooks and such\n",
            "\n",
            "out_dir = 'out-CEO_words'\n",
            "eval_interval = 250 # keep frequent because we'll overfit\n",
            "eval_iters = 200\n",
            "log_interval = 10 # don't print too too often\n",
            "\n",
            "# we expect to overfit on this small dataset, so only save when val improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "wandb_log = False # override via command line if you like\n",
            "wandb_project = 'CEO_words'\n",
            "wandb_run_name = 'mini-gpt'\n",
            "\n",
            "dataset = 'CEO_words'\n",
            "gradient_accumulation_steps = 1\n",
            "batch_size = 64\n",
            "block_size = 256 # context of up to 256 previous characters\n",
            "\n",
            "# baby GPT model :)\n",
            "n_layer = 6\n",
            "n_head = 6\n",
            "n_embd = 384\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
            "max_iters = 5000\n",
            "lr_decay_iters = 5000 # make equal to max_iters usually\n",
            "min_lr = 1e-4 # learning_rate / 10 usually\n",
            "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
            "\n",
            "warmup_iters = 100 # not super necessary potentially\n",
            "\n",
            "# on macbook also add\n",
            "# device = 'cpu'  # run on cpu only\n",
            "# compile = False # do not torch compile the model\n",
            "\n",
            "Overriding: n_layer = 4\n",
            "Overriding: n_head = 4\n",
            "Overriding: n_embd = 128\n",
            "Overriding: max_iters = 2000\n",
            "Overriding: lr_decay_iters = 2000\n",
            "Overriding: dropout = 0.2\n",
            "tokens per iteration will be: 16,384\n",
            "found vocab_size = 792 (inside data/CEO_words/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 0.89M\n",
            "num decayed parameter tensors: 18, with 920,576 parameters\n",
            "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
            "using fused AdamW: True\n",
            "compiling the model... (takes a ~minute)\n",
            "step 0: train loss 6.6670, val loss 6.6655\n",
            "[2023-09-20 06:14:44,658] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-20 06:14:45,127] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-20 06:14:46,193] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-20 06:14:46,455] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-20 06:14:46,860] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-20 06:14:47,138] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-20 06:14:47,543] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-09-20 06:14:47,808] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "iter 0: loss 6.6699, time 32272.49ms, mfu -100.00%\n",
            "iter 10: loss 6.4687, time 28.20ms, mfu 1.29%\n",
            "iter 20: loss 6.2354, time 26.43ms, mfu 1.29%\n",
            "iter 30: loss 5.9268, time 26.07ms, mfu 1.30%\n",
            "iter 40: loss 5.5752, time 30.37ms, mfu 1.29%\n",
            "iter 50: loss 5.2105, time 28.23ms, mfu 1.29%\n",
            "iter 60: loss 4.8022, time 26.33ms, mfu 1.30%\n",
            "iter 70: loss 4.3886, time 25.78ms, mfu 1.31%\n",
            "iter 80: loss 3.9798, time 21.98ms, mfu 1.35%\n",
            "iter 90: loss 3.5871, time 22.60ms, mfu 1.37%\n",
            "iter 100: loss 3.2218, time 21.94ms, mfu 1.40%\n",
            "iter 110: loss 2.9584, time 21.96ms, mfu 1.42%\n",
            "iter 120: loss 2.7587, time 22.30ms, mfu 1.44%\n",
            "iter 130: loss 2.6085, time 22.00ms, mfu 1.47%\n",
            "iter 140: loss 2.4273, time 22.60ms, mfu 1.48%\n",
            "iter 150: loss 2.3912, time 22.81ms, mfu 1.49%\n",
            "iter 160: loss 2.2418, time 22.27ms, mfu 1.50%\n",
            "iter 170: loss 2.1026, time 23.16ms, mfu 1.51%\n",
            "iter 180: loss 2.0018, time 21.92ms, mfu 1.52%\n",
            "iter 190: loss 1.9252, time 22.27ms, mfu 1.54%\n",
            "iter 200: loss 1.7749, time 22.11ms, mfu 1.55%\n",
            "iter 210: loss 1.7039, time 21.84ms, mfu 1.56%\n",
            "iter 220: loss 1.6966, time 22.01ms, mfu 1.57%\n",
            "iter 230: loss 1.5232, time 22.16ms, mfu 1.57%\n",
            "iter 240: loss 1.4852, time 22.04ms, mfu 1.58%\n",
            "step 250: train loss 1.1929, val loss 1.1806\n",
            "saving checkpoint to out-CEO_words\n",
            "iter 250: loss 1.4415, time 5513.49ms, mfu 1.42%\n",
            "iter 260: loss 1.4119, time 22.52ms, mfu 1.44%\n",
            "iter 270: loss 1.3379, time 22.50ms, mfu 1.46%\n",
            "iter 280: loss 1.2486, time 25.95ms, mfu 1.45%\n",
            "iter 290: loss 1.1970, time 24.75ms, mfu 1.45%\n",
            "iter 300: loss 1.1731, time 25.48ms, mfu 1.45%\n",
            "iter 310: loss 1.1670, time 24.81ms, mfu 1.45%\n",
            "iter 320: loss 1.1226, time 25.38ms, mfu 1.45%\n",
            "iter 330: loss 1.1088, time 25.85ms, mfu 1.45%\n",
            "iter 340: loss 1.0399, time 25.11ms, mfu 1.44%\n",
            "iter 350: loss 0.9930, time 24.99ms, mfu 1.45%\n",
            "iter 360: loss 0.9974, time 25.76ms, mfu 1.44%\n",
            "iter 370: loss 0.9442, time 26.27ms, mfu 1.44%\n",
            "iter 380: loss 0.9373, time 26.67ms, mfu 1.43%\n",
            "iter 390: loss 0.9070, time 26.41ms, mfu 1.42%\n",
            "iter 400: loss 0.8859, time 26.32ms, mfu 1.42%\n",
            "iter 410: loss 0.8728, time 26.50ms, mfu 1.41%\n",
            "iter 420: loss 0.8359, time 26.59ms, mfu 1.41%\n",
            "iter 430: loss 0.8425, time 26.02ms, mfu 1.41%\n",
            "iter 440: loss 0.7763, time 25.92ms, mfu 1.41%\n",
            "iter 450: loss 0.7877, time 36.28ms, mfu 1.37%\n",
            "iter 460: loss 0.7703, time 26.52ms, mfu 1.37%\n",
            "iter 470: loss 0.7637, time 22.34ms, mfu 1.39%\n",
            "iter 480: loss 0.7257, time 22.45ms, mfu 1.41%\n",
            "iter 490: loss 0.7436, time 22.79ms, mfu 1.43%\n",
            "step 500: train loss 0.5094, val loss 0.5214\n",
            "saving checkpoint to out-CEO_words\n",
            "iter 500: loss 0.7122, time 3246.36ms, mfu 1.29%\n",
            "iter 510: loss 0.7040, time 22.14ms, mfu 1.32%\n",
            "iter 520: loss 0.6881, time 22.46ms, mfu 1.35%\n",
            "iter 530: loss 0.6852, time 22.70ms, mfu 1.38%\n",
            "iter 540: loss 0.6479, time 22.04ms, mfu 1.40%\n",
            "iter 550: loss 0.6638, time 22.28ms, mfu 1.43%\n",
            "iter 560: loss 0.6888, time 22.32ms, mfu 1.45%\n",
            "iter 570: loss 0.6589, time 22.30ms, mfu 1.46%\n",
            "iter 580: loss 0.6678, time 22.48ms, mfu 1.48%\n",
            "iter 590: loss 0.6089, time 23.29ms, mfu 1.49%\n",
            "iter 600: loss 0.6268, time 22.60ms, mfu 1.50%\n",
            "iter 610: loss 0.6185, time 22.32ms, mfu 1.51%\n",
            "iter 620: loss 0.5959, time 23.78ms, mfu 1.51%\n",
            "iter 630: loss 0.6400, time 22.30ms, mfu 1.52%\n",
            "iter 640: loss 0.5844, time 22.45ms, mfu 1.53%\n",
            "iter 650: loss 0.5776, time 22.83ms, mfu 1.54%\n",
            "iter 660: loss 0.5699, time 22.54ms, mfu 1.55%\n",
            "iter 670: loss 0.5638, time 22.13ms, mfu 1.56%\n",
            "iter 680: loss 0.5589, time 22.08ms, mfu 1.56%\n",
            "iter 690: loss 0.5541, time 22.11ms, mfu 1.57%\n",
            "iter 700: loss 0.5625, time 22.55ms, mfu 1.58%\n",
            "iter 710: loss 0.5556, time 25.47ms, mfu 1.56%\n",
            "iter 720: loss 0.5540, time 22.30ms, mfu 1.57%\n",
            "iter 730: loss 0.5319, time 22.06ms, mfu 1.57%\n",
            "iter 740: loss 0.5059, time 21.94ms, mfu 1.58%\n",
            "step 750: train loss 0.3487, val loss 0.3651\n",
            "saving checkpoint to out-CEO_words\n",
            "iter 750: loss 0.5143, time 3841.56ms, mfu 1.43%\n",
            "iter 760: loss 0.5455, time 26.00ms, mfu 1.42%\n",
            "iter 770: loss 0.5101, time 26.18ms, mfu 1.42%\n",
            "iter 780: loss 0.5038, time 26.39ms, mfu 1.41%\n",
            "iter 790: loss 0.4977, time 28.06ms, mfu 1.40%\n",
            "iter 800: loss 0.4816, time 25.84ms, mfu 1.40%\n",
            "iter 810: loss 0.5142, time 22.59ms, mfu 1.42%\n",
            "iter 820: loss 0.4875, time 22.40ms, mfu 1.44%\n",
            "iter 830: loss 0.4836, time 22.17ms, mfu 1.46%\n",
            "iter 840: loss 0.4870, time 22.17ms, mfu 1.48%\n",
            "iter 850: loss 0.4621, time 22.83ms, mfu 1.49%\n",
            "iter 860: loss 0.4638, time 22.51ms, mfu 1.50%\n",
            "iter 870: loss 0.4594, time 22.55ms, mfu 1.51%\n",
            "iter 880: loss 0.4596, time 22.97ms, mfu 1.52%\n",
            "iter 890: loss 0.4981, time 22.09ms, mfu 1.53%\n",
            "iter 900: loss 0.4814, time 22.43ms, mfu 1.54%\n",
            "iter 910: loss 0.4485, time 24.22ms, mfu 1.54%\n",
            "iter 920: loss 0.4596, time 22.32ms, mfu 1.54%\n",
            "iter 930: loss 0.4422, time 22.56ms, mfu 1.55%\n",
            "iter 940: loss 0.4344, time 22.07ms, mfu 1.56%\n",
            "iter 950: loss 0.4585, time 22.23ms, mfu 1.57%\n",
            "iter 960: loss 0.4256, time 22.51ms, mfu 1.57%\n",
            "iter 970: loss 0.4501, time 22.30ms, mfu 1.58%\n",
            "iter 980: loss 0.4327, time 29.44ms, mfu 1.54%\n",
            "iter 990: loss 0.4372, time 22.54ms, mfu 1.55%\n",
            "step 1000: train loss 0.2903, val loss 0.3065\n",
            "saving checkpoint to out-CEO_words\n",
            "iter 1000: loss 0.4495, time 3238.36ms, mfu 1.40%\n",
            "iter 1010: loss 0.4364, time 23.74ms, mfu 1.41%\n",
            "iter 1020: loss 0.4428, time 22.40ms, mfu 1.43%\n",
            "iter 1030: loss 0.4344, time 22.71ms, mfu 1.45%\n",
            "iter 1040: loss 0.4294, time 23.99ms, mfu 1.45%\n",
            "iter 1050: loss 0.4250, time 22.42ms, mfu 1.47%\n",
            "iter 1060: loss 0.4214, time 22.33ms, mfu 1.48%\n",
            "iter 1070: loss 0.4201, time 22.28ms, mfu 1.50%\n",
            "iter 1080: loss 0.4195, time 22.49ms, mfu 1.51%\n",
            "iter 1090: loss 0.4234, time 22.29ms, mfu 1.52%\n",
            "iter 1100: loss 0.4093, time 26.66ms, mfu 1.51%\n",
            "iter 1110: loss 0.4049, time 31.38ms, mfu 1.47%\n",
            "iter 1120: loss 0.4229, time 25.36ms, mfu 1.47%\n",
            "iter 1130: loss 0.4010, time 24.94ms, mfu 1.47%\n",
            "iter 1140: loss 0.3927, time 24.87ms, mfu 1.46%\n",
            "iter 1150: loss 0.4274, time 29.93ms, mfu 1.44%\n",
            "iter 1160: loss 0.4086, time 25.39ms, mfu 1.44%\n",
            "iter 1170: loss 0.3928, time 26.94ms, mfu 1.43%\n",
            "iter 1180: loss 0.3811, time 26.87ms, mfu 1.42%\n",
            "iter 1190: loss 0.3914, time 30.03ms, mfu 1.40%\n",
            "iter 1200: loss 0.3864, time 29.90ms, mfu 1.38%\n",
            "iter 1210: loss 0.4043, time 26.28ms, mfu 1.38%\n",
            "iter 1220: loss 0.4060, time 26.11ms, mfu 1.38%\n",
            "iter 1230: loss 0.4191, time 31.49ms, mfu 1.36%\n",
            "iter 1240: loss 0.4020, time 26.23ms, mfu 1.36%\n",
            "step 1250: train loss 0.2600, val loss 0.2807\n",
            "saving checkpoint to out-CEO_words\n",
            "iter 1250: loss 0.3853, time 3455.98ms, mfu 1.23%\n",
            "iter 1260: loss 0.3751, time 22.82ms, mfu 1.26%\n",
            "iter 1270: loss 0.3816, time 22.28ms, mfu 1.30%\n",
            "iter 1280: loss 0.3745, time 22.85ms, mfu 1.33%\n",
            "iter 1290: loss 0.3897, time 23.31ms, mfu 1.35%\n",
            "iter 1300: loss 0.3732, time 24.36ms, mfu 1.36%\n",
            "iter 1310: loss 0.3850, time 22.93ms, mfu 1.39%\n",
            "iter 1320: loss 0.3685, time 22.63ms, mfu 1.41%\n",
            "iter 1330: loss 0.3852, time 22.27ms, mfu 1.43%\n",
            "iter 1340: loss 0.3767, time 22.78ms, mfu 1.45%\n",
            "iter 1350: loss 0.3836, time 22.52ms, mfu 1.46%\n",
            "iter 1360: loss 0.3820, time 22.55ms, mfu 1.48%\n",
            "iter 1370: loss 0.3519, time 23.12ms, mfu 1.49%\n",
            "iter 1380: loss 0.3803, time 22.30ms, mfu 1.50%\n",
            "iter 1390: loss 0.3623, time 22.18ms, mfu 1.51%\n",
            "iter 1400: loss 0.3948, time 22.69ms, mfu 1.52%\n",
            "iter 1410: loss 0.3657, time 23.47ms, mfu 1.52%\n",
            "iter 1420: loss 0.3570, time 22.56ms, mfu 1.53%\n",
            "iter 1430: loss 0.3468, time 22.29ms, mfu 1.54%\n",
            "iter 1440: loss 0.3567, time 22.43ms, mfu 1.55%\n",
            "iter 1450: loss 0.3592, time 22.37ms, mfu 1.56%\n",
            "iter 1460: loss 0.3638, time 23.28ms, mfu 1.56%\n",
            "iter 1470: loss 0.3864, time 22.77ms, mfu 1.56%\n",
            "iter 1480: loss 0.3561, time 22.92ms, mfu 1.56%\n",
            "iter 1490: loss 0.3457, time 22.84ms, mfu 1.57%\n",
            "step 1500: train loss 0.2439, val loss 0.2627\n",
            "saving checkpoint to out-CEO_words\n",
            "iter 1500: loss 0.3560, time 3477.87ms, mfu 1.41%\n",
            "iter 1510: loss 0.3566, time 24.90ms, mfu 1.41%\n",
            "iter 1520: loss 0.3540, time 25.60ms, mfu 1.41%\n",
            "iter 1530: loss 0.3502, time 26.09ms, mfu 1.41%\n",
            "iter 1540: loss 0.3480, time 26.05ms, mfu 1.41%\n",
            "iter 1550: loss 0.3690, time 25.92ms, mfu 1.41%\n",
            "iter 1560: loss 0.3547, time 27.81ms, mfu 1.40%\n",
            "iter 1570: loss 0.3357, time 27.12ms, mfu 1.39%\n",
            "iter 1580: loss 0.3414, time 26.93ms, mfu 1.39%\n",
            "iter 1590: loss 0.3333, time 33.07ms, mfu 1.36%\n",
            "iter 1600: loss 0.3576, time 26.08ms, mfu 1.36%\n",
            "iter 1610: loss 0.3644, time 25.98ms, mfu 1.37%\n",
            "iter 1620: loss 0.3434, time 26.50ms, mfu 1.37%\n",
            "iter 1630: loss 0.3457, time 22.32ms, mfu 1.39%\n",
            "iter 1640: loss 0.3350, time 23.54ms, mfu 1.41%\n",
            "iter 1650: loss 0.3415, time 22.63ms, mfu 1.43%\n",
            "iter 1660: loss 0.3289, time 22.48ms, mfu 1.44%\n",
            "iter 1670: loss 0.3418, time 22.89ms, mfu 1.46%\n",
            "iter 1680: loss 0.3391, time 23.49ms, mfu 1.47%\n",
            "iter 1690: loss 0.3353, time 23.48ms, mfu 1.48%\n",
            "iter 1700: loss 0.3379, time 22.86ms, mfu 1.49%\n",
            "iter 1710: loss 0.3361, time 31.24ms, mfu 1.45%\n",
            "iter 1720: loss 0.3375, time 27.72ms, mfu 1.44%\n",
            "iter 1730: loss 0.3272, time 23.12ms, mfu 1.45%\n",
            "iter 1740: loss 0.3321, time 22.64ms, mfu 1.47%\n",
            "step 1750: train loss 0.2361, val loss 0.2583\n",
            "saving checkpoint to out-CEO_words\n",
            "iter 1750: loss 0.3354, time 3263.19ms, mfu 1.32%\n",
            "iter 1760: loss 0.3276, time 23.95ms, mfu 1.34%\n",
            "iter 1770: loss 0.3258, time 22.91ms, mfu 1.36%\n",
            "iter 1780: loss 0.3281, time 22.69ms, mfu 1.39%\n",
            "iter 1790: loss 0.3422, time 24.90ms, mfu 1.40%\n",
            "iter 1800: loss 0.3333, time 23.15ms, mfu 1.41%\n",
            "iter 1810: loss 0.3311, time 22.62ms, mfu 1.43%\n",
            "iter 1820: loss 0.3297, time 23.34ms, mfu 1.44%\n",
            "iter 1830: loss 0.3379, time 23.07ms, mfu 1.46%\n",
            "iter 1840: loss 0.3245, time 23.22ms, mfu 1.47%\n",
            "iter 1850: loss 0.3250, time 22.47ms, mfu 1.48%\n",
            "iter 1860: loss 0.3251, time 25.21ms, mfu 1.48%\n",
            "iter 1870: loss 0.3169, time 22.52ms, mfu 1.49%\n",
            "iter 1880: loss 0.3362, time 22.90ms, mfu 1.50%\n",
            "iter 1890: loss 0.3387, time 24.30ms, mfu 1.50%\n",
            "iter 1900: loss 0.3189, time 23.47ms, mfu 1.50%\n",
            "iter 1910: loss 0.3266, time 22.56ms, mfu 1.51%\n",
            "iter 1920: loss 0.3464, time 27.39ms, mfu 1.50%\n",
            "iter 1930: loss 0.3365, time 25.53ms, mfu 1.49%\n",
            "iter 1940: loss 0.3254, time 25.31ms, mfu 1.48%\n",
            "iter 1950: loss 0.3374, time 25.08ms, mfu 1.48%\n",
            "iter 1960: loss 0.3315, time 28.29ms, mfu 1.46%\n",
            "iter 1970: loss 0.3272, time 26.38ms, mfu 1.45%\n",
            "iter 1980: loss 0.3249, time 25.18ms, mfu 1.45%\n",
            "iter 1990: loss 0.3227, time 25.27ms, mfu 1.45%\n",
            "step 2000: train loss 0.2305, val loss 0.2527\n",
            "saving checkpoint to out-CEO_words\n",
            "iter 2000: loss 0.3256, time 3763.47ms, mfu 1.30%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python sample.py --out_dir=out-CEO_words --start=\"合作與創新的看法\""
      ],
      "metadata": {
        "id": "S5z6htu2ONWz",
        "outputId": "23c48ac1-0340-4213-cd97-b4da929c06b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: out_dir = out-CEO_words\n",
            "Overriding: start = 合作與創新的看法\n",
            "number of parameters: 0.89M\n",
            "Loading meta from data/CEO_words/meta.pkl...\n",
            "合作與創新的看法是必要的\n",
            "\n",
            "經營管理的核心基礎\n",
            "以科學的方式進行任務：先蒐集資料，然後進行拆分與解決\n",
            "\n",
            "經營管理的核心基石\n",
            "進行任務的科學方法：先蒐集資料，然後進行拆分與解決\n",
            "\n",
            "具有解決問題的領導才能\n",
            "就算從事不同專業或行業，本質上我們都是在追蹤和修復問題\n",
            "\n",
            "經營管理的核心要素\n",
            "能夠預知未來的重要性在於能夠妥善準備\n",
            "\n",
            "領導的基石\n",
            "利用合適的分工，每個人都能熟練地完成自己的任務\n",
            "\n",
            "領導的根本要素\n",
            "去滿足重要的人和大部分的人的需求是分享的目的\n",
            "\n",
            "自身的優點是選擇產業時必需重視的三大要素之一\n",
            "除了滿足TAM夠大和CAGR夠高的條件外，選擇產業時還需考量自身的優勢是否能夠發揮，畢竟每個行業都有不同的特點\n",
            "\n",
            "解析生意：瞭解生意的本質\n",
            "做生意的祕訣在於精確掌握買進，而非賣出\n",
            "\n",
            "電動車的市場趨勢呈現上升的態勢\n",
            "預計到2025年，電動車市場將成長到6,0000億美元，年複合增長率高達42％所以，現在有很多人都爭相進入這個市場，而且市場的成長潛力仍然很大\n",
            "\n",
            "專注於學習的道路\n",
            "當你追求知識和專業時，要一直保持謙遜，並保持對學習的渴求\n",
            "\n",
            "選擇產業時，自身的優勢是其中三個關鍵考量之一\n",
            "除了滿足TAM夠大和CAGR夠高的條件外\n",
            "---------------\n",
            "合作與創新的看法\n",
            "\n",
            "分享的真諦\n",
            "分享智慧成果有助於避免不同的人反覆做同樣的事情，節省時間\n",
            "\n",
            "需要把優先順序理清楚\n",
            "以智慧般的方式進行選擇，正確排列當前需要處理的事項，並謹慎地分配時間\n",
            "\n",
            "經營管理的基本要素\n",
            "具有預測能力的重要性在於能做充分準備\n",
            "\n",
            "「論創業：經營者的心態剖析」\n",
            "商界如同戰場，勝敗關鍵在於是否有戰爭精神\n",
            "\n",
            "持續拓展知識面\n",
            "對於知識和專業，保持謙卑和持續學習的渴望是必不可少的\n",
            "\n",
            "經營管理的基本觀念\n",
            "前瞻性改正：提前找出並改正錯誤，減少浪費的產生\n",
            "\n",
            "經營管理的核心基礎\n",
            "不再迷津：不再走錯誤路，減少浪費的出現\n",
            "\n",
            "電動車發展的趨勢是不斷向上\n",
            "預計到2025年，電動車市場將達到6,0000億美元的規模，並以驚人的42％年均增長率迅猛增長因此，許多人正在爭相進入這一市場，且未來的發展前景非常廣闊\n",
            "\n",
            "領導的基本原則\n",
            "分工要得當，讓每個人都能在合適的崗位上游刃有餘\n",
            "\n",
            "經營管理的重要基礎\n",
            "具備預測的能力，讓我們能夠做好未來的準備\n",
            "\n",
            "解決問題的專業技能\n",
            "雖然專業背景不同，但解決問題是所有工作的核心\n",
            "\n",
            "領導的根本核心\n",
            "適合的分工使每個人都能在各自的崗位上毫不費力地完成任務\n",
            "\n",
            "經營管理的根本核心\n",
            "具備預測能力，讓\n",
            "---------------\n",
            "合作與創新的看法是必須妥善準備\n",
            "\n",
            "領導的基本原則\n",
            "恰當地分工使每個人都能輕鬆應對工作，遊刃有餘\n",
            "\n",
            "解決問題的才能\n",
            "不同專業、不同產業，但重要任務都是在找出並修正問題\n",
            "\n",
            "經營管理的核心基礎\n",
            "運用科學方法處理工作：先收集資料，再進行分析與拆分\n",
            "\n",
            "經營管理的根本原則\n",
            "探索事物的實質：通過建模應對各種變化\n",
            "\n",
            "由抄取到超越\n",
            "不必以抄襲爲恥，因爲所有事情都是從模仿開始，然後才能超越\n",
            "\n",
            "產業變遷的模式\n",
            "當一個產業的發展開始進入飽和階段，大約有70%的人會決定留在該產業，只有30%的人會轉移到其他有潛力的產業\n",
            "\n",
            "經營管理的基本基礎\n",
            "遠離重復：避免重蹈覆轍，減少重復的錯誤，節省心力與成本\n",
            "\n",
            "經營管理的核心觀念\n",
            "提前預測未來，能夠讓我們做出完善的準備\n",
            "\n",
            "經營管理的根本原則\n",
            "偵查事物的真相：建立模型可處理各種變化\n",
            "\n",
            "經營管理的核心觀點\n",
            "預測未來的重要性在於提前做好準備\n",
            "\n",
            "領導的核心基石\n",
            "分工得當，每個人都能在適合自己的崗位上發揮所長\n",
            "\n",
            "電動車市場持續增長中\n",
            "根據專家預測，到2025年，電動車市場預計將達到6,000億美元的規模，年複合增長率高達42％因此，許多人正在搶購這個市場，而且市場的發展空間還很大\n",
            "\n",
            "擁有巨大的市場需\n",
            "---------------\n",
            "合作與創新的看法\n",
            "\n",
            "經營管理的核心要素\n",
            "探尋事物真相的關鍵：以模方式應對各種變化\n",
            "\n",
            "經營管理的核心概念\n",
            "預測未來的重要性在於能夠充分規劃\n",
            "\n",
            "經營管理的核心要素\n",
            "能夠預知未來的重要性在於能夠妥善準備\n",
            "\n",
            "領導的基本概念\n",
            "分工要得當，讓每個人都能在合適的崗位上游刃有餘\n",
            "\n",
            "經營管理的根本核心\n",
            "以科學的方式處理事務：首先進行資料收集，然後分割與征服\n",
            "\n",
            "「探討商業：成功營運的不同取態」\n",
            "將商業視為戰場，你的戰鬥精神將決定成敗\n",
            "\n",
            "經營管理的根本原則\n",
            "探索事物的核心：建立模型，以適應各種變化\n",
            "\n",
            "領導的獨特質\n",
            "我們需要有理想，能夠宣揚理想，同時要具備敏銳的觀察力和屢勝屢敗的毅力\n",
            "\n",
            "經營管理的基本觀念\n",
            "遠離重復：避免重蹈覆轍，減少重復的錯誤，節省心力與成本\n",
            "\n",
            "選擇產業時，需考慮市場需求(TAM)的大小是否足夠\n",
            "唯有市場規模超過新臺幣一兆的產業，鴻海才會有興趣參與\n",
            "\n",
            "經營管理的核心要點\n",
            "透過科學方法來解決問題：先收集資料，再進行分割與征服\n",
            "\n",
            "領導的屬性\n",
            "我們應該擁有自己的夢想，並具備宣揚理想的能力，還要有敏銳的觀察力和屢戰屢敗的毅力\n",
            "\n",
            "經營管理的基本概念\n",
            "能夠提前預測未來，使我們能夠做充足的準備\n",
            "\n",
            "電動車的轉折點：TTC和\n",
            "---------------\n",
            "合作與創新的看法是可以抄襲爲抄襲是可恥的\n",
            "\n",
            "領導的基本原則\n",
            "聰明地分工，使每個人都能充分發揮自己的能力，輕松應對工作\n",
            "\n",
            "領導的品質\n",
            "我們應該擁有自己的夢想，並具備表達夢想的能力，還要有敏銳的觀察力和不斷努力的毅力\n",
            "\n",
            "EV發展的關鍵性：電動車發展趨勢(中)\n",
            "從前，擁有燃油引擎意味著處於領先地位；但未來，唯有掌握三電技術、軟體和半導體技術者能在頂峯獨尊\n",
            "\n",
            "電動車市場的新趨勢：TTC和TTM正陷入下跌局面\n",
            "由於產業變遷，EntryBarrier降低，這導致了競爭者的增加在目前的EV產業中，有傳統車廠、ICT公司和新創公司這三個主要的競爭勢力\n",
            "\n",
            "自身優勢是選擇產業時的三個主要考量之一\n",
            "除了滿足TAM夠大和CAGR夠高的條件外，選擇產業時還需要考量自身的優勢是否能夠發揮，畢竟每個行業都有不同的特點\n",
            "\n",
            "領導的特徵\n",
            "應該有個人理想，並有能力宣揚理想，還要有敏銳的觀察力和屢戰屢敗的毅力\n",
            "\n",
            "解構分享的真正含義\n",
            "善用大家的力量，一起創造更多的價值，這是實現社會進步的關鍵\n",
            "\n",
            "「論商業：探究成功商業心態的不同角度」\n",
            "商界如同戰場，勝敗關鍵在於是否有戰爭精神\n",
            "\n",
            "擅長找出解決問題的方法\n",
            "就算從事不同專業或行業，本質上我們都是在追\n",
            "---------------\n",
            "合作與創新的看法\n",
            "\n",
            "電動車的趨勢變化：TTC和TTM呈現逐漸下滑的趨勢\n",
            "由於產業變遷，EntryBarrier降低，這意味著競爭者的增加目前EV產業中，主要有傳統車廠、ICT公司和新創公司三個競爭勢力\n",
            "\n",
            "電動車的發展趨勢有所改變：TTC和TTM下降\n",
            "由於產業變遷，EntryBarrier降低，因此造成競爭對手的增加目前EV產業中，有三個主要的競爭勢力，包括傳統車廠、ICT公司和新創公司\n",
            "\n",
            "經營管理的核心要素\n",
            "避繞重犯：避免重蹈覆轍，減少重複錯誤的浪費\n",
            "\n",
            "經營管理的根本核心\n",
            "探尋事物本質的關鍵：以模型爲基礎，應對各種變化\n",
            "\n",
            "電動車的市場趨勢呈現上升的態勢\n",
            "根據專家預估，到2025年，電動車市場的規模將達到6,000億美元，年平均增長率將高達42％因此，目前有很多人都急於進入這個市場，且市場的潛力非常巨大\n",
            "\n",
            "領導的基本核心\n",
            "分工要得當，讓每個人都能在合適的崗位上游刃有餘\n",
            "\n",
            "經營管理的核心觀念\n",
            "進行工作的科學方法：先蒐集資料，然後進行分析與解決\n",
            "\n",
            "電動車市場趨勢的轉變：TTC和TTM出現下滑現象\n",
            "隨著產業變遷，EntryBarrier降低，這導致了競爭者的增加在目前的EV產業中，主要有傳統車廠、ICT公司和\n",
            "---------------\n",
            "合作與創新的看法是實現超越\n",
            "\n",
            "領導的基本特點\n",
            "要有夢想，且要能夠發揮夢想的力量，還要擁有敏銳的觀察力和屢戰的毅力\n",
            "\n",
            "談論生意：深入瞭解生意的本質\n",
            "做生意的成功在於買賣之間，著重於買進的技巧\n",
            "\n",
            "能夠從不同角度解決問題\n",
            "就算從事不同專業和產業，我們的工作都是在排除錯誤\n",
            "\n",
            "經營管理的根本要素\n",
            "從錯處學：透過錯誤學習，避免重蹈覆轍，減少浪費\n",
            "\n",
            "經營管理的根本原則\n",
            "探索問題的實質：通過建立模型，以適應各種變化\n",
            "\n",
            "經營管理的基本概念\n",
            "運用科學方法處理工作：先收集資料，再進行分析與拆分\n",
            "\n",
            "電動車的市場趨勢呈現上升的態勢\n",
            "預計到2025年，電動車市場將達到6,000億美元的規模，年均增長率高達42％因此，眾多人紛紛進入這個市場，而且還有很大的發展潛力\n",
            "\n",
            "經營管理的核心觀念\n",
            "具備預測的能力，讓我們能夠提前做好準備\n",
            "\n",
            "EV市場持續攀升\n",
            "2025年預計電動車市場將達到6,000億美元的規模，年複合增長率高達42％因此，眾多人目前紛紛進入這個市場，而且市場的發展潛力仍然非常巨大\n",
            "\n",
            "領導的基本基礎\n",
            "分工合理，每個人都能輕松應付各自的工作\n",
            "\n",
            "領導的核心觀念\n",
            "主要特性\n",
            "我們應該擁有自己的夢想，並具備宣揚理想的能力，還要有敏銳的觀察\n",
            "---------------\n",
            "合作與創新的看法\n",
            "\n",
            "產業轉變的走向\n",
            "當一個產業的成長逐漸趨緩，有70%的人會繼續在該產業中工作，而剩下的30%會轉向下一個有潛力的產業\n",
            "\n",
            "領導的基本原理\n",
            "分工合理，每個人都能輕松應付各自的工作\n",
            "\n",
            "經營管理的核心基礎\n",
            "避免重犯：努力避免重蹈覆轍，減少不必要的浪費\n",
            "\n",
            "從抄襲到超越\n",
            "沒有必要感到羞恥，因爲一切都從抄襲開始，然後才能超越\n",
            "\n",
            "持續續拓展知識面\n",
            "在知識和專業的領域，謙虛和對學習的渴望是關鍵\n",
            "\n",
            "EV市場持續擴大\n",
            "預計到2025年，電動車市場將成為價值6,000億美元的大市場，年平均增長率高達42％因此，目前有很多人紛紛進入這個市場，因為發展空間仍然非常大\n",
            "\n",
            "由模仿收穫超越\n",
            "不必感到羞愧，因爲一切都從模仿開始，然後才能超越\n",
            "\n",
            "領導的核心基礎\n",
            "適合的分工讓每個人能夠得心應手地執行工作\n",
            "\n",
            "經營管理的基本觀念\n",
            "探尋事物真相的關鍵：以模型爲基礎，應對各種變化\n",
            "\n",
            "領導的基本特性\n",
            "要有理想，並具備宣揚理想的能力，還要有敏銳的觀察力和屢戰屢敗的毅力\n",
            "\n",
            "產業選擇的三大考量：必須有高年均複合增長率\n",
            "只有年複合成長率超過20%，新的勢力纔有機會在市場上有所作為\n",
            "\n",
            "電動車市場持續增長中\n",
            "預計到2025年，電動車市場將成為價\n",
            "---------------\n",
            "合作與創新的看法是必要的\n",
            "三個決定產業選擇的重要因素：持續高的年均複合增長率\n",
            "我們希望年複合成長率能夠達到20%以上，這樣市場才能容納更多的競爭者\n",
            "\n",
            "經營管理的基本核心\n",
            "預測的能力使我們能夠充分準備未來\n",
            "\n",
            "從照搬到創新\n",
            "不必嫌抄是可恥的，因爲所有事情都是從抄開始，然後纔有超越\n",
            "\n",
            "經營管理的基本核心\n",
            "預測未來的重要性在於提前做好準備\n",
            "\n",
            "產業的選擇關鍵在於市場需求(TAM)的龐大程度\n",
            "鴻海只會考慮進入市場規模超過新臺幣一兆的產業\n",
            "\n",
            "分享的精髓\n",
            "分享智慧成果有助於避免不同的人反覆做同樣的事情，節省時間\n",
            "\n",
            "「談經商：探討經營態度的不同面向」\n",
            "商場就是一場戰爭，成功依賴於你是否持有戰鬥精神\n",
            "\n",
            "經營管理的核心基礎\n",
            "用科學方式執行工作：先收集資料，再進行分析與征服\n",
            "\n",
            "談論生意：探究生意的本質\n",
            "在商業領域中，學會好好好買進比賣出更加重要\n",
            "\n",
            "由模仿收穫超越\n",
            "不必感到羞愧，因爲所有事情都是從抄襲開始，然後才能超越\n",
            "\n",
            "經營管理的核心基礎\n",
            "預測的重要價值在於能夠充分做好準備\n",
            "\n",
            "永不止步的學習\n",
            "對於知識和專業，要保持謙遜，並堅守對學習的渴望\n",
            "\n",
            "經營管理的關鍵要點\n",
            "以科學方法進行任務：先蒐集資料，然後進行拆分與解決\n",
            "\n",
            "「談生意：\n",
            "---------------\n",
            "合作與創新的看法是可以上可以上的領域，才能吸引鴻海進入\n",
            "\n",
            "領導的根本基石\n",
            "將工作適當地分配給每個人，讓他們各有所長，輕鬆應對\n",
            "\n",
            "經營管理的核心要素\n",
            "防微杜漸：從小處著手，預防錯誤發生，減少不必要的浪費\n",
            "\n",
            "探索生意：揭示生意的核心所在\n",
            "在做生意時，買東西比賣東西更有關鍵性的意義\n",
            "\n",
            "關於電動車的趨勢分析：EV的關鍵(中)\n",
            "從前，擁有燃油引擎意味著處於領先地位；但未來，唯有掌握三電技術、軟體和半導體技術者能在頂峯獨尊\n",
            "\n",
            "經營管理的核心基礎\n",
            "探究情況的實質：以模型爲基礎，能夠適應不同的變化\n",
            "\n",
            "經營管理的核心基礎\n",
            "前瞻性改正：提前找出並改正錯誤，減少浪費的產生\n",
            "\n",
            "經營管理的核心概念\n",
            "運用科學方法處理工作：先收集資料，再進行分析與拆分\n",
            "\n",
            "領導的根本核心\n",
            "分工得當使每個人都能以遊刃有餘的姿態執行工作\n",
            "\n",
            "自身優勢是選擇產業時的三大關鍵之一\n",
            "除了考慮市場規模足夠大和年平均成長率足夠高之外，選擇產業時還要考慮自身的優勢是否能夠與之相對應，因為各個行業有其獨特的特點\n",
            "\n",
            "經營管理的基本要素\n",
            "具備預測能力，使我們能夠做好未來的準備\n",
            "\n",
            "解決問題的才能\n",
            "不同專業、不同產業，但重要任務都是在找出並修正問題\n",
            "\n",
            "產業選擇中的三個關鍵要素\n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3aw41-97O9Bv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}